{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件包含的字段：\n",
      "- map_name\n",
      "- token\n",
      "- ego_current_state\n",
      "- ego_agent_future\n",
      "- neighbor_agents_past\n",
      "- neighbor_agents_future\n",
      "- static_objects\n",
      "- lanes\n",
      "- lanes_speed_limit\n",
      "- lanes_has_speed_limit\n",
      "- route_lanes\n",
      "- route_lanes_speed_limit\n",
      "- route_lanes_has_speed_limit\n",
      "\n",
      "各字段详细信息：\n",
      "\n",
      "字段名: map_name\n",
      "形状: ()\n",
      "数据类型: <U12\n",
      "前2个元素示例:\n",
      "sg-one-north\n",
      "\n",
      "字段名: token\n",
      "形状: ()\n",
      "数据类型: <U16\n",
      "前2个元素示例:\n",
      "0cc1ed4044a35a0e\n",
      "\n",
      "字段名: ego_current_state\n",
      "形状: (10,)\n",
      "数据类型: float32\n",
      "前2个元素示例:\n",
      "[0. 0.]\n",
      "\n",
      "字段名: ego_agent_future\n",
      "形状: (80, 3)\n",
      "数据类型: float32\n",
      "前2个元素示例:\n",
      "[[ 2.7070570e-01 -5.0810920e-03  4.5437351e-04]\n",
      " [ 5.3612548e-01 -8.7091913e-03  1.4120192e-03]]\n",
      "\n",
      "字段名: neighbor_agents_past\n",
      "形状: (32, 21, 11)\n",
      "数据类型: float32\n",
      "前2个元素示例:\n",
      "[[[ 2.99279499e+01 -6.43578482e+00 -2.52232462e-01  9.67666686e-01\n",
      "   -1.60076380e+00  6.14640522e+00  9.38235164e-01  2.10661817e+00\n",
      "    0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.97640381e+01 -5.85319901e+00 -2.57299006e-01  9.66331840e-01\n",
      "   -1.62611616e+00  6.00196314e+00  9.37697709e-01  2.10792351e+00\n",
      "    0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.95878773e+01 -5.27033567e+00 -2.64564395e-01  9.64368045e-01\n",
      "   -1.68225384e+00  5.90226936e+00  9.36573863e-01  2.10625196e+00\n",
      "    0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.94294147e+01 -4.64231968e+00 -2.57289916e-01  9.66334283e-01\n",
      "   -1.61827016e+00  6.09951305e+00  9.36767876e-01  2.10154891e+00\n",
      "    0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.92654972e+01 -4.03542423e+00 -2.60596037e-01  9.65447962e-01\n",
      "   -1.62182653e+00  5.97807598e+00  9.35999811e-01  2.10276580e+00\n",
      "    0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.90792236e+01 -3.45219707e+00 -2.78587162e-01  9.60410953e-01\n",
      "   -1.75016761e+00  5.85064459e+00  9.34765160e-01  2.09927988e+00\n",
      "    0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.88787231e+01 -2.83663225e+00 -2.92286217e-01  9.56330895e-01\n",
      "   -1.85544717e+00  5.97689056e+00  9.34108913e-01  2.09817362e+00\n",
      "    0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.86729927e+01 -2.20361948e+00 -2.97991186e-01  9.54568624e-01\n",
      "   -1.94696689e+00  6.13861513e+00  9.33923781e-01  2.09909868e+00\n",
      "    0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.84683762e+01 -1.60053456e+00 -3.06887507e-01  9.51745808e-01\n",
      "   -2.01481128e+00  6.05064249e+00  9.33220923e-01  2.10047126e+00\n",
      "    0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.82500954e+01 -9.99985337e-01 -3.19321334e-01  9.47646499e-01\n",
      "   -2.10974503e+00  5.98451519e+00  9.33439732e-01  2.10224271e+00\n",
      "    0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.80206795e+01 -3.99666905e-01 -3.37465256e-01  9.41338003e-01\n",
      "   -2.19109154e+00  5.92885923e+00  9.33159113e-01  2.10348582e+00\n",
      "    0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.77759590e+01  1.93140045e-01 -3.55662346e-01  9.34614539e-01\n",
      "   -2.32186413e+00  5.89973259e+00  9.32507873e-01  2.10333061e+00\n",
      "    0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.75420322e+01  7.53788292e-01 -3.69159520e-01  9.29366052e-01\n",
      "   -2.33163786e+00  5.69984961e+00  9.32732761e-01  2.10507560e+00\n",
      "    0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.72973900e+01  1.34268510e+00 -3.72194499e-01  9.28154767e-01\n",
      "   -2.36614275e+00  5.75607777e+00  9.33378994e-01  2.10592008e+00\n",
      "    0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.70580826e+01  1.94497240e+00 -3.70652646e-01  9.28771555e-01\n",
      "   -2.36056876e+00  5.84525204e+00  9.34522092e-01  2.10638666e+00\n",
      "    0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.68177280e+01  2.55053329e+00 -3.67305428e-01  9.30100381e-01\n",
      "   -2.34543920e+00  5.89187050e+00  9.35255110e-01  2.10657310e+00\n",
      "    0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.65687313e+01  3.14950299e+00 -3.78434122e-01  9.25628245e-01\n",
      "   -2.43624282e+00  5.90093803e+00  9.35182810e-01  2.10573101e+00\n",
      "    0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.63135662e+01  3.72318435e+00 -3.87322664e-01  9.21944201e-01\n",
      "   -2.48983145e+00  5.81753349e+00  9.34850216e-01  2.10455799e+00\n",
      "    0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.60461712e+01  4.31090403e+00 -3.95574182e-01  9.18434024e-01\n",
      "   -2.57870746e+00  5.82834530e+00  9.34810400e-01  2.10353732e+00\n",
      "    0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.57805557e+01  4.90657711e+00 -3.97966623e-01  9.17399883e-01\n",
      "   -2.61780429e+00  5.90304375e+00  9.34594631e-01  2.10149193e+00\n",
      "    0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.55046196e+01  5.48631430e+00 -4.04474109e-01  9.14549470e-01\n",
      "   -2.69029999e+00  5.82115316e+00  9.35060620e-01  2.09950733e+00\n",
      "    0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "\n",
      " [[-1.62376461e+01 -2.14665165e+01  6.22510724e-02  9.98060524e-01\n",
      "    4.63058148e-03  1.01826536e-02  7.60521591e-01  7.03348696e-01\n",
      "    0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      "  [-1.62413216e+01 -2.14620266e+01 -4.66257781e-02  9.98912454e-01\n",
      "   -4.11686115e-03  9.00805369e-03  7.60404706e-01  7.03075945e-01\n",
      "    0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      "  [-1.62409897e+01 -2.14589634e+01 -2.55364255e-04  9.99999940e-01\n",
      "   -1.54175563e-03  7.16350926e-03  7.60482788e-01  7.02189744e-01\n",
      "    0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      "  [-1.62413197e+01 -2.14607925e+01  5.29325418e-02  9.98598099e-01\n",
      "   -2.52347253e-03 -4.88539808e-04  7.60557294e-01  7.01110899e-01\n",
      "    0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      "  [-1.62451057e+01 -2.14678135e+01 -3.78393412e-01  9.25644875e-01\n",
      "   -4.97502601e-03 -1.09930700e-02  7.61051893e-01  7.01103628e-01\n",
      "    0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      "  [-1.62529964e+01 -2.14751835e+01 -7.50967503e-01  6.60339117e-01\n",
      "   -1.31541491e-02 -2.07007024e-02  7.61999607e-01  7.01673210e-01\n",
      "    0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      "  [-1.62666721e+01 -2.14852009e+01 -8.80998671e-01  4.73118752e-01\n",
      "   -3.70894670e-02 -2.91970875e-02  7.62998104e-01  7.02893674e-01\n",
      "    0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      "  [-1.62806511e+01 -2.14937477e+01 -9.70823944e-01  2.39793301e-01\n",
      "   -3.18505354e-02 -3.47183906e-02  7.63638258e-01  7.03167439e-01\n",
      "    0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      "  [-1.63070526e+01 -2.14999313e+01 -9.99088287e-01 -4.26918976e-02\n",
      "   -6.23084567e-02 -3.32708620e-02  7.65612543e-01  7.03599334e-01\n",
      "    0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      "  [-1.63261051e+01 -2.15023994e+01 -9.99902487e-01  1.39665995e-02\n",
      "   -5.94145097e-02 -1.87160503e-02  7.65809059e-01  7.02755690e-01\n",
      "    0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      "  [-1.63487816e+01 -2.15029221e+01 -9.82790589e-01 -1.84723139e-01\n",
      "   -5.92779554e-02 -1.49427960e-02  7.65294313e-01  7.02236474e-01\n",
      "    0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      "  [-1.63747063e+01 -2.15174809e+01 -9.07904148e-01 -4.19177860e-01\n",
      "   -1.13603614e-01 -7.24401623e-02  7.65220106e-01  7.01796532e-01\n",
      "    0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      "  [-1.64045849e+01 -2.15280457e+01 -7.47357547e-01 -6.64422095e-01\n",
      "   -1.34239390e-01 -6.65232763e-02  7.65918732e-01  7.03374147e-01\n",
      "    0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      "  [-1.64316292e+01 -2.15360279e+01 -4.67565119e-01 -8.83958638e-01\n",
      "   -9.60047022e-02 -4.79155444e-02  7.66172051e-01  7.03031421e-01\n",
      "    0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      "  [-1.64569283e+01 -2.15519314e+01 -3.73139381e-01 -9.27775264e-01\n",
      "   -7.35179558e-02 -5.61745651e-02  7.65727401e-01  7.03227699e-01\n",
      "    0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      "  [-1.64782658e+01 -2.15684834e+01 -4.53111708e-01 -8.91453743e-01\n",
      "   -5.98820522e-02 -9.26782116e-02  7.66638696e-01  7.03794181e-01\n",
      "    0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      "  [-1.64990635e+01 -2.15848980e+01 -4.25074756e-01 -9.05158222e-01\n",
      "   -6.73402995e-02 -8.87059569e-02  7.68641651e-01  7.05655813e-01\n",
      "    0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      "  [-1.65098915e+01 -2.15953960e+01 -2.50071228e-01 -9.68227446e-01\n",
      "   -5.75389527e-02 -5.25586084e-02  7.69015789e-01  7.06588984e-01\n",
      "    0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      "  [-1.65236206e+01 -2.15950394e+01  3.58849764e-01 -9.33395326e-01\n",
      "   -3.58566456e-02 -1.97416879e-02  7.69114196e-01  7.07487524e-01\n",
      "    0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      "  [-1.65324154e+01 -2.16000309e+01  8.02669078e-02 -9.96773422e-01\n",
      "   -3.15285586e-02 -1.66516323e-02  7.68644750e-01  7.07359135e-01\n",
      "    0.00000000e+00  1.00000000e+00  0.00000000e+00]\n",
      "  [-1.65462170e+01 -2.15962543e+01 -4.67927992e-01 -8.83766592e-01\n",
      "   -2.83685848e-02  3.65448766e-03  7.68366396e-01  7.07730770e-01\n",
      "    0.00000000e+00  1.00000000e+00  0.00000000e+00]]]\n",
      "\n",
      "字段名: neighbor_agents_future\n",
      "形状: (32, 80, 3)\n",
      "数据类型: float32\n",
      "前2个元素示例:\n",
      "[[[ 25.2554       6.046562     1.9891763 ]\n",
      "  [ 25.012714     6.58739      1.9837649 ]\n",
      "  [ 24.740587     7.1592493    1.9867651 ]\n",
      "  [ 24.452137     7.726655     1.9936599 ]\n",
      "  [ 24.174107     8.307573     1.9997786 ]\n",
      "  [ 23.889223     8.889684     2.005036  ]\n",
      "  [ 23.60193      9.478575     2.0074124 ]\n",
      "  [ 23.341152    10.064839     1.9925016 ]\n",
      "  [ 23.084648    10.649955     1.9876553 ]\n",
      "  [ 22.808825    11.245274     1.9941766 ]\n",
      "  [ 22.541286    11.843675     1.996164  ]\n",
      "  [ 22.272638    12.450412     1.9968503 ]\n",
      "  [ 22.012312    13.028824     2.0012646 ]\n",
      "  [ 21.763794    13.573987     2.0028043 ]\n",
      "  [ 21.510008    14.119662     2.0117042 ]\n",
      "  [ 21.255535    14.658848     2.014705  ]\n",
      "  [ 21.002468    15.206351     2.0110333 ]\n",
      "  [ 20.751635    15.73648      2.0082533 ]\n",
      "  [ 20.535793    16.21349      2.000009  ]\n",
      "  [ 20.364435    16.652828     1.9657104 ]\n",
      "  [ 20.225718    17.147453     1.9113482 ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]]\n",
      "\n",
      " [[-16.561432   -21.595545    -2.668276  ]\n",
      "  [-16.576187   -21.596107    -2.965668  ]\n",
      "  [-16.591045   -21.590569    -2.8289015 ]\n",
      "  [-16.604082   -21.587915    -2.8643372 ]\n",
      "  [-16.622059   -21.584394    -2.9015183 ]\n",
      "  [-16.634071   -21.584661    -2.660447  ]\n",
      "  [-16.644175   -21.58666     -2.473635  ]\n",
      "  [-16.655207   -21.58714     -2.3795109 ]\n",
      "  [-16.666876   -21.583351    -2.5173178 ]\n",
      "  [-16.676922   -21.571045    -3.0447776 ]\n",
      "  [-16.683142   -21.558712     2.952883  ]\n",
      "  [-16.687754   -21.541405     2.7401454 ]\n",
      "  [-16.69724    -21.529882     2.8703933 ]\n",
      "  [-16.701775   -21.514006     2.4717033 ]\n",
      "  [-16.706709   -21.507544     2.6085706 ]\n",
      "  [-16.70341    -21.501463     2.3968227 ]\n",
      "  [-16.70596    -21.50408      2.5843925 ]\n",
      "  [-16.70458    -21.512075     2.7118673 ]\n",
      "  [-16.71141    -21.519781     2.8040721 ]\n",
      "  [-16.713247   -21.523914     2.9344406 ]\n",
      "  [-16.717726   -21.525585    -2.896095  ]\n",
      "  [-16.716997   -21.52968     -2.395387  ]\n",
      "  [-16.712017   -21.53042     -2.5939426 ]\n",
      "  [-16.714882   -21.529737    -2.5959446 ]\n",
      "  [-16.710419   -21.531975    -2.2986166 ]\n",
      "  [-16.712114   -21.532679    -2.2652209 ]\n",
      "  [-16.71152    -21.534393    -2.25912   ]\n",
      "  [-16.717615   -21.540325    -1.8933945 ]\n",
      "  [-16.720959   -21.550638    -1.6064339 ]\n",
      "  [-16.734526   -21.553684    -1.8888191 ]\n",
      "  [-16.746447   -21.562119    -2.0843718 ]\n",
      "  [-16.746822   -21.56805     -1.8664955 ]\n",
      "  [-16.741638   -21.567104    -1.7205906 ]\n",
      "  [-16.744312   -21.558405    -2.1586962 ]\n",
      "  [-16.742199   -21.546751    -2.3148856 ]\n",
      "  [-16.74499    -21.527235    -2.065189  ]\n",
      "  [-16.747644   -21.508215    -2.5397875 ]\n",
      "  [-16.74534    -21.488663    -2.6031795 ]\n",
      "  [-16.742298   -21.478886    -2.7000651 ]\n",
      "  [-16.74234    -21.479954    -2.6220593 ]\n",
      "  [-16.74462    -21.47184     -2.6274261 ]\n",
      "  [-16.746244   -21.453608    -2.5269334 ]\n",
      "  [-16.746841   -21.437002    -2.4158807 ]\n",
      "  [-16.748833   -21.459234    -2.1940954 ]\n",
      "  [-16.754831   -21.429556    -2.4741952 ]\n",
      "  [-16.758343   -21.408302    -2.5419571 ]\n",
      "  [-16.763968   -21.396893    -2.4973183 ]\n",
      "  [-16.774813   -21.393364    -2.4039574 ]\n",
      "  [-16.7801     -21.399023    -2.4039574 ]\n",
      "  [-16.785389   -21.404682    -2.4039574 ]\n",
      "  [-16.790678   -21.410341    -2.4039574 ]\n",
      "  [-16.795965   -21.416       -2.4039574 ]\n",
      "  [-16.801254   -21.421661    -2.4039574 ]\n",
      "  [-16.806543   -21.42732     -2.4039574 ]\n",
      "  [-16.811832   -21.43298     -2.4039574 ]\n",
      "  [-16.81712    -21.438639    -2.4039574 ]\n",
      "  [-16.822409   -21.444298    -2.4039574 ]\n",
      "  [-16.827694   -21.449953    -2.4039574 ]\n",
      "  [-16.832977   -21.455608    -2.4039574 ]\n",
      "  [-16.838263   -21.461264    -2.4039574 ]\n",
      "  [-16.843546   -21.466917    -2.4039574 ]\n",
      "  [-16.798378   -21.375473     2.2594204 ]\n",
      "  [-16.819324   -21.350338     2.0037968 ]\n",
      "  [-16.822594   -21.34795      2.0037968 ]\n",
      "  [-16.834494   -21.329384     1.776831  ]\n",
      "  [-16.837273   -21.326529     1.776831  ]\n",
      "  [-16.84005    -21.323673     1.776831  ]\n",
      "  [-16.842829   -21.320818     1.776831  ]\n",
      "  [-16.845608   -21.31796      1.776831  ]\n",
      "  [-16.848387   -21.315105     1.776831  ]\n",
      "  [-16.851164   -21.31225      1.776831  ]\n",
      "  [-16.853943   -21.309397     1.776831  ]\n",
      "  [-16.85672    -21.306541     1.776831  ]\n",
      "  [-16.83937    -21.321268     1.0504869 ]\n",
      "  [-16.862637   -21.303377     1.0049682 ]\n",
      "  [-16.870018   -21.3119       0.97291315]\n",
      "  [-16.859444   -21.323273     0.808787  ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]\n",
      "  [  0.           0.           0.        ]]]\n",
      "\n",
      "字段名: static_objects\n",
      "形状: (5, 10)\n",
      "数据类型: float32\n",
      "前2个元素示例:\n",
      "[[-12.495235     4.3897934   -0.5838769   -0.8118422    0.32558724\n",
      "    0.32251382   0.           0.           1.           0.        ]\n",
      " [-26.19704     12.0444565    0.75732315  -0.6530403    0.667786\n",
      "    0.3314729    0.           0.           0.           1.        ]]\n",
      "\n",
      "字段名: lanes\n",
      "形状: (70, 20, 12)\n",
      "数据类型: float32\n",
      "前2个元素示例:\n",
      "[[[-1.34628725e+01  5.38638793e-02  1.13446426e+00 -1.07483789e-02\n",
      "    1.78047466e+00  2.15858340e+00  1.74277782e+00 -1.72280419e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [-1.23284082e+01  4.31155004e-02  1.13451004e+00 -3.45930830e-03\n",
      "    1.50370598e+00  2.20064282e+00  1.43454552e+00 -1.70276928e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [-1.11938982e+01  3.96561921e-02  1.13450813e+00  3.96787375e-03\n",
      "    1.22676277e+00  2.23873639e+00  1.12626743e+00 -1.69002354e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [-1.00593901e+01  4.36240658e-02  1.13445854e+00  1.12841465e-02\n",
      "    9.49634552e-01  2.27374196e+00  8.17991257e-01 -1.68470502e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [-8.92493153e+00  5.49082123e-02  1.13436174e+00  1.86834596e-02\n",
      "    6.72356606e-01  2.30562019e+00  5.09764671e-01 -1.68670261e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [-7.79056978e+00  7.35916719e-02  1.13421631e+00  2.60256976e-02\n",
      "    3.94943714e-01  2.33445501e+00  2.01634884e-01 -1.69609976e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [-6.65635347e+00  9.96173695e-02  1.13402367e+00  3.33958939e-02\n",
      "    1.17434502e-01  2.36013460e+00 -1.06329918e-01 -1.71513319e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [-5.52232981e+00  1.33013263e-01  1.13378239e+00  4.07626927e-02\n",
      "   -1.60155296e-01  2.38278365e+00 -4.14081573e-01 -1.74406111e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [-4.38854742e+00  1.73775956e-01  1.13371038e+00  4.27460372e-02\n",
      "   -4.37788963e-01  2.40226769e+00 -7.21591949e-01 -1.78035569e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [-3.25483704e+00  2.16521993e-01  1.13395643e+00  3.55956405e-02\n",
      "   -7.15582371e-01  2.42298579e+00 -1.02902985e+00 -1.81863379e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [-2.12088060e+00  2.52117634e-01  1.13416266e+00  2.82802880e-02\n",
      "   -9.93399143e-01  2.44776702e+00 -1.33671427e+00 -1.84976137e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [-9.86717880e-01  2.80397922e-01  1.13432348e+00  2.08566785e-02\n",
      "   -1.27114034e+00  2.47572470e+00 -1.64462256e+00 -1.87209678e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 1.47605553e-01  3.01254600e-01  1.13443410e+00  1.35664642e-02\n",
      "   -1.54877746e+00  2.50691319e+00 -1.95276463e+00 -1.87997591e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 1.28203964e+00  3.14821064e-01  1.13449860e+00  6.11415505e-03\n",
      "   -1.82628608e+00  2.54129457e+00 -2.26096511e+00 -1.88447261e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.41653824e+00  3.20935220e-01  1.13451457e+00 -1.17501616e-03\n",
      "   -2.10363483e+00  2.57893443e+00 -2.56919622e+00 -1.88547027e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 3.55105281e+00  3.19760203e-01  1.13448262e+00 -8.60396028e-03\n",
      "   -2.38079834e+00  2.61972237e+00 -2.87742805e+00 -1.88316214e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 4.68553543e+00  3.11156243e-01  1.13440323e+00 -1.59181654e-02\n",
      "   -2.65774965e+00  2.66383171e+00 -3.18563223e+00 -1.87733364e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 5.81993866e+00  2.95238078e-01  1.13427591e+00 -2.33188272e-02\n",
      "   -2.93445706e+00  2.71102691e+00 -3.49377942e+00 -1.86815310e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 6.95421457e+00  2.71919250e-01  1.13410139e+00 -3.06275934e-02\n",
      "   -3.21089745e+00  2.76152349e+00 -3.80184126e+00 -1.85554779e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 8.08831596e+00  2.41291657e-01  0.00000000e+00  0.00000000e+00\n",
      "   -3.48683786e+00  2.80521560e+00 -4.10989094e+00 -1.84314179e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "\n",
      " [[ 8.45951736e-01  7.64460945e+00 -1.44562304e-01 -6.34648323e-01\n",
      "    3.26876044e+00 -3.96824384e+00 -4.42974377e+00 -4.12198305e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 7.01389432e-01  7.00996113e+00 -7.35236406e-02 -6.46670818e-01\n",
      "    3.43692970e+00 -3.36843324e+00 -4.05478382e+00 -3.94206142e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 6.27865791e-01  6.36329031e+00  4.01079655e-04 -6.50890350e-01\n",
      "    3.53406000e+00 -2.75660038e+00 -3.75086236e+00 -3.75011730e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 6.28266871e-01  5.71239996e+00  7.46062994e-02 -6.46551609e-01\n",
      "    3.55726624e+00 -2.14054775e+00 -3.52086568e+00 -3.55395389e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 7.02873170e-01  5.06584835e+00  1.45621300e-01 -6.34393692e-01\n",
      "    3.50626659e+00 -1.52883387e+00 -3.36507440e+00 -3.36212897e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 8.48494470e-01  4.43145466e+00  2.18621075e-01 -6.13088131e-01\n",
      "    3.38425207e+00 -9.29277897e-01 -3.18997192e+00 -3.12071848e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 1.06711555e+00  3.81836653e+00  2.85272479e-01 -5.84981918e-01\n",
      "    3.18923783e+00 -3.51027489e-01 -3.05099964e+00 -2.87082958e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 1.35238802e+00  3.23338461e+00  3.49657655e-01 -5.48998117e-01\n",
      "    2.92757225e+00  1.99116468e-01 -2.95916581e+00 -2.62871027e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 1.70204568e+00  2.68438649e+00  4.10168409e-01 -5.05335331e-01\n",
      "    2.60152125e+00  7.13276863e-01 -2.91400218e+00 -2.40196252e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.11221409e+00  2.17905116e+00  4.63531971e-01 -4.56928968e-01\n",
      "    2.21496010e+00  1.18377447e+00 -2.91258645e+00 -2.19715714e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.57574606e+00  1.72212219e+00  5.13498545e-01 -3.99984837e-01\n",
      "    1.77503490e+00  1.60586572e+00 -2.94879746e+00 -2.01796484e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 3.08924460e+00  1.32213736e+00  5.54804564e-01 -3.40254664e-01\n",
      "    1.28514314e+00  1.97101283e+00 -3.01964355e+00 -1.87059641e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 3.64404917e+00  9.81882691e-01  5.89673042e-01 -2.75584280e-01\n",
      "    7.53945351e-01  2.27642965e+00 -3.11707091e+00 -1.75519037e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 4.23372221e+00  7.06298411e-01  6.17394447e-01 -2.05996752e-01\n",
      "    1.87879086e-01  2.51717615e+00 -3.23745298e+00 -1.67831135e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 4.85111666e+00  5.00301659e-01  6.36396408e-01 -1.36500180e-01\n",
      "   -4.00575638e-01  2.69280648e+00 -3.37499714e+00 -1.64400697e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 5.48751307e+00  3.63801479e-01  6.47942543e-01 -6.19532168e-02\n",
      "   -1.00678444e+00  2.79998660e+00 -3.52159262e+00 -1.64829600e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 6.13545561e+00  3.01848263e-01  6.50917053e-01 -2.11173296e-02\n",
      "   -1.62453985e+00  2.83261967e+00 -3.67182732e+00 -1.69593847e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 6.78637266e+00  2.80730933e-01  6.50971413e-01 -1.97196305e-02\n",
      "   -2.24526930e+00  2.82441664e+00 -3.81943774e+00 -1.75485885e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 7.43734407e+00  2.61011302e-01  6.50971889e-01 -1.97196454e-02\n",
      "   -2.86605310e+00  2.81481624e+00 -3.96472311e+00 -1.79944396e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 8.08831596e+00  2.41291657e-01  0.00000000e+00  0.00000000e+00\n",
      "   -3.48683786e+00  2.80521560e+00 -4.10989094e+00 -1.84314179e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]]\n",
      "\n",
      "字段名: lanes_speed_limit\n",
      "形状: (70, 1)\n",
      "数据类型: float32\n",
      "前2个元素示例:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "字段名: lanes_has_speed_limit\n",
      "形状: (70, 1)\n",
      "数据类型: bool\n",
      "前2个元素示例:\n",
      "[[False]\n",
      " [False]]\n",
      "\n",
      "字段名: route_lanes\n",
      "形状: (25, 20, 12)\n",
      "数据类型: float32\n",
      "前2个元素示例:\n",
      "[[[-1.34628725e+01  5.38638793e-02  1.13446426e+00 -1.07483789e-02\n",
      "    1.78047466e+00  2.15858340e+00  1.74277782e+00 -1.72280419e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [-1.23284082e+01  4.31155004e-02  1.13451004e+00 -3.45930830e-03\n",
      "    1.50370598e+00  2.20064282e+00  1.43454552e+00 -1.70276928e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [-1.11938982e+01  3.96561921e-02  1.13450813e+00  3.96787375e-03\n",
      "    1.22676277e+00  2.23873639e+00  1.12626743e+00 -1.69002354e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [-1.00593901e+01  4.36240658e-02  1.13445854e+00  1.12841465e-02\n",
      "    9.49634552e-01  2.27374196e+00  8.17991257e-01 -1.68470502e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [-8.92493153e+00  5.49082123e-02  1.13436174e+00  1.86834596e-02\n",
      "    6.72356606e-01  2.30562019e+00  5.09764671e-01 -1.68670261e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [-7.79056978e+00  7.35916719e-02  1.13421631e+00  2.60256976e-02\n",
      "    3.94943714e-01  2.33445501e+00  2.01634884e-01 -1.69609976e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [-6.65635347e+00  9.96173695e-02  1.13402367e+00  3.33958939e-02\n",
      "    1.17434502e-01  2.36013460e+00 -1.06329918e-01 -1.71513319e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [-5.52232981e+00  1.33013263e-01  1.13378239e+00  4.07626927e-02\n",
      "   -1.60155296e-01  2.38278365e+00 -4.14081573e-01 -1.74406111e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [-4.38854742e+00  1.73775956e-01  1.13371038e+00  4.27460372e-02\n",
      "   -4.37788963e-01  2.40226769e+00 -7.21591949e-01 -1.78035569e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [-3.25483704e+00  2.16521993e-01  1.13395643e+00  3.55956405e-02\n",
      "   -7.15582371e-01  2.42298579e+00 -1.02902985e+00 -1.81863379e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [-2.12088060e+00  2.52117634e-01  1.13416266e+00  2.82802880e-02\n",
      "   -9.93399143e-01  2.44776702e+00 -1.33671427e+00 -1.84976137e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [-9.86717880e-01  2.80397922e-01  1.13432348e+00  2.08566785e-02\n",
      "   -1.27114034e+00  2.47572470e+00 -1.64462256e+00 -1.87209678e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 1.47605553e-01  3.01254600e-01  1.13443410e+00  1.35664642e-02\n",
      "   -1.54877746e+00  2.50691319e+00 -1.95276463e+00 -1.87997591e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 1.28203964e+00  3.14821064e-01  1.13449860e+00  6.11415505e-03\n",
      "   -1.82628608e+00  2.54129457e+00 -2.26096511e+00 -1.88447261e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.41653824e+00  3.20935220e-01  1.13451457e+00 -1.17501616e-03\n",
      "   -2.10363483e+00  2.57893443e+00 -2.56919622e+00 -1.88547027e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 3.55105281e+00  3.19760203e-01  1.13448262e+00 -8.60396028e-03\n",
      "   -2.38079834e+00  2.61972237e+00 -2.87742805e+00 -1.88316214e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 4.68553543e+00  3.11156243e-01  1.13440323e+00 -1.59181654e-02\n",
      "   -2.65774965e+00  2.66383171e+00 -3.18563223e+00 -1.87733364e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 5.81993866e+00  2.95238078e-01  1.13427591e+00 -2.33188272e-02\n",
      "   -2.93445706e+00  2.71102691e+00 -3.49377942e+00 -1.86815310e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 6.95421457e+00  2.71919250e-01  1.13410139e+00 -3.06275934e-02\n",
      "   -3.21089745e+00  2.76152349e+00 -3.80184126e+00 -1.85554779e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 8.08831596e+00  2.41291657e-01  0.00000000e+00  0.00000000e+00\n",
      "   -3.48683786e+00  2.80521560e+00 -4.10989094e+00 -1.84314179e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "\n",
      " [[ 8.08831596e+00  2.41291657e-01  1.14621544e+00 -3.47171873e-02\n",
      "   -3.48683786e+00  2.80521560e+00 -4.10989094e+00 -1.84314179e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 9.23453140e+00  2.06574470e-01  1.14621639e+00 -3.47171724e-02\n",
      "   -3.35516024e+00  2.21868515e+00 -3.82993269e+00 -1.80193174e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 1.03807478e+01  1.71857297e-01  1.14621544e+00 -3.47171873e-02\n",
      "   -3.02643538e+00  2.09525490e+00 -3.54999113e+00 -1.75279856e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 1.15269632e+01  1.37140110e-01  1.14621639e+00 -3.47030237e-02\n",
      "   -2.69771004e+00  1.97182477e+00 -3.27004814e+00 -1.70366514e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 1.26731796e+01  1.02437086e-01  1.14640617e+00  4.47261333e-03\n",
      "   -2.36898518e+00  1.84838033e+00 -2.99010658e+00 -1.65454602e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 1.38195858e+01  1.06909700e-01  1.14169312e+00  1.03408791e-01\n",
      "   -2.03602886e+00  1.81983006e+00 -2.71038151e+00 -1.64333498e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 1.49612789e+01  2.10318491e-01  1.12858582e+00  2.01099530e-01\n",
      "   -1.69556713e+00  1.77706563e+00 -2.42665672e+00 -1.69860721e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 1.60898647e+01  4.11418021e-01  1.10677338e+00  2.98692107e-01\n",
      "   -1.34199810e+00  1.63661051e+00 -2.12978268e+00 -1.85421491e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 1.71966381e+01  7.10110128e-01  1.07683372e+00  3.93140018e-01\n",
      "   -1.01364899e+00  1.66199827e+00 -1.81033134e+00 -2.15720034e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 1.82734718e+01  1.10325015e+00  1.03887939e+00  4.84650254e-01\n",
      "   -7.15877533e-01  1.81705105e+00 -1.46126747e+00 -2.54947948e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 1.93123512e+01  1.58790040e+00  9.96234894e-01  5.67288160e-01\n",
      "   -4.69026566e-01  2.06137061e+00 -1.08588028e+00 -2.84977627e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.03085861e+01  2.15518856e+00  9.47315216e-01  6.45667791e-01\n",
      "   -2.67505646e-01  2.36814785e+00 -7.18416214e-01 -3.04810262e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.12559013e+01  2.80085635e+00  8.93255234e-01  7.18590021e-01\n",
      "   -2.34098434e-01  2.83542418e+00 -3.63718033e-01 -3.11346507e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.21491566e+01  3.51944637e+00  8.32769394e-01  7.87899971e-01\n",
      "   -2.74353027e-01  3.32640791e+00 -9.38415527e-04 -3.15639758e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.29819260e+01  4.30734634e+00  7.67383575e-01  8.51696968e-01\n",
      "   -3.57204437e-01  3.81567192e+00  3.21807861e-01 -3.11115503e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.37493095e+01  5.15904331e+00  6.97233200e-01  9.10031796e-01\n",
      "   -4.71384048e-01  4.29580975e+00  6.96144104e-01 -3.10809398e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.44465427e+01  6.06907511e+00  6.21921539e-01  9.63064194e-01\n",
      "   -5.88083267e-01  4.75024271e+00  1.02332497e+00 -3.02778459e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.50684643e+01  7.03213930e+00  5.43777466e-01  1.00925398e+00\n",
      "   -6.48698807e-01  5.16027594e+00  1.34613991e+00 -2.92827463e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.56122417e+01  8.04139328e+00  4.73361969e-01  1.04439831e+00\n",
      "   -7.33951569e-01  5.56159019e+00  1.62085152e+00 -2.77063847e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]\n",
      "  [ 2.60856037e+01  9.08579159e+00  0.00000000e+00  0.00000000e+00\n",
      "   -9.05206680e-01  5.96575165e+00  1.88200760e+00 -2.61885595e+00\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  1.00000000e+00]]]\n",
      "\n",
      "字段名: route_lanes_speed_limit\n",
      "形状: (25, 1)\n",
      "数据类型: float32\n",
      "前2个元素示例:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "字段名: route_lanes_has_speed_limit\n",
      "形状: (25, 1)\n",
      "数据类型: bool\n",
      "前2个元素示例:\n",
      "[[False]\n",
      " [False]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 加载NPZ文件\n",
    "npz_path = \"/home/SENSETIME/yanzichen/Diffusion-Planner/preprocess_training_data/sg-one-north_0cc1ed4044a35a0e.npz\"\n",
    "data = np.load(npz_path, allow_pickle=True)\n",
    "\n",
    "# 打印所有字段名\n",
    "print(\"文件包含的字段：\")\n",
    "for key in data.keys():\n",
    "    print(f\"- {key}\")\n",
    "\n",
    "# 打印每个字段的形状和部分内容（示例）\n",
    "print(\"\\n各字段详细信息：\")\n",
    "for key in data.keys():\n",
    "    arr = data[key]\n",
    "    print(f\"\\n字段名: {key}\")\n",
    "    print(f\"形状: {arr.shape}\")\n",
    "    print(f\"数据类型: {arr.dtype}\")\n",
    "    # 打印前2个元素（避免数据量过大）\n",
    "    if arr.size > 0:\n",
    "        print(\"前2个元素示例:\")\n",
    "        print(arr[:2] if arr.ndim > 0 else arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用设备: cuda:0, 数据类型: torch.float32\n",
      "GPU名称: NVIDIA GeForce RTX 4060\n",
      "CUDA版本: 11.1\n",
      "加载模型和配置...\n",
      "模型配置: {\n",
      "  \"future_len\": 80,\n",
      "  \"time_len\": 21,\n",
      "  \"agent_state_dim\": 11,\n",
      "  \"agent_num\": 32,\n",
      "  \"static_objects_state_dim\": 10,\n",
      "  \"static_objects_num\": 5,\n",
      "  \"lane_len\": 20,\n",
      "  \"lane_state_dim\": 12,\n",
      "  \"lane_num\": 70,\n",
      "  \"map_len\": 10,\n",
      "  \"map_state_dim\": 4,\n",
      "  \"map_num\": 5,\n",
      "  \"route_len\": 20,\n",
      "  \"route_state_dim\": 12,\n",
      "  \"route_num\": 25,\n",
      "  \"encoder_drop_path_rate\": 0.1,\n",
      "  \"decoder_drop_path_rate\": 0.1,\n",
      "  \"device\": \"cuda\",\n",
      "  \"encoder_depth\": 3,\n",
      "  \"decoder_depth\": 3,\n",
      "  \"num_heads\": 6,\n",
      "  \"hidden_dim\": 192,\n",
      "  \"diffu...\n",
      "加载NPZ数据...\n",
      "成功加载NPZ文件，包含字段: ['map_name', 'token', 'ego_current_state', 'ego_agent_future', 'neighbor_agents_past', 'neighbor_agents_future', 'static_objects', 'lanes', 'lanes_speed_limit', 'lanes_has_speed_limit', 'route_lanes', 'route_lanes_speed_limit', 'route_lanes_has_speed_limit']\n",
      "\n",
      "解析后的数据维度和类型：\n",
      "ego_current_state: 形状=torch.Size([1, 10]), 类型=torch.float32\n",
      "lanes_has_speed_limit: 形状=torch.Size([1, 70, 1]), 类型=torch.bool\n",
      "lanes: 形状=torch.Size([1, 70, 20, 12]), 类型=torch.float32\n",
      "route_lanes: 形状=torch.Size([1, 25, 20, 12]), 类型=torch.float32\n",
      "\n",
      "开始推理...\n",
      "推理失败：expected scalar type double but found float\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_20095/4136014427.py\", line 169, in main\n",
      "    outputs = model(input_data)\n",
      "  File \"/home/SENSETIME/yanzichen/anaconda3/envs/nuplan/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/SENSETIME/yanzichen/Diffusion-Planner/diffusion_planner/model/diffusion_planner.py\", line 23, in forward\n",
      "    decoder_outputs = self.decoder(encoder_outputs, inputs)\n",
      "  File \"/home/SENSETIME/yanzichen/anaconda3/envs/nuplan/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/SENSETIME/yanzichen/Diffusion-Planner/diffusion_planner/model/diffusion_planner.py\", line 100, in forward\n",
      "    decoder_outputs = self.decoder(encoder_outputs, inputs)\n",
      "  File \"/home/SENSETIME/yanzichen/anaconda3/envs/nuplan/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1051, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/SENSETIME/yanzichen/Diffusion-Planner/diffusion_planner/model/module/decoder.py\", line 113, in forward\n",
      "    x0 = dpm_sampler(\n",
      "  File \"/home/SENSETIME/yanzichen/Diffusion-Planner/diffusion_planner/model/diffusion_utils/sampling.py\", line 37, in dpm_sampler\n",
      "    sample_dpm = dpm_solver.sample(\n",
      "  File \"/home/SENSETIME/yanzichen/Diffusion-Planner/diffusion_planner/model/diffusion_utils/dpm_solver_pytorch.py\", line 1179, in sample\n",
      "    model_prev_list = [self.model_fn(x, t)]\n",
      "  File \"/home/SENSETIME/yanzichen/Diffusion-Planner/diffusion_planner/model/diffusion_utils/dpm_solver_pytorch.py\", line 449, in model_fn\n",
      "    return self.data_prediction_fn(x, t)\n",
      "  File \"/home/SENSETIME/yanzichen/Diffusion-Planner/diffusion_planner/model/diffusion_utils/dpm_solver_pytorch.py\", line 437, in data_prediction_fn\n",
      "    noise = self.noise_prediction_fn(x, t)\n",
      "  File \"/home/SENSETIME/yanzichen/Diffusion-Planner/diffusion_planner/model/diffusion_utils/dpm_solver_pytorch.py\", line 431, in noise_prediction_fn\n",
      "    return self.model(x, t)\n",
      "  File \"/home/SENSETIME/yanzichen/Diffusion-Planner/diffusion_planner/model/diffusion_utils/dpm_solver_pytorch.py\", line 404, in <lambda>\n",
      "    self.model = lambda x, t: model_fn(x, t.expand((x.shape[0])))\n",
      "  File \"/home/SENSETIME/yanzichen/Diffusion-Planner/diffusion_planner/model/diffusion_utils/dpm_solver_pytorch.py\", line 316, in model_fn\n",
      "    cond_grad = cond_grad_fn(x, t_input)\n",
      "  File \"/home/SENSETIME/yanzichen/Diffusion-Planner/diffusion_planner/model/diffusion_utils/dpm_solver_pytorch.py\", line 304, in cond_grad_fn\n",
      "    log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)\n",
      "  File \"/home/SENSETIME/yanzichen/Diffusion-Planner/diffusion_planner/model/guidance/guidance_wrapper.py\", line 46, in __call__\n",
      "    energy += guidance_fn(x_in, t_input, cond, **kwargs)\n",
      "  File \"/home/SENSETIME/yanzichen/Diffusion-Planner/diffusion_planner/model/guidance/collision.py\", line 112, in collision_guidance_fn\n",
      "    distances = batch_signed_distance_rect(ego_bbox, neighbor_bbox)\n",
      "  File \"/home/SENSETIME/yanzichen/Diffusion-Planner/diffusion_planner/model/guidance/collision.py\", line 35, in batch_signed_distance_rect\n",
      "    positive_distance = torch.where(overlap < 0, 1e5, overlap)\n",
      "RuntimeError: expected scalar type double but found float\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# 导入模型和配置类\n",
    "from diffusion_planner.model.diffusion_planner import Diffusion_Planner\n",
    "from diffusion_planner.utils.config import Config\n",
    "\n",
    "def load_model(args_file, ckpt_file, device, dtype=torch.float64):  # 新增dtype参数，默认double\n",
    "    \"\"\"加载模型和配置，并设置模型数据类型\"\"\"\n",
    "    with open(args_file, 'r') as f:\n",
    "        args = json.load(f)\n",
    "    \n",
    "    guidance_fn = None\n",
    "    config = Config(args_file, guidance_fn)\n",
    "    model = Diffusion_Planner(config)\n",
    "\n",
    "    # 加载并处理权重\n",
    "    ckpt = torch.load(ckpt_file, map_location=device)\n",
    "    if 'model' in ckpt:\n",
    "        state_dict = ckpt['model']\n",
    "    elif 'state_dict' in ckpt:\n",
    "        state_dict = ckpt['state_dict']\n",
    "    else:\n",
    "        state_dict = ckpt\n",
    "    \n",
    "    new_state_dict = {k[len('module.'): ] if k.startswith('module.') else k: v for k, v in state_dict.items()}\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    \n",
    "    # 转换模型到目标设备和数据类型\n",
    "    model = model.to(device, dtype=dtype).eval()\n",
    "    return model, args\n",
    "\n",
    "def load_npz_data(npz_path, config, device, dtype=torch.float64):  # 新增dtype参数\n",
    "    \"\"\"加载NPZ数据，转换为模型所需的输入格式和类型\"\"\"\n",
    "    data = np.load(npz_path, allow_pickle=True)\n",
    "    print(f\"成功加载NPZ文件，包含字段: {list(data.keys())}\")\n",
    "\n",
    "    # 提取自车历史状态\n",
    "    ego_agent_past = data.get(\n",
    "        \"ego_agent_past\", \n",
    "        np.zeros((config[\"time_len\"], config[\"agent_state_dim\"]), dtype=np.float64)  # 用64位浮点\n",
    "    )\n",
    "\n",
    "    # 解析核心字段\n",
    "    parsed_data = {\n",
    "        \"neighbor_agents_past\": data.get(\n",
    "            \"neighbor_agents_past\", \n",
    "            np.zeros((3, config[\"time_len\"], config[\"agent_state_dim\"]), dtype=np.float64)\n",
    "        ),\n",
    "        \"static_objects\": data.get(\n",
    "            \"static_objects\", \n",
    "            np.zeros((config[\"static_objects_num\"], config[\"static_objects_state_dim\"]), dtype=np.float64)\n",
    "        ),\n",
    "        \"lanes\": data.get(\n",
    "            \"lanes\", \n",
    "            np.zeros((config[\"lane_num\"], config[\"lane_len\"], config[\"lane_state_dim\"]), dtype=np.float64)\n",
    "        ),\n",
    "        \"lanes_speed_limit\": data.get(\n",
    "            \"lanes_speed_limit\", \n",
    "            np.full(config[\"lane_num\"], 30.0, dtype=np.float64)\n",
    "        ),\n",
    "        \"lanes_has_speed_limit\": data.get(\n",
    "            \"lanes_has_speed_limit\", \n",
    "            np.zeros(config[\"lane_num\"], dtype=bool)\n",
    "        ),\n",
    "        \"ego_agent_past\": ego_agent_past,\n",
    "        \"ego_current_state\": data.get(\n",
    "            \"ego_current_state\", \n",
    "            ego_agent_past[-1:] if len(ego_agent_past) > 0 else \n",
    "            np.zeros((1, config[\"agent_state_dim\"]), dtype=np.float64)\n",
    "        ),\n",
    "        \"start\": data.get(\n",
    "            \"start\", \n",
    "            np.array([[0.0, 0.0]], dtype=np.float64)\n",
    "        ),\n",
    "        \"goal\": data.get(\n",
    "            \"goal\", \n",
    "            np.array([[10.0, 10.0]], dtype=np.float64)\n",
    "        ),\n",
    "        \"route_lanes\": data.get(\n",
    "            \"route_lanes\", \n",
    "            np.zeros((config[\"route_num\"], config[\"route_len\"], 2), dtype=np.float64)\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # 适配模型维度要求\n",
    "    batch_data = {}\n",
    "    for key, value in parsed_data.items():\n",
    "        # 转换为torch张量（注意数据类型）\n",
    "        if key == \"lanes_has_speed_limit\":\n",
    "            tensor = torch.tensor(value, dtype=torch.bool, device=device)\n",
    "        else:\n",
    "            tensor = torch.tensor(value, dtype=dtype, device=device)  # 使用指定的dtype\n",
    "        \n",
    "        # 补充batch维度\n",
    "        if len(tensor.shape) >= 1:\n",
    "            tensor = tensor.unsqueeze(0)  # [batch=1, ...]\n",
    "        \n",
    "        # 坐标字段扩展为3D（x, y, z）\n",
    "        if key in [\"lanes\", \"start\", \"goal\", \"route_lanes\"] and tensor.shape[-1] == 2:\n",
    "            z = torch.zeros_like(tensor[..., :1], dtype=dtype)  # 确保z的类型一致\n",
    "            tensor = torch.cat([tensor, z], dim=-1)\n",
    "        \n",
    "        batch_data[key] = tensor\n",
    "\n",
    "    # 确保邻居代理数量符合要求\n",
    "    num_neighbors = batch_data[\"neighbor_agents_past\"].shape[1]\n",
    "    max_agents = config[\"agent_num\"]\n",
    "    if num_neighbors > max_agents:\n",
    "        batch_data[\"neighbor_agents_past\"] = batch_data[\"neighbor_agents_past\"][:, :max_agents, ...]\n",
    "    elif num_neighbors < max_agents:\n",
    "        pad = torch.zeros(\n",
    "            1, max_agents - num_neighbors, *batch_data[\"neighbor_agents_past\"].shape[2:], \n",
    "            device=device, dtype=dtype  # 确保pad的类型一致\n",
    "        )\n",
    "        batch_data[\"neighbor_agents_past\"] = torch.cat([batch_data[\"neighbor_agents_past\"], pad], dim=1)\n",
    "\n",
    "    # 打印关键字段信息（包含数据类型）\n",
    "    print(\"\\n解析后的数据维度和类型：\")\n",
    "    for key in [\"ego_current_state\", \"lanes_has_speed_limit\", \"lanes\", \"route_lanes\"]:\n",
    "        print(f\"{key}: 形状={batch_data[key].shape}, 类型={batch_data[key].dtype}\")\n",
    "\n",
    "    return batch_data\n",
    "\n",
    "def main():\n",
    "    # 设备配置（4090 GPU）\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dtype = torch.float32  # 改为float32\n",
    "    print(f\"使用设备: {device}, 数据类型: {dtype}\")\n",
    "\n",
    "    # 验证GPU是否可用\n",
    "    if device.type == \"cuda\":\n",
    "        print(f\"GPU名称: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"CUDA版本: {torch.version.cuda}\")\n",
    "    else:\n",
    "        print(\"警告: 未检测到可用GPU，将使用CPU进行推理\")\n",
    "\n",
    "    # 文件路径\n",
    "    npz_path = \"/home/SENSETIME/yanzichen/Diffusion-Planner/preprocess_training_data/sg-one-north_0cc1ed4044a35a0e.npz\"\n",
    "    args_file = \"/home/SENSETIME/yanzichen/Diffusion-Planner/checkpoints/args.json\"\n",
    "    ckpt_file = \"/home/SENSETIME/yanzichen/Diffusion-Planner/checkpoints/model.pth\"\n",
    "    \n",
    "    # 验证文件存在性\n",
    "    if not Path(npz_path).exists():\n",
    "        print(f\"错误：NPZ文件不存在 - {npz_path}\")\n",
    "        return\n",
    "    if not Path(args_file).exists():\n",
    "        print(f\"错误：配置文件不存在 - {args_file}\")\n",
    "        return\n",
    "    if not Path(ckpt_file).exists():\n",
    "        print(f\"错误：模型权重文件不存在 - {ckpt_file}\")\n",
    "        return\n",
    "    \n",
    "    # 加载模型和配置（指定dtype）\n",
    "    print(\"加载模型和配置...\")\n",
    "    model, config = load_model(args_file, ckpt_file, device, dtype)\n",
    "    print(f\"模型配置: {json.dumps(config, indent=2)[:500]}...\")\n",
    "    \n",
    "    # 加载并解析NPZ数据（指定dtype）\n",
    "    print(\"加载NPZ数据...\")\n",
    "    input_data = load_npz_data(npz_path, config, device, dtype)\n",
    "    \n",
    "    # 推理\n",
    "    print(\"\\n开始推理...\")\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            outputs = model(input_data)\n",
    "            # 处理模型输出\n",
    "            if isinstance(outputs, tuple):\n",
    "                decoder_outputs = outputs[-1]\n",
    "            else:\n",
    "                decoder_outputs = outputs\n",
    "        except KeyError as e:\n",
    "            print(f\"推理失败：输入数据缺少字段 - {e}\")\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"推理失败：{str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return\n",
    "    \n",
    "    # 处理输出（转换为float32方便后续处理）\n",
    "    print(\"\\n推理成功！\")\n",
    "    future_len = config[\"future_len\"]\n",
    "    plan = decoder_outputs.cpu().numpy().astype(np.float32)  # 转为float32\n",
    "    \n",
    "    print(f\"规划路径形状: {plan.shape}\")\n",
    "    if plan.ndim >= 2 and plan.shape[1] == future_len:\n",
    "        print(f\"规划路径长度: {future_len}（与配置一致）\")\n",
    "        print(\"\\n前10步规划路径 (x, y):\")\n",
    "        for i in range(min(10, future_len)):\n",
    "            x, y = plan[0, i, :2]\n",
    "            print(f\"第{i}步: ({x:.2f}, {y:.2f})\")\n",
    "    else:\n",
    "        print(\"输出维度说明：\")\n",
    "        print(\"可能需要根据模型实际输出调整解析逻辑\")\n",
    "        print(f\"输出维度: {plan.shape}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# LoRA组件\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank=8, alpha=32.0):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.A = nn.Parameter(torch.zeros(in_dim, rank))\n",
    "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.B)  # Explicit zero initialization\n",
    "        self.scaling = self.alpha / self.rank\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (x @ self.A @ self.B) * self.scaling\n",
    "\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, linear, rank=8, alpha=32.0):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(linear.in_features, linear.out_features, rank, alpha)\n",
    "        \n",
    "        # Freeze the original weights\n",
    "        for param in self.linear.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)\n",
    "\n",
    "def apply_lora_to_model(model, rank=8, alpha=32.0):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            setattr(model, name, LoRALinear(module, rank, alpha))\n",
    "        else:\n",
    "            apply_lora_to_model(module, rank, alpha)\n",
    "    return model\n",
    "\n",
    "# MMoE组件\n",
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            LoRALinear(nn.Linear(input_dim, hidden_dim)),\n",
    "            nn.GELU(),\n",
    "            LoRALinear(nn.Linear(hidden_dim, hidden_dim)),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class GateNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_experts):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(input_dim, num_experts)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.gate(x))\n",
    "\n",
    "class MMoE6(nn.Module):\n",
    "    def __init__(self, feature_dim, hidden_dim=256, num_experts=6, num_tasks=6):\n",
    "        super().__init__()\n",
    "        self.experts = nn.ModuleList([Expert(feature_dim, hidden_dim) for _ in range(num_experts)])\n",
    "        self.gates = nn.ModuleList([GateNetwork(feature_dim, num_experts) for _ in range(num_tasks)])\n",
    "        self.num_experts = num_experts\n",
    "        self.num_tasks = num_tasks\n",
    "\n",
    "    def forward(self, x):\n",
    "        expert_outputs = [self.experts[i](x) for i in range(self.num_experts)]\n",
    "        expert_outputs = torch.stack(expert_outputs, dim=1)  # [batch_size, num_experts, hidden_dim]\n",
    "        \n",
    "        task_outputs = []\n",
    "        for i in range(self.num_tasks):\n",
    "            gates = self.gates[i](x).unsqueeze(2)  # [batch_size, num_experts, 1]\n",
    "            task_output = torch.sum(expert_outputs * gates, dim=1)  # [batch_size, hidden_dim]\n",
    "            task_outputs.append(task_output)\n",
    "            \n",
    "        return task_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lora_layer():\n",
    "    # Test parameters\n",
    "    batch_size = 4\n",
    "    in_dim = 128\n",
    "    out_dim = 256\n",
    "    rank = 8\n",
    "    \n",
    "    # Create layer\n",
    "    lora_layer = LoRALayer(in_dim, out_dim, rank)\n",
    "    \n",
    "    # Test initialization\n",
    "    assert lora_layer.A.shape == (in_dim, rank), \"A matrix has wrong shape\"\n",
    "    assert lora_layer.B.shape == (rank, out_dim), \"B matrix has wrong shape\"\n",
    "    assert not torch.allclose(lora_layer.A, torch.zeros_like(lora_layer.A)), \"A matrix not initialized\"\n",
    "    assert torch.allclose(lora_layer.B, torch.zeros_like(lora_layer.B)), \"B matrix should be zero-initialized\"\n",
    "    \n",
    "    # Test forward pass\n",
    "    x = torch.randn(batch_size, in_dim)\n",
    "    output = lora_layer(x)\n",
    "    assert output.shape == (batch_size, out_dim), \"Output shape incorrect\"\n",
    "    \n",
    "    print(\"LoRALayer test passed!\")\n",
    "\n",
    "def test_lora_linear():\n",
    "    # Test parameters\n",
    "    batch_size = 4\n",
    "    in_dim = 128\n",
    "    out_dim = 256\n",
    "    \n",
    "    # Create original linear layer and LoRA wrapper\n",
    "    linear = nn.Linear(in_dim, out_dim)\n",
    "    original_weight = linear.weight.detach().clone()\n",
    "    original_bias = linear.bias.detach().clone()\n",
    "    \n",
    "    lora_linear = LoRALinear(linear)\n",
    "    \n",
    "    # Verify original weights are frozen\n",
    "    assert not linear.weight.requires_grad, \"Original weights should be frozen\"\n",
    "    assert not linear.bias.requires_grad, \"Original bias should be frozen\"\n",
    "    \n",
    "    # Test forward pass\n",
    "    x = torch.randn(batch_size, in_dim)\n",
    "    original_output = linear(x)\n",
    "    lora_output = lora_linear(x)\n",
    "    \n",
    "    assert lora_output.shape == original_output.shape, \"Output shapes should match\"\n",
    "    \n",
    "    # After one forward pass, outputs should be different because:\n",
    "    # 1. A is randomly initialized (non-zero)\n",
    "    # 2. B is zero, but A@B is not exactly zero due to numerical computation\n",
    "    # So we'll check if weights are being properly applied\n",
    "    \n",
    "    # Verify the original linear layer weights haven't changed\n",
    "    assert torch.allclose(linear.weight, original_weight), \"Original weights should not change\"\n",
    "    assert torch.allclose(linear.bias, original_bias), \"Original bias should not change\"\n",
    "    \n",
    "    # Verify LoRA parameters are trainable\n",
    "    assert lora_linear.lora.A.requires_grad, \"LoRA A should be trainable\"\n",
    "    assert lora_linear.lora.B.requires_grad, \"LoRA B should be trainable\"\n",
    "    \n",
    "    print(\"LoRALinear test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_expert():\n",
    "    # Test parameters\n",
    "    batch_size = 4\n",
    "    input_dim = 128\n",
    "    hidden_dim = 256\n",
    "    \n",
    "    # Create expert with LoRA\n",
    "    expert = Expert(input_dim, hidden_dim)\n",
    "    \n",
    "    # Verify architecture\n",
    "    assert len(expert.fc) == 4, \"Expert should have 4 layers\"\n",
    "    assert isinstance(expert.fc[0], LoRALinear), \"First layer should be LoRALinear\"\n",
    "    assert isinstance(expert.fc[2], LoRALinear), \"Third layer should be LoRALinear\"\n",
    "    \n",
    "    # Test forward pass\n",
    "    x = torch.randn(batch_size, input_dim)\n",
    "    output = expert(x)\n",
    "    assert output.shape == (batch_size, hidden_dim), \"Output shape incorrect\"\n",
    "    \n",
    "    print(\"Expert test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_gate_network():\n",
    "    # Test parameters\n",
    "    batch_size = 4\n",
    "    input_dim = 128\n",
    "    num_experts = 6\n",
    "    \n",
    "    # Create gate network\n",
    "    gate = GateNetwork(input_dim, num_experts)\n",
    "    \n",
    "    # Test forward pass\n",
    "    x = torch.randn(batch_size, input_dim)\n",
    "    output = gate(x)\n",
    "    \n",
    "    assert output.shape == (batch_size, num_experts), \"Output shape incorrect\"\n",
    "    assert torch.allclose(output.sum(dim=1), torch.ones(batch_size)), \"Output should sum to 1\"\n",
    "    \n",
    "    print(\"GateNetwork test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mmoe6_integration():\n",
    "    # Test parameters\n",
    "    batch_size = 4\n",
    "    feature_dim = 128\n",
    "    hidden_dim = 256\n",
    "    num_experts = 6\n",
    "    num_tasks = 6\n",
    "    \n",
    "    # Create and apply LoRA\n",
    "    model = MMoE6(feature_dim, hidden_dim, num_experts, num_tasks)\n",
    "    model = apply_lora_to_model(model)\n",
    "    \n",
    "    # Verify architecture\n",
    "    assert len(model.experts) == num_experts, \"Wrong number of experts\"\n",
    "    assert len(model.gates) == num_tasks, \"Wrong number of gates\"\n",
    "    \n",
    "    # Check LoRA application\n",
    "    for expert in model.experts:\n",
    "        assert isinstance(expert.fc[0], LoRALinear), \"Experts should have LoRA layers\"\n",
    "        assert isinstance(expert.fc[2], LoRALinear), \"Experts should have LoRA layers\"\n",
    "    \n",
    "    # Test forward pass\n",
    "    x = torch.randn(batch_size, feature_dim)\n",
    "    task_outputs = model(x)\n",
    "    \n",
    "    # Verify outputs\n",
    "    assert len(task_outputs) == num_tasks, \"Number of task outputs should match num_tasks\"\n",
    "    for output in task_outputs:\n",
    "        assert output.shape == (batch_size, hidden_dim), f\"Output shape should be ({batch_size}, {hidden_dim})\"\n",
    "    \n",
    "    print(\"MMoE6 integration test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running component tests...\n",
      "LoRALayer test passed!\n",
      "LoRALinear test passed!\n",
      "Expert test passed!\n",
      "GateNetwork test passed!\n",
      "MMoE6 integration test passed!\n",
      "All tests passed successfully!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Running component tests...\")\n",
    "    test_lora_layer()\n",
    "    test_lora_linear()\n",
    "    test_expert()\n",
    "    test_gate_network()\n",
    "    test_mmoe6_integration()\n",
    "    print(\"All tests passed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed! All outputs have the correct shape.\n"
     ]
    }
   ],
   "source": [
    "def test_mmoe6_with_lora():\n",
    "    # Test parameters\n",
    "    batch_size = 4\n",
    "    feature_dim = 128\n",
    "    hidden_dim = 256\n",
    "    num_experts = 6\n",
    "    num_tasks = 6\n",
    "    \n",
    "    # Create model\n",
    "    model = MMoE6(feature_dim, hidden_dim, num_experts, num_tasks)\n",
    "    \n",
    "    # Apply LoRA to all linear layers (including those inside experts)\n",
    "    model = apply_lora_to_model(model, rank=8, alpha=32.0)\n",
    "    \n",
    "    # Create test input\n",
    "    x = torch.randn(batch_size, feature_dim)\n",
    "    \n",
    "    # Forward pass\n",
    "    task_outputs = model(x)\n",
    "    \n",
    "    # Verify outputs\n",
    "    assert len(task_outputs) == num_tasks, \"Number of task outputs should match num_tasks\"\n",
    "    for output in task_outputs:\n",
    "        assert output.shape == (batch_size, hidden_dim), f\"Output shape should be ({batch_size}, {hidden_dim})\"\n",
    "    \n",
    "    print(\"Test passed! All outputs have the correct shape.\")\n",
    "\n",
    "# Run the test\n",
    "test_mmoe6_with_lora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprepare_datasets\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mTuple\u001b[49m[DataLoader, DataLoader]:\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"准备混合数据集（20% nuPlan + 80% 实车数据）\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m# 导入必要的模块\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tuple' is not defined"
     ]
    }
   ],
   "source": [
    "def prepare_datasets() -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"准备混合数据集（20% nuPlan + 80% 实车数据）\"\"\"\n",
    "    # 导入必要的模块\n",
    "    from torch.utils.data import Dataset, DataLoader, ConcatDataset, Subset\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    \n",
    "    class SimulatedDataset(Dataset):\n",
    "        def __init__(self, size=1000, feature_dim=512, num_classes=6):\n",
    "            self.size = size\n",
    "            self.feature_dim = feature_dim\n",
    "            self.num_classes = num_classes\n",
    "            \n",
    "        def __len__(self):\n",
    "            return self.size\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            # 生成随机特征和标签\n",
    "            features = torch.randn(self.feature_dim)\n",
    "            trajectory = torch.randn(2)  # 假设输出是二维轨迹点\n",
    "            scene_label = torch.randint(0, self.num_classes, (1,)).long().squeeze()\n",
    "            \n",
    "            return features, trajectory, scene_label\n",
    "\n",
    "    # 创建模拟数据集\n",
    "    nuplan_dataset = SimulatedDataset(size=1000, feature_dim=512, num_classes=6)\n",
    "    realworld_dataset = SimulatedDataset(size=4000, feature_dim=512, num_classes=6)\n",
    "\n",
    "    # 计算混合比例\n",
    "    nuplan_size = len(nuplan_dataset)\n",
    "    realworld_size = len(realworld_dataset)\n",
    "    total_size = nuplan_size + realworld_size\n",
    "\n",
    "    # 按20:80比例采样\n",
    "    nuplan_samples = int(total_size * 0.2)\n",
    "    realworld_samples = int(total_size * 0.8)\n",
    "\n",
    "    # 创建采样器\n",
    "    nuplan_indices = torch.randperm(len(nuplan_dataset))[:nuplan_samples]\n",
    "    realworld_indices = torch.randperm(len(realworld_dataset))[:realworld_samples]\n",
    "\n",
    "    nuplan_subset = Subset(nuplan_dataset, nuplan_indices)\n",
    "    realworld_subset = Subset(realworld_dataset, realworld_indices)\n",
    "\n",
    "    # 合并数据集\n",
    "    mixed_dataset = ConcatDataset([nuplan_subset, realworld_subset])\n",
    "\n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(\n",
    "        mixed_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=0,  # 使用0个worker以避免潜在问题\n",
    "        pin_memory=False  # 如果没有GPU可以设置为False\n",
    "    )\n",
    "\n",
    "    # 准备验证集\n",
    "    val_dataset = SimulatedDataset(size=1000, feature_dim=512, num_classes=6)\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=False\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 121\u001b[0m\n\u001b[1;32m    118\u001b[0m     enhanced_model \u001b[38;5;241m=\u001b[39m DiffusionPlannerWithMMoE(base_model, feature_dim)\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m enhanced_model\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprepare_datasets\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mTuple\u001b[49m[DataLoader, DataLoader]:\n\u001b[1;32m    122\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"准备混合数据集（20% nuPlan + 80% 实车数据）\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m# 导入数据集类（根据实际项目结构调整）\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tuple' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "from diffusion_planner.utils.data_augmentation import StatePerturbation   \n",
    "from diffusion_planner.utils.train_utils import get_epoch_mean_loss\n",
    "from diffusion_planner.utils import ddp\n",
    "from diffusion_planner.loss import diffusion_loss_func\n",
    "\n",
    "\n",
    "# 配置参数\n",
    "BRANCH_NAME = \"diffusion_planner_release\"\n",
    "ARGS_FILE = \"/home/SENSETIME/yanzichen/Diffusion-Planner/checkpoints/args.json\"\n",
    "CKPT_FILE = \"/home/SENSETIME/yanzichen/Diffusion-Planner/checkpoints/model.pth\"\n",
    "OUTPUT_DIR = f\"/home/SENSETIME/yanzichen/Diffusion-Planner/checkpoints/{BRANCH_NAME}\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 创建输出目录\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def load_base_model() -> nn.Module:\n",
    "    \"\"\"加载预训练的Diffusion Planner基模型\"\"\"\n",
    "    # 加载配置参数\n",
    "    with open(ARGS_FILE, 'r') as f:\n",
    "        args = json.load(f)\n",
    "    \n",
    "    # 导入模型类（根据实际项目结构调整）\n",
    "    from diffusion_planner.model.diffusion_planner import DiffusionPlanner\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = DiffusionPlanner(\n",
    "        input_dim=args['input_dim'],\n",
    "        hidden_dim=args['hidden_dim'],\n",
    "        num_layers=args['num_layers'],\n",
    "        diffusion_steps=args['diffusion_steps']\n",
    "    )\n",
    "    \n",
    "    # 加载预训练权重\n",
    "    checkpoint = torch.load(CKPT_FILE, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded base model from {CKPT_FILE}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def setup_mmoe_lora(base_model: nn.Module, feature_dim: int) -> nn.Module:\n",
    "    \"\"\"配置MMoE-6专家网络和LoRA适配器\"\"\"\n",
    "    # 冻结基模型参数\n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # 应用LoRA到基模型的线性层\n",
    "    base_model = apply_lora_to_model(base_model, rank=10, alpha=32.0)\n",
    "    \n",
    "    # 添加MMoE-6专家网络\n",
    "    class DiffusionPlannerWithMMoE(nn.Module):\n",
    "        def __init__(self, base_model, feature_dim):\n",
    "            super().__init__()\n",
    "            self.base_model = base_model\n",
    "            self.mmoe = MMoE6(feature_dim=feature_dim, hidden_dim=256, num_experts=6, num_tasks=6)\n",
    "            \n",
    "            # 场景分类器\n",
    "            self.scene_classifier = nn.Sequential(\n",
    "                nn.Linear(feature_dim, 256),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(256, 6),  # 6类场景\n",
    "                nn.Softmax(dim=1)\n",
    "            )\n",
    "            \n",
    "            # 任务特定输出头\n",
    "            self.task_heads = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    LoRALinear(nn.Linear(256, 256), rank=10),\n",
    "                    nn.GELU(),\n",
    "                    LoRALinear(nn.Linear(256, 2), rank=10)  # 输出轨迹点(x,y)\n",
    "                ) for _ in range(6)  # 6类场景\n",
    "            ])\n",
    "            \n",
    "            # 场景名称映射\n",
    "            self.scene_names = [\"普通巡航\", \"坡道\", \"弯道\", \"绕障\", \"会车\", \"横穿\"]\n",
    "        \n",
    "        def forward(self, x, t=None):\n",
    "            # 提取特征（根据实际模型调整）\n",
    "            if hasattr(self.base_model, 'extract_features'):\n",
    "                features = self.base_model.extract_features(x, t)\n",
    "            else:\n",
    "                # 如果模型没有extract_features方法，使用forward的中间输出\n",
    "                with torch.no_grad():\n",
    "                    base_output = self.base_model(x, t)\n",
    "                features = base_output['features'] if isinstance(base_output, dict) else base_output\n",
    "            \n",
    "            # 场景分类\n",
    "            scene_probs = self.scene_classifier(features)\n",
    "            scene_id = torch.argmax(scene_probs, dim=1)\n",
    "            \n",
    "            # MMoE处理\n",
    "            mmoe_outputs = self.mmoe(features)\n",
    "            \n",
    "            # 根据场景选择输出头\n",
    "            batch_size = x.size(0)\n",
    "            outputs = torch.zeros(batch_size, 2, device=x.device)\n",
    "            \n",
    "            for i in range(6):\n",
    "                mask = (scene_id == i).unsqueeze(1).expand(-1, 2)\n",
    "                if mask.any():\n",
    "                    outputs[mask] = self.task_heads[i](mmoe_outputs[i])[mask]\n",
    "                    \n",
    "            return {\n",
    "                \"trajectory\": outputs,\n",
    "                \"scene_id\": scene_id,\n",
    "                \"scene_probs\": scene_probs,\n",
    "                \"features\": features\n",
    "            }\n",
    "    \n",
    "    # 初始化增强模型\n",
    "    enhanced_model = DiffusionPlannerWithMMoE(base_model, feature_dim)\n",
    "    return enhanced_model\n",
    "\n",
    "def prepare_datasets() -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"准备混合数据集（20% nuPlan + 80% 实车数据）\"\"\"\n",
    "    # 导入数据集类（根据实际项目结构调整）\n",
    "    from diffusion_planner.data.nuplan_dataset import NuPlanDataset\n",
    "    from diffusion_planner.data.real_world_dataset import RealWorldDataset\n",
    "    \n",
    "    # 创建数据集\n",
    "    nuplan_dataset = NuPlanDataset(\n",
    "        data_dir=\"/path/to/nuplan\",\n",
    "        split=\"train\",\n",
    "        scene_types=[\"cruise\", \"ramp\", \"curve\", \"obstacle\", \"meeting\", \"crossing\"]\n",
    "    )\n",
    "    \n",
    "    realworld_dataset = RealWorldDataset(\n",
    "        data_dir=\"/path/to/real_world_data\",\n",
    "        split=\"train\",\n",
    "        scene_types=[\"cruise\", \"ramp\", \"curve\", \"obstacle\", \"meeting\", \"crossing\"]\n",
    "    )\n",
    "    \n",
    "    # 计算混合比例\n",
    "    nuplan_size = len(nuplan_dataset)\n",
    "    realworld_size = len(realworld_dataset)\n",
    "    total_size = nuplan_size + realworld_size\n",
    "    \n",
    "    # 按20:80比例采样\n",
    "    nuplan_samples = int(total_size * 0.2)\n",
    "    realworld_samples = int(total_size * 0.8)\n",
    "    \n",
    "    # 创建采样器\n",
    "    nuplan_indices = torch.randperm(len(nuplan_dataset))[:nuplan_samples]\n",
    "    realworld_indices = torch.randperm(len(realworld_dataset))[:realworld_samples]\n",
    "    \n",
    "    nuplan_subset = torch.utils.data.Subset(nuplan_dataset, nuplan_indices)\n",
    "    realworld_subset = torch.utils.data.Subset(realworld_dataset, realworld_indices)\n",
    "    \n",
    "    # 合并数据集\n",
    "    mixed_dataset = ConcatDataset([nuplan_subset, realworld_subset])\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(\n",
    "        mixed_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # 准备验证集\n",
    "    val_dataset = RealWorldDataset(\n",
    "        data_dir=\"/path/to/real_world_data\",\n",
    "        split=\"val\",\n",
    "        scene_types=[\"cruise\", \"ramp\", \"curve\", \"obstacle\", \"meeting\", \"crossing\"]\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    num_epochs: int = 50,\n",
    "    lr: float = 1e-4\n",
    ") -> None:\n",
    "    \"\"\"训练增强模型\"\"\"\n",
    "    # 定义优化器（只优化LoRA和MMoE参数）\n",
    "    optimizer = optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=lr,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "    \n",
    "    # 定义损失函数\n",
    "    criterion = diffusion_loss_func  # 使用项目中的扩散损失函数\n",
    "    \n",
    "    # 学习率调度器\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # 训练循环\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (inputs, targets, scene_labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "            scene_labels = scene_labels.to(DEVICE)\n",
    "            \n",
    "            # 前向传播\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            trajectory = outputs[\"trajectory\"]\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = criterion(outputs, targets)  # 使用项目中的损失计算方式\n",
    "            \n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        scene_correct = 0\n",
    "        scene_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, scene_labels in val_loader:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                targets = targets.to(DEVICE)\n",
    "                scene_labels = scene_labels.to(DEVICE)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                trajectory = outputs[\"trajectory\"]\n",
    "                scene_preds = outputs[\"scene_id\"]\n",
    "                \n",
    "                # 计算损失\n",
    "                val_loss += criterion(outputs, targets).item()\n",
    "                \n",
    "                # 计算场景分类准确率\n",
    "                scene_total += scene_labels.size(0)\n",
    "                scene_correct += (scene_preds == scene_labels).sum().item()\n",
    "        \n",
    "        # 计算平均损失和准确率\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        scene_acc = 100.0 * scene_correct / scene_total\n",
    "        \n",
    "        # 打印统计信息\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Scene Acc: {scene_acc:.2f}%\")\n",
    "        \n",
    "        # 调整学习率\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'scene_acc': scene_acc\n",
    "            }, f\"{OUTPUT_DIR}/best_model.pth\")\n",
    "            print(f\"Saved best model with val loss: {val_loss:.4f}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    # 设置随机种子\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # 初始化分布式训练（如果需要）\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        ddp.init_distributed_mode()\n",
    "    \n",
    "    # 加载基模型\n",
    "    base_model = load_base_model()\n",
    "    base_model = base_model.to(DEVICE)\n",
    "    \n",
    "    # 设置MMoE和LoRA\n",
    "    feature_dim = 512  # 根据实际模型调整\n",
    "    enhanced_model = setup_mmoe_lora(base_model, feature_dim)\n",
    "    enhanced_model = enhanced_model.to(DEVICE)\n",
    "    \n",
    "    # 准备数据集\n",
    "    train_loader, val_loader = prepare_datasets()\n",
    "    \n",
    "    # 训练模型\n",
    "    train_model(enhanced_model, train_loader, val_loader, num_epochs=50)\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[9], line 121\u001b[0m\n",
      "\u001b[1;32m    118\u001b[0m     enhanced_model \u001b[38;5;241m=\u001b[39m DiffusionPlannerWithMMoE(base_model, feature_dim)\n",
      "\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m enhanced_model\n",
      "\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprepare_datasets\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mTuple\u001b[49m[DataLoader, DataLoader]:\n",
      "\u001b[1;32m    122\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"准备混合数据集（20% nuPlan + 80% 实车数据）\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m# 导入数据集类（根据实际项目结构调整）\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tuple' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "from diffusion_planner.utils.data_augmentation import StatePerturbation   \n",
    "from diffusion_planner.utils.train_utils import get_epoch_mean_loss\n",
    "from diffusion_planner.utils import ddp\n",
    "from diffusion_planner.loss import diffusion_loss_func\n",
    "\n",
    "\n",
    "# 配置参数\n",
    "BRANCH_NAME = \"diffusion_planner_release\"\n",
    "ARGS_FILE = \"/home/SENSETIME/yanzichen/Diffusion-Planner/checkpoints/args.json\"\n",
    "CKPT_FILE = \"/home/SENSETIME/yanzichen/Diffusion-Planner/checkpoints/model.pth\"\n",
    "OUTPUT_DIR = f\"/home/SENSETIME/yanzichen/Diffusion-Planner/checkpoints/{BRANCH_NAME}\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 创建输出目录\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def load_base_model() -> nn.Module:\n",
    "    \"\"\"加载预训练的Diffusion Planner基模型\"\"\"\n",
    "    # 加载配置参数\n",
    "    with open(ARGS_FILE, 'r') as f:\n",
    "        args = json.load(f)\n",
    "    \n",
    "    # 导入模型类（根据实际项目结构调整）\n",
    "    from diffusion_planner.model.diffusion_planner import DiffusionPlanner\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = DiffusionPlanner(\n",
    "        input_dim=args['input_dim'],\n",
    "        hidden_dim=args['hidden_dim'],\n",
    "        num_layers=args['num_layers'],\n",
    "        diffusion_steps=args['diffusion_steps']\n",
    "    )\n",
    "    \n",
    "    # 加载预训练权重\n",
    "    checkpoint = torch.load(CKPT_FILE, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded base model from {CKPT_FILE}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def setup_mmoe_lora(base_model: nn.Module, feature_dim: int) -> nn.Module:\n",
    "    \"\"\"配置MMoE-6专家网络和LoRA适配器\"\"\"\n",
    "    # 冻结基模型参数\n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # 应用LoRA到基模型的线性层\n",
    "    base_model = apply_lora_to_model(base_model, rank=10, alpha=32.0)\n",
    "    \n",
    "    # 添加MMoE-6专家网络\n",
    "    class DiffusionPlannerWithMMoE(nn.Module):\n",
    "        def __init__(self, base_model, feature_dim):\n",
    "            super().__init__()\n",
    "            self.base_model = base_model\n",
    "            self.mmoe = MMoE6(feature_dim=feature_dim, hidden_dim=256, num_experts=6, num_tasks=6)\n",
    "            \n",
    "            # 场景分类器\n",
    "            self.scene_classifier = nn.Sequential(\n",
    "                nn.Linear(feature_dim, 256),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(256, 6),  # 6类场景\n",
    "                nn.Softmax(dim=1)\n",
    "            )\n",
    "            \n",
    "            # 任务特定输出头\n",
    "            self.task_heads = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    LoRALinear(nn.Linear(256, 256), rank=10),\n",
    "                    nn.GELU(),\n",
    "                    LoRALinear(nn.Linear(256, 2), rank=10)  # 输出轨迹点(x,y)\n",
    "                ) for _ in range(6)  # 6类场景\n",
    "            ])\n",
    "            \n",
    "            # 场景名称映射\n",
    "            self.scene_names = [\"普通巡航\", \"坡道\", \"弯道\", \"绕障\", \"会车\", \"横穿\"]\n",
    "        \n",
    "        def forward(self, x, t=None):\n",
    "            # 提取特征（根据实际模型调整）\n",
    "            if hasattr(self.base_model, 'extract_features'):\n",
    "                features = self.base_model.extract_features(x, t)\n",
    "            else:\n",
    "                # 如果模型没有extract_features方法，使用forward的中间输出\n",
    "                with torch.no_grad():\n",
    "                    base_output = self.base_model(x, t)\n",
    "                features = base_output['features'] if isinstance(base_output, dict) else base_output\n",
    "            \n",
    "            # 场景分类\n",
    "            scene_probs = self.scene_classifier(features)\n",
    "            scene_id = torch.argmax(scene_probs, dim=1)\n",
    "            \n",
    "            # MMoE处理\n",
    "            mmoe_outputs = self.mmoe(features)\n",
    "            \n",
    "            # 根据场景选择输出头\n",
    "            batch_size = x.size(0)\n",
    "            outputs = torch.zeros(batch_size, 2, device=x.device)\n",
    "            \n",
    "            for i in range(6):\n",
    "                mask = (scene_id == i).unsqueeze(1).expand(-1, 2)\n",
    "                if mask.any():\n",
    "                    outputs[mask] = self.task_heads[i](mmoe_outputs[i])[mask]\n",
    "                    \n",
    "            return {\n",
    "                \"trajectory\": outputs,\n",
    "                \"scene_id\": scene_id,\n",
    "                \"scene_probs\": scene_probs,\n",
    "                \"features\": features\n",
    "            }\n",
    "    \n",
    "    # 初始化增强模型\n",
    "    enhanced_model = DiffusionPlannerWithMMoE(base_model, feature_dim)\n",
    "    return enhanced_model\n",
    "\n",
    "def prepare_datasets() -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"准备混合数据集（20% nuPlan + 80% 实车数据）\"\"\"\n",
    "    # 导入数据集类（根据实际项目结构调整）\n",
    "    from diffusion_planner.data.nuplan_dataset import NuPlanDataset\n",
    "    from diffusion_planner.data.real_world_dataset import RealWorldDataset\n",
    "    \n",
    "    # 创建数据集\n",
    "    nuplan_dataset = NuPlanDataset(\n",
    "        data_dir=\"/path/to/nuplan\",\n",
    "        split=\"train\",\n",
    "        scene_types=[\"cruise\", \"ramp\", \"curve\", \"obstacle\", \"meeting\", \"crossing\"]\n",
    "    )\n",
    "    \n",
    "    realworld_dataset = RealWorldDataset(\n",
    "        data_dir=\"/path/to/real_world_data\",\n",
    "        split=\"train\",\n",
    "        scene_types=[\"cruise\", \"ramp\", \"curve\", \"obstacle\", \"meeting\", \"crossing\"]\n",
    "    )\n",
    "    \n",
    "    # 计算混合比例\n",
    "    nuplan_size = len(nuplan_dataset)\n",
    "    realworld_size = len(realworld_dataset)\n",
    "    total_size = nuplan_size + realworld_size\n",
    "    \n",
    "    # 按20:80比例采样\n",
    "    nuplan_samples = int(total_size * 0.2)\n",
    "    realworld_samples = int(total_size * 0.8)\n",
    "    \n",
    "    # 创建采样器\n",
    "    nuplan_indices = torch.randperm(len(nuplan_dataset))[:nuplan_samples]\n",
    "    realworld_indices = torch.randperm(len(realworld_dataset))[:realworld_samples]\n",
    "    \n",
    "    nuplan_subset = torch.utils.data.Subset(nuplan_dataset, nuplan_indices)\n",
    "    realworld_subset = torch.utils.data.Subset(realworld_dataset, realworld_indices)\n",
    "    \n",
    "    # 合并数据集\n",
    "    mixed_dataset = ConcatDataset([nuplan_subset, realworld_subset])\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(\n",
    "        mixed_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # 准备验证集\n",
    "    val_dataset = RealWorldDataset(\n",
    "        data_dir=\"/path/to/real_world_data\",\n",
    "        split=\"val\",\n",
    "        scene_types=[\"cruise\", \"ramp\", \"curve\", \"obstacle\", \"meeting\", \"crossing\"]\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    num_epochs: int = 50,\n",
    "    lr: float = 1e-4\n",
    ") -> None:\n",
    "    \"\"\"训练增强模型\"\"\"\n",
    "    # 定义优化器（只优化LoRA和MMoE参数）\n",
    "    optimizer = optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=lr,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "    \n",
    "    # 定义损失函数\n",
    "    criterion = diffusion_loss_func  # 使用项目中的扩散损失函数\n",
    "    \n",
    "    # 学习率调度器\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # 训练循环\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (inputs, targets, scene_labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "            scene_labels = scene_labels.to(DEVICE)\n",
    "            \n",
    "            # 前向传播\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            trajectory = outputs[\"trajectory\"]\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = criterion(outputs, targets)  # 使用项目中的损失计算方式\n",
    "            \n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        scene_correct = 0\n",
    "        scene_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, scene_labels in val_loader:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                targets = targets.to(DEVICE)\n",
    "                scene_labels = scene_labels.to(DEVICE)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                trajectory = outputs[\"trajectory\"]\n",
    "                scene_preds = outputs[\"scene_id\"]\n",
    "                \n",
    "                # 计算损失\n",
    "                val_loss += criterion(outputs, targets).item()\n",
    "                \n",
    "                # 计算场景分类准确率\n",
    "                scene_total += scene_labels.size(0)\n",
    "                scene_correct += (scene_preds == scene_labels).sum().item()\n",
    "        \n",
    "        # 计算平均损失和准确率\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        scene_acc = 100.0 * scene_correct / scene_total\n",
    "        \n",
    "        # 打印统计信息\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Scene Acc: {scene_acc:.2f}%\")\n",
    "        \n",
    "        # 调整学习率\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'scene_acc': scene_acc\n",
    "            }, f\"{OUTPUT_DIR}/best_model.pth\")\n",
    "            print(f\"Saved best model with val loss: {val_loss:.4f}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    # 设置随机种子\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # 初始化分布式训练（如果需要）\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        ddp.init_distributed_mode()\n",
    "    \n",
    "    # 加载基模型\n",
    "    base_model = load_base_model()\n",
    "    base_model = base_model.to(DEVICE)\n",
    "    \n",
    "    # 设置MMoE和LoRA\n",
    "    feature_dim = 512  # 根据实际模型调整\n",
    "    enhanced_model = setup_mmoe_lora(base_model, feature_dim)\n",
    "    enhanced_model = enhanced_model.to(DEVICE)\n",
    "    \n",
    "    # 准备数据集\n",
    "    train_loader, val_loader = prepare_datasets()\n",
    "    \n",
    "    # 训练模型\n",
    "    train_model(enhanced_model, train_loader, val_loader, num_epochs=50)\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[9], line 121\u001b[0m\n",
      "\u001b[1;32m    118\u001b[0m     enhanced_model \u001b[38;5;241m=\u001b[39m DiffusionPlannerWithMMoE(base_model, feature_dim)\n",
      "\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m enhanced_model\n",
      "\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprepare_datasets\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mTuple\u001b[49m[DataLoader, DataLoader]:\n",
      "\u001b[1;32m    122\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"准备混合数据集（20% nuPlan + 80% 实车数据）\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m# 导入数据集类（根据实际项目结构调整）\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tuple' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "from diffusion_planner.utils.data_augmentation import StatePerturbation   \n",
    "from diffusion_planner.utils.train_utils import get_epoch_mean_loss\n",
    "from diffusion_planner.utils import ddp\n",
    "from diffusion_planner.loss import diffusion_loss_func\n",
    "\n",
    "\n",
    "# 配置参数\n",
    "BRANCH_NAME = \"diffusion_planner_release\"\n",
    "ARGS_FILE = \"/home/SENSETIME/yanzichen/Diffusion-Planner/checkpoints/args.json\"\n",
    "CKPT_FILE = \"/home/SENSETIME/yanzichen/Diffusion-Planner/checkpoints/model.pth\"\n",
    "OUTPUT_DIR = f\"/home/SENSETIME/yanzichen/Diffusion-Planner/checkpoints/{BRANCH_NAME}\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 创建输出目录\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def load_base_model() -> nn.Module:\n",
    "    \"\"\"加载预训练的Diffusion Planner基模型\"\"\"\n",
    "    # 加载配置参数\n",
    "    with open(ARGS_FILE, 'r') as f:\n",
    "        args = json.load(f)\n",
    "    \n",
    "    # 导入模型类（根据实际项目结构调整）\n",
    "    from diffusion_planner.model.diffusion_planner import DiffusionPlanner\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = DiffusionPlanner(\n",
    "        input_dim=args['input_dim'],\n",
    "        hidden_dim=args['hidden_dim'],\n",
    "        num_layers=args['num_layers'],\n",
    "        diffusion_steps=args['diffusion_steps']\n",
    "    )\n",
    "    \n",
    "    # 加载预训练权重\n",
    "    checkpoint = torch.load(CKPT_FILE, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded base model from {CKPT_FILE}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def setup_mmoe_lora(base_model: nn.Module, feature_dim: int) -> nn.Module:\n",
    "    \"\"\"配置MMoE-6专家网络和LoRA适配器\"\"\"\n",
    "    # 冻结基模型参数\n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # 应用LoRA到基模型的线性层\n",
    "    base_model = apply_lora_to_model(base_model, rank=10, alpha=32.0)\n",
    "    \n",
    "    # 添加MMoE-6专家网络\n",
    "    class DiffusionPlannerWithMMoE(nn.Module):\n",
    "        def __init__(self, base_model, feature_dim):\n",
    "            super().__init__()\n",
    "            self.base_model = base_model\n",
    "            self.mmoe = MMoE6(feature_dim=feature_dim, hidden_dim=256, num_experts=6, num_tasks=6)\n",
    "            \n",
    "            # 场景分类器\n",
    "            self.scene_classifier = nn.Sequential(\n",
    "                nn.Linear(feature_dim, 256),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(256, 6),  # 6类场景\n",
    "                nn.Softmax(dim=1)\n",
    "            )\n",
    "            \n",
    "            # 任务特定输出头\n",
    "            self.task_heads = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    LoRALinear(nn.Linear(256, 256), rank=10),\n",
    "                    nn.GELU(),\n",
    "                    LoRALinear(nn.Linear(256, 2), rank=10)  # 输出轨迹点(x,y)\n",
    "                ) for _ in range(6)  # 6类场景\n",
    "            ])\n",
    "            \n",
    "            # 场景名称映射\n",
    "            self.scene_names = [\"普通巡航\", \"坡道\", \"弯道\", \"绕障\", \"会车\", \"横穿\"]\n",
    "        \n",
    "        def forward(self, x, t=None):\n",
    "            # 提取特征（根据实际模型调整）\n",
    "            if hasattr(self.base_model, 'extract_features'):\n",
    "                features = self.base_model.extract_features(x, t)\n",
    "            else:\n",
    "                # 如果模型没有extract_features方法，使用forward的中间输出\n",
    "                with torch.no_grad():\n",
    "                    base_output = self.base_model(x, t)\n",
    "                features = base_output['features'] if isinstance(base_output, dict) else base_output\n",
    "            \n",
    "            # 场景分类\n",
    "            scene_probs = self.scene_classifier(features)\n",
    "            scene_id = torch.argmax(scene_probs, dim=1)\n",
    "            \n",
    "            # MMoE处理\n",
    "            mmoe_outputs = self.mmoe(features)\n",
    "            \n",
    "            # 根据场景选择输出头\n",
    "            batch_size = x.size(0)\n",
    "            outputs = torch.zeros(batch_size, 2, device=x.device)\n",
    "            \n",
    "            for i in range(6):\n",
    "                mask = (scene_id == i).unsqueeze(1).expand(-1, 2)\n",
    "                if mask.any():\n",
    "                    outputs[mask] = self.task_heads[i](mmoe_outputs[i])[mask]\n",
    "                    \n",
    "            return {\n",
    "                \"trajectory\": outputs,\n",
    "                \"scene_id\": scene_id,\n",
    "                \"scene_probs\": scene_probs,\n",
    "                \"features\": features\n",
    "            }\n",
    "    \n",
    "    # 初始化增强模型\n",
    "    enhanced_model = DiffusionPlannerWithMMoE(base_model, feature_dim)\n",
    "    return enhanced_model\n",
    "\n",
    "def prepare_datasets() -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"准备混合数据集（20% nuPlan + 80% 实车数据）\"\"\"\n",
    "    # 导入数据集类（根据实际项目结构调整）\n",
    "    from diffusion_planner.data.nuplan_dataset import NuPlanDataset\n",
    "    from diffusion_planner.data.real_world_dataset import RealWorldDataset\n",
    "    \n",
    "    # 创建数据集\n",
    "    nuplan_dataset = NuPlanDataset(\n",
    "        data_dir=\"/path/to/nuplan\",\n",
    "        split=\"train\",\n",
    "        scene_types=[\"cruise\", \"ramp\", \"curve\", \"obstacle\", \"meeting\", \"crossing\"]\n",
    "    )\n",
    "    \n",
    "    realworld_dataset = RealWorldDataset(\n",
    "        data_dir=\"/path/to/real_world_data\",\n",
    "        split=\"train\",\n",
    "        scene_types=[\"cruise\", \"ramp\", \"curve\", \"obstacle\", \"meeting\", \"crossing\"]\n",
    "    )\n",
    "    \n",
    "    # 计算混合比例\n",
    "    nuplan_size = len(nuplan_dataset)\n",
    "    realworld_size = len(realworld_dataset)\n",
    "    total_size = nuplan_size + realworld_size\n",
    "    \n",
    "    # 按20:80比例采样\n",
    "    nuplan_samples = int(total_size * 0.2)\n",
    "    realworld_samples = int(total_size * 0.8)\n",
    "    \n",
    "    # 创建采样器\n",
    "    nuplan_indices = torch.randperm(len(nuplan_dataset))[:nuplan_samples]\n",
    "    realworld_indices = torch.randperm(len(realworld_dataset))[:realworld_samples]\n",
    "    \n",
    "    nuplan_subset = torch.utils.data.Subset(nuplan_dataset, nuplan_indices)\n",
    "    realworld_subset = torch.utils.data.Subset(realworld_dataset, realworld_indices)\n",
    "    \n",
    "    # 合并数据集\n",
    "    mixed_dataset = ConcatDataset([nuplan_subset, realworld_subset])\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(\n",
    "        mixed_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # 准备验证集\n",
    "    val_dataset = RealWorldDataset(\n",
    "        data_dir=\"/path/to/real_world_data\",\n",
    "        split=\"val\",\n",
    "        scene_types=[\"cruise\", \"ramp\", \"curve\", \"obstacle\", \"meeting\", \"crossing\"]\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    num_epochs: int = 50,\n",
    "    lr: float = 1e-4\n",
    ") -> None:\n",
    "    \"\"\"训练增强模型\"\"\"\n",
    "    # 定义优化器（只优化LoRA和MMoE参数）\n",
    "    optimizer = optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=lr,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "    \n",
    "    # 定义损失函数\n",
    "    criterion = diffusion_loss_func  # 使用项目中的扩散损失函数\n",
    "    \n",
    "    # 学习率调度器\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # 训练循环\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (inputs, targets, scene_labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "            scene_labels = scene_labels.to(DEVICE)\n",
    "            \n",
    "            # 前向传播\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            trajectory = outputs[\"trajectory\"]\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = criterion(outputs, targets)  # 使用项目中的损失计算方式\n",
    "            \n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        scene_correct = 0\n",
    "        scene_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, scene_labels in val_loader:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                targets = targets.to(DEVICE)\n",
    "                scene_labels = scene_labels.to(DEVICE)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                trajectory = outputs[\"trajectory\"]\n",
    "                scene_preds = outputs[\"scene_id\"]\n",
    "                \n",
    "                # 计算损失\n",
    "                val_loss += criterion(outputs, targets).item()\n",
    "                \n",
    "                # 计算场景分类准确率\n",
    "                scene_total += scene_labels.size(0)\n",
    "                scene_correct += (scene_preds == scene_labels).sum().item()\n",
    "        \n",
    "        # 计算平均损失和准确率\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        scene_acc = 100.0 * scene_correct / scene_total\n",
    "        \n",
    "        # 打印统计信息\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Scene Acc: {scene_acc:.2f}%\")\n",
    "        \n",
    "        # 调整学习率\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'scene_acc': scene_acc\n",
    "            }, f\"{OUTPUT_DIR}/best_model.pth\")\n",
    "            print(f\"Saved best model with val loss: {val_loss:.4f}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    # 设置随机种子\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # 初始化分布式训练（如果需要）\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        ddp.init_distributed_mode()\n",
    "    \n",
    "    # 加载基模型\n",
    "    base_model = load_base_model()\n",
    "    base_model = base_model.to(DEVICE)\n",
    "    \n",
    "    # 设置MMoE和LoRA\n",
    "    feature_dim = 512  # 根据实际模型调整\n",
    "    enhanced_model = setup_mmoe_lora(base_model, feature_dim)\n",
    "    enhanced_model = enhanced_model.to(DEVICE)\n",
    "    \n",
    "    # 准备数据集\n",
    "    train_loader, val_loader = prepare_datasets()\n",
    "    \n",
    "    # 训练模型\n",
    "    train_model(enhanced_model, train_loader, val_loader, num_epochs=50)\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[9], line 121\u001b[0m\n",
      "\u001b[1;32m    118\u001b[0m     enhanced_model \u001b[38;5;241m=\u001b[39m DiffusionPlannerWithMMoE(base_model, feature_dim)\n",
      "\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m enhanced_model\n",
      "\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprepare_datasets\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mTuple\u001b[49m[DataLoader, DataLoader]:\n",
      "\u001b[1;32m    122\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"准备混合数据集（20% nuPlan + 80% 实车数据）\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m# 导入数据集类（根据实际项目结构调整）\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tuple' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "from diffusion_planner.utils.data_augmentation import StatePerturbation   \n",
    "from diffusion_planner.utils.train_utils import get_epoch_mean_loss\n",
    "from diffusion_planner.utils import ddp\n",
    "from diffusion_planner.loss import diffusion_loss_func\n",
    "\n",
    "\n",
    "# 配置参数\n",
    "BRANCH_NAME = \"diffusion_planner_release\"\n",
    "ARGS_FILE = \"/home/SENSETIME/yanzichen/Diffusion-Planner/checkpoints/args.json\"\n",
    "CKPT_FILE = \"/home/SENSETIME/yanzichen/Diffusion-Planner/checkpoints/model.pth\"\n",
    "OUTPUT_DIR = f\"/home/SENSETIME/yanzichen/Diffusion-Planner/checkpoints/{BRANCH_NAME}\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 创建输出目录\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def load_base_model() -> nn.Module:\n",
    "    \"\"\"加载预训练的Diffusion Planner基模型\"\"\"\n",
    "    # 加载配置参数\n",
    "    with open(ARGS_FILE, 'r') as f:\n",
    "        args = json.load(f)\n",
    "    \n",
    "    # 导入模型类（根据实际项目结构调整）\n",
    "    from diffusion_planner.model.diffusion_planner import DiffusionPlanner\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = DiffusionPlanner(\n",
    "        input_dim=args['input_dim'],\n",
    "        hidden_dim=args['hidden_dim'],\n",
    "        num_layers=args['num_layers'],\n",
    "        diffusion_steps=args['diffusion_steps']\n",
    "    )\n",
    "    \n",
    "    # 加载预训练权重\n",
    "    checkpoint = torch.load(CKPT_FILE, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded base model from {CKPT_FILE}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def setup_mmoe_lora(base_model: nn.Module, feature_dim: int) -> nn.Module:\n",
    "    \"\"\"配置MMoE-6专家网络和LoRA适配器\"\"\"\n",
    "    # 冻结基模型参数\n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # 应用LoRA到基模型的线性层\n",
    "    base_model = apply_lora_to_model(base_model, rank=10, alpha=32.0)\n",
    "    \n",
    "    # 添加MMoE-6专家网络\n",
    "    class DiffusionPlannerWithMMoE(nn.Module):\n",
    "        def __init__(self, base_model, feature_dim):\n",
    "            super().__init__()\n",
    "            self.base_model = base_model\n",
    "            self.mmoe = MMoE6(feature_dim=feature_dim, hidden_dim=256, num_experts=6, num_tasks=6)\n",
    "            \n",
    "            # 场景分类器\n",
    "            self.scene_classifier = nn.Sequential(\n",
    "                nn.Linear(feature_dim, 256),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(256, 6),  # 6类场景\n",
    "                nn.Softmax(dim=1)\n",
    "            )\n",
    "            \n",
    "            # 任务特定输出头\n",
    "            self.task_heads = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    LoRALinear(nn.Linear(256, 256), rank=10),\n",
    "                    nn.GELU(),\n",
    "                    LoRALinear(nn.Linear(256, 2), rank=10)  # 输出轨迹点(x,y)\n",
    "                ) for _ in range(6)  # 6类场景\n",
    "            ])\n",
    "            \n",
    "            # 场景名称映射\n",
    "            self.scene_names = [\"普通巡航\", \"坡道\", \"弯道\", \"绕障\", \"会车\", \"横穿\"]\n",
    "        \n",
    "        def forward(self, x, t=None):\n",
    "            # 提取特征（根据实际模型调整）\n",
    "            if hasattr(self.base_model, 'extract_features'):\n",
    "                features = self.base_model.extract_features(x, t)\n",
    "            else:\n",
    "                # 如果模型没有extract_features方法，使用forward的中间输出\n",
    "                with torch.no_grad():\n",
    "                    base_output = self.base_model(x, t)\n",
    "                features = base_output['features'] if isinstance(base_output, dict) else base_output\n",
    "            \n",
    "            # 场景分类\n",
    "            scene_probs = self.scene_classifier(features)\n",
    "            scene_id = torch.argmax(scene_probs, dim=1)\n",
    "            \n",
    "            # MMoE处理\n",
    "            mmoe_outputs = self.mmoe(features)\n",
    "            \n",
    "            # 根据场景选择输出头\n",
    "            batch_size = x.size(0)\n",
    "            outputs = torch.zeros(batch_size, 2, device=x.device)\n",
    "            \n",
    "            for i in range(6):\n",
    "                mask = (scene_id == i).unsqueeze(1).expand(-1, 2)\n",
    "                if mask.any():\n",
    "                    outputs[mask] = self.task_heads[i](mmoe_outputs[i])[mask]\n",
    "                    \n",
    "            return {\n",
    "                \"trajectory\": outputs,\n",
    "                \"scene_id\": scene_id,\n",
    "                \"scene_probs\": scene_probs,\n",
    "                \"features\": features\n",
    "            }\n",
    "    \n",
    "    # 初始化增强模型\n",
    "    enhanced_model = DiffusionPlannerWithMMoE(base_model, feature_dim)\n",
    "    return enhanced_model\n",
    "\n",
    "def prepare_datasets() -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"准备混合数据集（20% nuPlan + 80% 实车数据）\"\"\"\n",
    "    # 导入数据集类（根据实际项目结构调整）\n",
    "    from diffusion_planner.data.nuplan_dataset import NuPlanDataset\n",
    "    from diffusion_planner.data.real_world_dataset import RealWorldDataset\n",
    "    \n",
    "    # 创建数据集\n",
    "    nuplan_dataset = NuPlanDataset(\n",
    "        data_dir=\"/path/to/nuplan\",\n",
    "        split=\"train\",\n",
    "        scene_types=[\"cruise\", \"ramp\", \"curve\", \"obstacle\", \"meeting\", \"crossing\"]\n",
    "    )\n",
    "    \n",
    "    realworld_dataset = RealWorldDataset(\n",
    "        data_dir=\"/path/to/real_world_data\",\n",
    "        split=\"train\",\n",
    "        scene_types=[\"cruise\", \"ramp\", \"curve\", \"obstacle\", \"meeting\", \"crossing\"]\n",
    "    )\n",
    "    \n",
    "    # 计算混合比例\n",
    "    nuplan_size = len(nuplan_dataset)\n",
    "    realworld_size = len(realworld_dataset)\n",
    "    total_size = nuplan_size + realworld_size\n",
    "    \n",
    "    # 按20:80比例采样\n",
    "    nuplan_samples = int(total_size * 0.2)\n",
    "    realworld_samples = int(total_size * 0.8)\n",
    "    \n",
    "    # 创建采样器\n",
    "    nuplan_indices = torch.randperm(len(nuplan_dataset))[:nuplan_samples]\n",
    "    realworld_indices = torch.randperm(len(realworld_dataset))[:realworld_samples]\n",
    "    \n",
    "    nuplan_subset = torch.utils.data.Subset(nuplan_dataset, nuplan_indices)\n",
    "    realworld_subset = torch.utils.data.Subset(realworld_dataset, realworld_indices)\n",
    "    \n",
    "    # 合并数据集\n",
    "    mixed_dataset = ConcatDataset([nuplan_subset, realworld_subset])\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(\n",
    "        mixed_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # 准备验证集\n",
    "    val_dataset = RealWorldDataset(\n",
    "        data_dir=\"/path/to/real_world_data\",\n",
    "        split=\"val\",\n",
    "        scene_types=[\"cruise\", \"ramp\", \"curve\", \"obstacle\", \"meeting\", \"crossing\"]\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    num_epochs: int = 50,\n",
    "    lr: float = 1e-4\n",
    ") -> None:\n",
    "    \"\"\"训练增强模型\"\"\"\n",
    "    # 定义优化器（只优化LoRA和MMoE参数）\n",
    "    optimizer = optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=lr,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "    \n",
    "    # 定义损失函数\n",
    "    criterion = diffusion_loss_func  # 使用项目中的扩散损失函数\n",
    "    \n",
    "    # 学习率调度器\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # 训练循环\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (inputs, targets, scene_labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "            scene_labels = scene_labels.to(DEVICE)\n",
    "            \n",
    "            # 前向传播\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            trajectory = outputs[\"trajectory\"]\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = criterion(outputs, targets)  # 使用项目中的损失计算方式\n",
    "            \n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        scene_correct = 0\n",
    "        scene_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, scene_labels in val_loader:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                targets = targets.to(DEVICE)\n",
    "                scene_labels = scene_labels.to(DEVICE)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                trajectory = outputs[\"trajectory\"]\n",
    "                scene_preds = outputs[\"scene_id\"]\n",
    "                \n",
    "                # 计算损失\n",
    "                val_loss += criterion(outputs, targets).item()\n",
    "                \n",
    "                # 计算场景分类准确率\n",
    "                scene_total += scene_labels.size(0)\n",
    "                scene_correct += (scene_preds == scene_labels).sum().item()\n",
    "        \n",
    "        # 计算平均损失和准确率\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        scene_acc = 100.0 * scene_correct / scene_total\n",
    "        \n",
    "        # 打印统计信息\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Scene Acc: {scene_acc:.2f}%\")\n",
    "        \n",
    "        # 调整学习率\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'scene_acc': scene_acc\n",
    "            }, f\"{OUTPUT_DIR}/best_model.pth\")\n",
    "            print(f\"Saved best model with val loss: {val_loss:.4f}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    # 设置随机种子\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # 初始化分布式训练（如果需要）\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        ddp.init_distributed_mode()\n",
    "    \n",
    "    # 加载基模型\n",
    "    base_model = load_base_model()\n",
    "    base_model = base_model.to(DEVICE)\n",
    "    \n",
    "    # 设置MMoE和LoRA\n",
    "    feature_dim = 512  # 根据实际模型调整\n",
    "    enhanced_model = setup_mmoe_lora(base_model, feature_dim)\n",
    "    enhanced_model = enhanced_model.to(DEVICE)\n",
    "    \n",
    "    # 准备数据集\n",
    "    train_loader, val_loader = prepare_datasets()\n",
    "    \n",
    "    # 训练模型\n",
    "    train_model(enhanced_model, train_loader, val_loader, num_epochs=50)\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[9], line 121\u001b[0m\n",
      "\u001b[1;32m    118\u001b[0m     enhanced_model \u001b[38;5;241m=\u001b[39m DiffusionPlannerWithMMoE(base_model, feature_dim)\n",
      "\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m enhanced_model\n",
      "\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprepare_datasets\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mTuple\u001b[49m[DataLoader, DataLoader]:\n",
      "\u001b[1;32m    122\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"准备混合数据集（20% nuPlan + 80% 实车数据）\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m# 导入数据集类（根据实际项目结构调整）\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tuple' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "from diffusion_planner.utils.data_augmentation import StatePerturbation   \n",
    "from diffusion_planner.utils.train_utils import get_epoch_mean_loss\n",
    "from diffusion_planner.utils import ddp\n",
    "from diffusion_planner.loss import diffusion_loss_func\n",
    "\n",
    "\n",
    "# 配置参数\n",
    "BRANCH_NAME = \"diffusion_planner_release\"\n",
    "ARGS_FILE = \"/home/SENSETIME/yanzichen/Diffusion-Planner/checkpoints/args.json\"\n",
    "CKPT_FILE = \"/home/SENSETIME/yanzichen/Diffusion-Planner/checkpoints/model.pth\"\n",
    "OUTPUT_DIR = f\"/home/SENSETIME/yanzichen/Diffusion-Planner/checkpoints/{BRANCH_NAME}\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 创建输出目录\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def load_base_model() -> nn.Module:\n",
    "    \"\"\"加载预训练的Diffusion Planner基模型\"\"\"\n",
    "    # 加载配置参数\n",
    "    with open(ARGS_FILE, 'r') as f:\n",
    "        args = json.load(f)\n",
    "    \n",
    "    # 导入模型类（根据实际项目结构调整）\n",
    "    from diffusion_planner.model.diffusion_planner import DiffusionPlanner\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = DiffusionPlanner(\n",
    "        input_dim=args['input_dim'],\n",
    "        hidden_dim=args['hidden_dim'],\n",
    "        num_layers=args['num_layers'],\n",
    "        diffusion_steps=args['diffusion_steps']\n",
    "    )\n",
    "    \n",
    "    # 加载预训练权重\n",
    "    checkpoint = torch.load(CKPT_FILE, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded base model from {CKPT_FILE}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def setup_mmoe_lora(base_model: nn.Module, feature_dim: int) -> nn.Module:\n",
    "    \"\"\"配置MMoE-6专家网络和LoRA适配器\"\"\"\n",
    "    # 冻结基模型参数\n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # 应用LoRA到基模型的线性层\n",
    "    base_model = apply_lora_to_model(base_model, rank=10, alpha=32.0)\n",
    "    \n",
    "    # 添加MMoE-6专家网络\n",
    "    class DiffusionPlannerWithMMoE(nn.Module):\n",
    "        def __init__(self, base_model, feature_dim):\n",
    "            super().__init__()\n",
    "            self.base_model = base_model\n",
    "            self.mmoe = MMoE6(feature_dim=feature_dim, hidden_dim=256, num_experts=6, num_tasks=6)\n",
    "            \n",
    "            # 场景分类器\n",
    "            self.scene_classifier = nn.Sequential(\n",
    "                nn.Linear(feature_dim, 256),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(256, 6),  # 6类场景\n",
    "                nn.Softmax(dim=1)\n",
    "            )\n",
    "            \n",
    "            # 任务特定输出头\n",
    "            self.task_heads = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    LoRALinear(nn.Linear(256, 256), rank=10),\n",
    "                    nn.GELU(),\n",
    "                    LoRALinear(nn.Linear(256, 2), rank=10)  # 输出轨迹点(x,y)\n",
    "                ) for _ in range(6)  # 6类场景\n",
    "            ])\n",
    "            \n",
    "            # 场景名称映射\n",
    "            self.scene_names = [\"普通巡航\", \"坡道\", \"弯道\", \"绕障\", \"会车\", \"横穿\"]\n",
    "        \n",
    "        def forward(self, x, t=None):\n",
    "            # 提取特征（根据实际模型调整）\n",
    "            if hasattr(self.base_model, 'extract_features'):\n",
    "                features = self.base_model.extract_features(x, t)\n",
    "            else:\n",
    "                # 如果模型没有extract_features方法，使用forward的中间输出\n",
    "                with torch.no_grad():\n",
    "                    base_output = self.base_model(x, t)\n",
    "                features = base_output['features'] if isinstance(base_output, dict) else base_output\n",
    "            \n",
    "            # 场景分类\n",
    "            scene_probs = self.scene_classifier(features)\n",
    "            scene_id = torch.argmax(scene_probs, dim=1)\n",
    "            \n",
    "            # MMoE处理\n",
    "            mmoe_outputs = self.mmoe(features)\n",
    "            \n",
    "            # 根据场景选择输出头\n",
    "            batch_size = x.size(0)\n",
    "            outputs = torch.zeros(batch_size, 2, device=x.device)\n",
    "            \n",
    "            for i in range(6):\n",
    "                mask = (scene_id == i).unsqueeze(1).expand(-1, 2)\n",
    "                if mask.any():\n",
    "                    outputs[mask] = self.task_heads[i](mmoe_outputs[i])[mask]\n",
    "                    \n",
    "            return {\n",
    "                \"trajectory\": outputs,\n",
    "                \"scene_id\": scene_id,\n",
    "                \"scene_probs\": scene_probs,\n",
    "                \"features\": features\n",
    "            }\n",
    "    \n",
    "    # 初始化增强模型\n",
    "    enhanced_model = DiffusionPlannerWithMMoE(base_model, feature_dim)\n",
    "    return enhanced_model\n",
    "\n",
    "def prepare_datasets() -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"准备混合数据集（20% nuPlan + 80% 实车数据）\"\"\"\n",
    "    # 导入数据集类（根据实际项目结构调整）\n",
    "    from diffusion_planner.data.nuplan_dataset import NuPlanDataset\n",
    "    from diffusion_planner.data.real_world_dataset import RealWorldDataset\n",
    "    \n",
    "    # 创建数据集\n",
    "    nuplan_dataset = NuPlanDataset(\n",
    "        data_dir=\"/path/to/nuplan\",\n",
    "        split=\"train\",\n",
    "        scene_types=[\"cruise\", \"ramp\", \"curve\", \"obstacle\", \"meeting\", \"crossing\"]\n",
    "    )\n",
    "    \n",
    "    realworld_dataset = RealWorldDataset(\n",
    "        data_dir=\"/path/to/real_world_data\",\n",
    "        split=\"train\",\n",
    "        scene_types=[\"cruise\", \"ramp\", \"curve\", \"obstacle\", \"meeting\", \"crossing\"]\n",
    "    )\n",
    "    \n",
    "    # 计算混合比例\n",
    "    nuplan_size = len(nuplan_dataset)\n",
    "    realworld_size = len(realworld_dataset)\n",
    "    total_size = nuplan_size + realworld_size\n",
    "    \n",
    "    # 按20:80比例采样\n",
    "    nuplan_samples = int(total_size * 0.2)\n",
    "    realworld_samples = int(total_size * 0.8)\n",
    "    \n",
    "    # 创建采样器\n",
    "    nuplan_indices = torch.randperm(len(nuplan_dataset))[:nuplan_samples]\n",
    "    realworld_indices = torch.randperm(len(realworld_dataset))[:realworld_samples]\n",
    "    \n",
    "    nuplan_subset = torch.utils.data.Subset(nuplan_dataset, nuplan_indices)\n",
    "    realworld_subset = torch.utils.data.Subset(realworld_dataset, realworld_indices)\n",
    "    \n",
    "    # 合并数据集\n",
    "    mixed_dataset = ConcatDataset([nuplan_subset, realworld_subset])\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(\n",
    "        mixed_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # 准备验证集\n",
    "    val_dataset = RealWorldDataset(\n",
    "        data_dir=\"/path/to/real_world_data\",\n",
    "        split=\"val\",\n",
    "        scene_types=[\"cruise\", \"ramp\", \"curve\", \"obstacle\", \"meeting\", \"crossing\"]\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    num_epochs: int = 50,\n",
    "    lr: float = 1e-4\n",
    ") -> None:\n",
    "    \"\"\"训练增强模型\"\"\"\n",
    "    # 定义优化器（只优化LoRA和MMoE参数）\n",
    "    optimizer = optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=lr,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "    \n",
    "    # 定义损失函数\n",
    "    criterion = diffusion_loss_func  # 使用项目中的扩散损失函数\n",
    "    \n",
    "    # 学习率调度器\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # 训练循环\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (inputs, targets, scene_labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "            scene_labels = scene_labels.to(DEVICE)\n",
    "            \n",
    "            # 前向传播\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            trajectory = outputs[\"trajectory\"]\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = criterion(outputs, targets)  # 使用项目中的损失计算方式\n",
    "            \n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        scene_correct = 0\n",
    "        scene_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, scene_labels in val_loader:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                targets = targets.to(DEVICE)\n",
    "                scene_labels = scene_labels.to(DEVICE)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                trajectory = outputs[\"trajectory\"]\n",
    "                scene_preds = outputs[\"scene_id\"]\n",
    "                \n",
    "                # 计算损失\n",
    "                val_loss += criterion(outputs, targets).item()\n",
    "                \n",
    "                # 计算场景分类准确率\n",
    "                scene_total += scene_labels.size(0)\n",
    "                scene_correct += (scene_preds == scene_labels).sum().item()\n",
    "        \n",
    "        # 计算平均损失和准确率\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        scene_acc = 100.0 * scene_correct / scene_total\n",
    "        \n",
    "        # 打印统计信息\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Scene Acc: {scene_acc:.2f}%\")\n",
    "        \n",
    "        # 调整学习率\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'scene_acc': scene_acc\n",
    "            }, f\"{OUTPUT_DIR}/best_model.pth\")\n",
    "            print(f\"Saved best model with val loss: {val_loss:.4f}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    # 设置随机种子\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # 初始化分布式训练（如果需要）\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        ddp.init_distributed_mode()\n",
    "    \n",
    "    # 加载基模型\n",
    "    base_model = load_base_model()\n",
    "    base_model = base_model.to(DEVICE)\n",
    "    \n",
    "    # 设置MMoE和LoRA\n",
    "    feature_dim = 512  # 根据实际模型调整\n",
    "    enhanced_model = setup_mmoe_lora(base_model, feature_dim)\n",
    "    enhanced_model = enhanced_model.to(DEVICE)\n",
    "    \n",
    "    # 准备数据集\n",
    "    train_loader, val_loader = prepare_datasets()\n",
    "    \n",
    "    # 训练模型\n",
    "    train_model(enhanced_model, train_loader, val_loader, num_epochs=50)\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Tuple' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[9], line 121\u001b[0m\n",
      "\u001b[1;32m    118\u001b[0m     enhanced_model \u001b[38;5;241m=\u001b[39m DiffusionPlannerWithMMoE(base_model, feature_dim)\n",
      "\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m enhanced_model\n",
      "\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprepare_datasets\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[43mTuple\u001b[49m[DataLoader, DataLoader]:\n",
      "\u001b[1;32m    122\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"准备混合数据集（20% nuPlan + 80% 实车数据）\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m# 导入数据集类（根据实际项目结构调整）\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Tuple' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "\n",
    "from diffusion_planner.utils.data_augmentation import StatePerturbation   \n",
    "from diffusion_planner.utils.train_utils import get_epoch_mean_loss\n",
    "from diffusion_planner.utils import ddp\n",
    "from diffusion_planner.loss import diffusion_loss_func\n",
    "\n",
    "\n",
    "# 配置参数\n",
    "BRANCH_NAME = \"diffusion_planner_release\"\n",
    "ARGS_FILE = \"/home/SENSETIME/yanzichen/Diffusion-Planner/checkpoints/args.json\"\n",
    "CKPT_FILE = \"/home/SENSETIME/yanzichen/Diffusion-Planner/checkpoints/model.pth\"\n",
    "OUTPUT_DIR = f\"/home/SENSETIME/yanzichen/Diffusion-Planner/checkpoints/{BRANCH_NAME}\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 创建输出目录\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def load_base_model() -> nn.Module:\n",
    "    \"\"\"加载预训练的Diffusion Planner基模型\"\"\"\n",
    "    # 加载配置参数\n",
    "    with open(ARGS_FILE, 'r') as f:\n",
    "        args = json.load(f)\n",
    "    \n",
    "    # 导入模型类（根据实际项目结构调整）\n",
    "    from diffusion_planner.model.diffusion_planner import DiffusionPlanner\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = DiffusionPlanner(\n",
    "        input_dim=args['input_dim'],\n",
    "        hidden_dim=args['hidden_dim'],\n",
    "        num_layers=args['num_layers'],\n",
    "        diffusion_steps=args['diffusion_steps']\n",
    "    )\n",
    "    \n",
    "    # 加载预训练权重\n",
    "    checkpoint = torch.load(CKPT_FILE, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Loaded base model from {CKPT_FILE}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def setup_mmoe_lora(base_model: nn.Module, feature_dim: int) -> nn.Module:\n",
    "    \"\"\"配置MMoE-6专家网络和LoRA适配器\"\"\"\n",
    "    # 冻结基模型参数\n",
    "    for param in base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # 应用LoRA到基模型的线性层\n",
    "    base_model = apply_lora_to_model(base_model, rank=10, alpha=32.0)\n",
    "    \n",
    "    # 添加MMoE-6专家网络\n",
    "    class DiffusionPlannerWithMMoE(nn.Module):\n",
    "        def __init__(self, base_model, feature_dim):\n",
    "            super().__init__()\n",
    "            self.base_model = base_model\n",
    "            self.mmoe = MMoE6(feature_dim=feature_dim, hidden_dim=256, num_experts=6, num_tasks=6)\n",
    "            \n",
    "            # 场景分类器\n",
    "            self.scene_classifier = nn.Sequential(\n",
    "                nn.Linear(feature_dim, 256),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(256, 6),  # 6类场景\n",
    "                nn.Softmax(dim=1)\n",
    "            )\n",
    "            \n",
    "            # 任务特定输出头\n",
    "            self.task_heads = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    LoRALinear(nn.Linear(256, 256), rank=10),\n",
    "                    nn.GELU(),\n",
    "                    LoRALinear(nn.Linear(256, 2), rank=10)  # 输出轨迹点(x,y)\n",
    "                ) for _ in range(6)  # 6类场景\n",
    "            ])\n",
    "            \n",
    "            # 场景名称映射\n",
    "            self.scene_names = [\"普通巡航\", \"坡道\", \"弯道\", \"绕障\", \"会车\", \"横穿\"]\n",
    "        \n",
    "        def forward(self, x, t=None):\n",
    "            # 提取特征（根据实际模型调整）\n",
    "            if hasattr(self.base_model, 'extract_features'):\n",
    "                features = self.base_model.extract_features(x, t)\n",
    "            else:\n",
    "                # 如果模型没有extract_features方法，使用forward的中间输出\n",
    "                with torch.no_grad():\n",
    "                    base_output = self.base_model(x, t)\n",
    "                features = base_output['features'] if isinstance(base_output, dict) else base_output\n",
    "            \n",
    "            # 场景分类\n",
    "            scene_probs = self.scene_classifier(features)\n",
    "            scene_id = torch.argmax(scene_probs, dim=1)\n",
    "            \n",
    "            # MMoE处理\n",
    "            mmoe_outputs = self.mmoe(features)\n",
    "            \n",
    "            # 根据场景选择输出头\n",
    "            batch_size = x.size(0)\n",
    "            outputs = torch.zeros(batch_size, 2, device=x.device)\n",
    "            \n",
    "            for i in range(6):\n",
    "                mask = (scene_id == i).unsqueeze(1).expand(-1, 2)\n",
    "                if mask.any():\n",
    "                    outputs[mask] = self.task_heads[i](mmoe_outputs[i])[mask]\n",
    "                    \n",
    "            return {\n",
    "                \"trajectory\": outputs,\n",
    "                \"scene_id\": scene_id,\n",
    "                \"scene_probs\": scene_probs,\n",
    "                \"features\": features\n",
    "            }\n",
    "    \n",
    "    # 初始化增强模型\n",
    "    enhanced_model = DiffusionPlannerWithMMoE(base_model, feature_dim)\n",
    "    return enhanced_model\n",
    "\n",
    "def prepare_datasets() -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"准备混合数据集（20% nuPlan + 80% 实车数据）\"\"\"\n",
    "    # 导入数据集类（根据实际项目结构调整）\n",
    "    from diffusion_planner.data.nuplan_dataset import NuPlanDataset\n",
    "    from diffusion_planner.data.real_world_dataset import RealWorldDataset\n",
    "    \n",
    "    # 创建数据集\n",
    "    nuplan_dataset = NuPlanDataset(\n",
    "        data_dir=\"/path/to/nuplan\",\n",
    "        split=\"train\",\n",
    "        scene_types=[\"cruise\", \"ramp\", \"curve\", \"obstacle\", \"meeting\", \"crossing\"]\n",
    "    )\n",
    "    \n",
    "    realworld_dataset = RealWorldDataset(\n",
    "        data_dir=\"/path/to/real_world_data\",\n",
    "        split=\"train\",\n",
    "        scene_types=[\"cruise\", \"ramp\", \"curve\", \"obstacle\", \"meeting\", \"crossing\"]\n",
    "    )\n",
    "    \n",
    "    # 计算混合比例\n",
    "    nuplan_size = len(nuplan_dataset)\n",
    "    realworld_size = len(realworld_dataset)\n",
    "    total_size = nuplan_size + realworld_size\n",
    "    \n",
    "    # 按20:80比例采样\n",
    "    nuplan_samples = int(total_size * 0.2)\n",
    "    realworld_samples = int(total_size * 0.8)\n",
    "    \n",
    "    # 创建采样器\n",
    "    nuplan_indices = torch.randperm(len(nuplan_dataset))[:nuplan_samples]\n",
    "    realworld_indices = torch.randperm(len(realworld_dataset))[:realworld_samples]\n",
    "    \n",
    "    nuplan_subset = torch.utils.data.Subset(nuplan_dataset, nuplan_indices)\n",
    "    realworld_subset = torch.utils.data.Subset(realworld_dataset, realworld_indices)\n",
    "    \n",
    "    # 合并数据集\n",
    "    mixed_dataset = ConcatDataset([nuplan_subset, realworld_subset])\n",
    "    \n",
    "    # 创建数据加载器\n",
    "    train_loader = DataLoader(\n",
    "        mixed_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # 准备验证集\n",
    "    val_dataset = RealWorldDataset(\n",
    "        data_dir=\"/path/to/real_world_data\",\n",
    "        split=\"val\",\n",
    "        scene_types=[\"cruise\", \"ramp\", \"curve\", \"obstacle\", \"meeting\", \"crossing\"]\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "def train_model(\n",
    "    model: nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    val_loader: DataLoader,\n",
    "    num_epochs: int = 50,\n",
    "    lr: float = 1e-4\n",
    ") -> None:\n",
    "    \"\"\"训练增强模型\"\"\"\n",
    "    # 定义优化器（只优化LoRA和MMoE参数）\n",
    "    optimizer = optim.AdamW(\n",
    "        [p for p in model.parameters() if p.requires_grad],\n",
    "        lr=lr,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "    \n",
    "    # 定义损失函数\n",
    "    criterion = diffusion_loss_func  # 使用项目中的扩散损失函数\n",
    "    \n",
    "    # 学习率调度器\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # 训练循环\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 训练阶段\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (inputs, targets, scene_labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "            scene_labels = scene_labels.to(DEVICE)\n",
    "            \n",
    "            # 前向传播\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            trajectory = outputs[\"trajectory\"]\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = criterion(outputs, targets)  # 使用项目中的损失计算方式\n",
    "            \n",
    "            # 反向传播\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        scene_correct = 0\n",
    "        scene_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets, scene_labels in val_loader:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                targets = targets.to(DEVICE)\n",
    "                scene_labels = scene_labels.to(DEVICE)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                trajectory = outputs[\"trajectory\"]\n",
    "                scene_preds = outputs[\"scene_id\"]\n",
    "                \n",
    "                # 计算损失\n",
    "                val_loss += criterion(outputs, targets).item()\n",
    "                \n",
    "                # 计算场景分类准确率\n",
    "                scene_total += scene_labels.size(0)\n",
    "                scene_correct += (scene_preds == scene_labels).sum().item()\n",
    "        \n",
    "        # 计算平均损失和准确率\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        scene_acc = 100.0 * scene_correct / scene_total\n",
    "        \n",
    "        # 打印统计信息\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Scene Acc: {scene_acc:.2f}%\")\n",
    "        \n",
    "        # 调整学习率\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # 保存最佳模型\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'scene_acc': scene_acc\n",
    "            }, f\"{OUTPUT_DIR}/best_model.pth\")\n",
    "            print(f\"Saved best model with val loss: {val_loss:.4f}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数\"\"\"\n",
    "    # 设置随机种子\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # 初始化分布式训练（如果需要）\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        ddp.init_distributed_mode()\n",
    "    \n",
    "    # 加载基模型\n",
    "    base_model = load_base_model()\n",
    "    base_model = base_model.to(DEVICE)\n",
    "    \n",
    "    # 设置MMoE和LoRA\n",
    "    feature_dim = 512  # 根据实际模型调整\n",
    "    enhanced_model = setup_mmoe_lora(base_model, feature_dim)\n",
    "    enhanced_model = enhanced_model.to(DEVICE)\n",
    "    \n",
    "    # 准备数据集\n",
    "    train_loader, val_loader = prepare_datasets()\n",
    "    \n",
    "    # 训练模型\n",
    "    train_model(enhanced_model, train_loader, val_loader, num_epochs=50)\n",
    "    \n",
    "    print(\"Training completed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "class DynamicMoE(nn.Module):\n",
    "    \"\"\"动态混合专家模型：根据输入复杂度自适应选择专家数量\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        num_experts: int,\n",
    "        expert_hidden_dim: int,\n",
    "        output_dim: int,\n",
    "        router_threshold: float = 0.7,  # 专家分数累积阈值\n",
    "        router_noise: float = 0.1,      # 路由噪声强度\n",
    "        capacity_factor: float = 1.2,   # 专家容量因子\n",
    "        min_experts: int = 1,           # 每个样本最少选择的专家数\n",
    "        max_experts: int = 4,           # 每个样本最多选择的专家数\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 专家网络\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(input_dim, expert_hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(expert_hidden_dim, output_dim)\n",
    "            )\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "\n",
    "        # self.experts = nn.ModuleList([\n",
    "        #     nn.Sequential(\n",
    "        #         nn.Linear(input_dim, expert_hidden_dim),\n",
    "        #         nn.GELU(),\n",
    "        #         nn.Linear(expert_hidden_dim, output_dim)\n",
    "        #     )\n",
    "        #     for _ in range(num_experts)\n",
    "        # ])\n",
    "\n",
    "        # 路由网络\n",
    "        self.router = nn.Linear(input_dim, num_experts)\n",
    "\n",
    "        # 动态路由参数\n",
    "        self.threshold = router_threshold\n",
    "        self.noise = router_noise\n",
    "        self.capacity_factor = capacity_factor\n",
    "        self.min_experts = min_experts\n",
    "        self.max_experts = max_experts\n",
    "        \n",
    "        # 用于负载均衡的统计\n",
    "        self.register_buffer('expert_usage', torch.zeros(num_experts))\n",
    "    \n",
    "    def compute_router_probs(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"计算路由概率并添加噪声\"\"\"\n",
    "        logits = self.router(x)\n",
    "        \n",
    "        # 添加噪声以增强探索性\n",
    "        if self.training and self.noise > 0:\n",
    "            logits += torch.randn_like(logits) * self.noise\n",
    "        \n",
    "        # 计算softmax概率\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        return probs\n",
    "    \n",
    "    def dynamic_router(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"动态路由：根据累积概率阈值选择专家\"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        print(\"x.shape: \", batch_size)\n",
    "        probs = self.compute_router_probs(x)\n",
    "        print(\"probs: \", probs.shape)\n",
    "        # 对专家概率排序\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, dim=-1, descending=True)\n",
    "        \n",
    "        # 计算累积概率\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        \n",
    "        # 确定每个样本需要的专家数量（累积概率超过阈值）\n",
    "        expert_counts = torch.sum(cumulative_probs < self.threshold, dim=-1) + 1\n",
    "        \n",
    "        # 确保专家数量在[min_experts, max_experts]范围内\n",
    "        expert_counts = torch.clamp(expert_counts, self.min_experts, self.max_experts)\n",
    "        \n",
    "        # 构建选择矩阵和权重矩阵\n",
    "        gates = torch.zeros_like(probs)\n",
    "        for i in range(batch_size):\n",
    "            k = expert_counts[i]\n",
    "            selected_indices = sorted_indices[i, :k]\n",
    "            gates[i, selected_indices] = sorted_probs[i, :k]\n",
    "        \n",
    "        # 计算每个专家的负载\n",
    "        expert_load = torch.sum(gates > 0, dim=0)\n",
    "        \n",
    "        # 更新专家使用统计\n",
    "        if self.training:\n",
    "            self.expert_usage += expert_load.detach()\n",
    "        \n",
    "        return gates, expert_load\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"模型前向传播\"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # 获取动态路由结果\n",
    "        gates, expert_load = self.dynamic_router(x)\n",
    "        \n",
    "        # 计算每个专家的容量限制\n",
    "        expert_capacity = int(batch_size * self.capacity_factor)\n",
    "        \n",
    "        # 专家选择和输出计算\n",
    "        final_output = torch.zeros(batch_size, self.experts[0][-1].out_features, device=x.device)\n",
    "        \n",
    "        for i, expert in enumerate(self.experts):\n",
    "            # 选择激活当前专家的样本\n",
    "            mask = gates[:, i] > 0\n",
    "            selected_inputs = x[mask]\n",
    "            selected_gates = gates[mask, i]\n",
    "            \n",
    "            # 容量控制：随机丢弃超出容量的样本\n",
    "            if len(selected_inputs) > expert_capacity and self.training:\n",
    "                indices = torch.randperm(len(selected_inputs), device=x.device)[:expert_capacity]\n",
    "                selected_inputs = selected_inputs[indices]\n",
    "                selected_gates = selected_gates[indices]\n",
    "                mask = mask.clone()\n",
    "                mask[mask] = False\n",
    "                mask[mask.nonzero(as_tuple=True)[0][indices]] = True\n",
    "            \n",
    "            # 计算专家输出并加权\n",
    "            if len(selected_inputs) > 0:\n",
    "                expert_output = expert(selected_inputs)\n",
    "                final_output[mask] += selected_gates.unsqueeze(1) * expert_output\n",
    "        \n",
    "        return final_output\n",
    "    \n",
    "    def get_expert_usage(self) -> torch.Tensor:\n",
    "        \"\"\"获取专家使用频率\"\"\"\n",
    "        return self.expert_usage / (self.expert_usage.sum() + 1e-8)\n",
    "\n",
    "class DynamicMoELoss(nn.Module):\n",
    "    \"\"\"Dynamic MoE的损失函数，包含负载均衡项\"\"\"\n",
    "    \n",
    "    def __init__(self, aux_weight: float = 0.01):\n",
    "        super().__init__()\n",
    "        self.aux_weight = aux_weight\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, outputs, targets, moe_model: DynamicMoE):\n",
    "        \"\"\"计算主损失和负载均衡辅助损失\"\"\"\n",
    "        main_loss = self.criterion(outputs, targets)\n",
    "        \n",
    "        # 计算负载均衡损失（鼓励专家均匀使用）\n",
    "        expert_usage = moe_model.get_expert_usage()\n",
    "        ideal_usage = torch.ones_like(expert_usage) / len(expert_usage)\n",
    "        aux_loss = F.kl_div(expert_usage.log(), ideal_usage, reduction='batchmean')\n",
    "        \n",
    "        return main_loss + self.aux_weight * aux_loss\n",
    "\n",
    "# 使用示例\n",
    "def train_with_dynamic_moe():\n",
    "    # 初始化模型\n",
    "    model = DynamicMoE(\n",
    "        input_dim=512,\n",
    "        num_experts=8,\n",
    "        expert_hidden_dim=1024,\n",
    "        output_dim=10,\n",
    "        router_threshold=0.7,\n",
    "        min_experts=1,\n",
    "        max_experts=4\n",
    "    )\n",
    "    \n",
    "    # 初始化损失函数\n",
    "    criterion = DynamicMoELoss(aux_weight=0.01)\n",
    "    \n",
    "    # 优化器\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # 模拟训练循环\n",
    "    for epoch in range(100):\n",
    "        # 生成随机输入和标签\n",
    "        inputs = torch.randn(64, 512)  # 批次大小64，特征维度512\n",
    "        targets = torch.randint(0, 10, (64,))  # 10分类任务\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets, model)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 打印专家使用情况\n",
    "        if epoch % 10 == 0:\n",
    "            expert_usage = model.get_expert_usage()\n",
    "            print(f\"Epoch {epoch}: Expert Usage = {expert_usage.tolist()}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "Epoch 0, Loss: 5.4924, Test Acc: 0.1400\n",
      "Expert Usage: [0.11962225 0.11752361 0.15949632 0.1227702  0.09129066 0.09653725\n",
      " 0.18467996 0.10807975]\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "Epoch 10, Loss: 0.0001, Test Acc: 0.1550\n",
      "Expert Usage: [0.113315   0.13295366 0.1556363  0.11586803 0.08984682 0.07776905\n",
      " 0.20512569 0.10948547]\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "Epoch 20, Loss: 0.0001, Test Acc: 0.1550\n",
      "Expert Usage: [0.11234792 0.13398913 0.15485375 0.11633445 0.09049961 0.07631375\n",
      " 0.20460781 0.11105359]\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "Epoch 30, Loss: 0.0001, Test Acc: 0.1600\n",
      "Expert Usage: [0.1123896  0.13424118 0.15475562 0.11633062 0.09057321 0.07579436\n",
      " 0.20419438 0.11172103]\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "Epoch 40, Loss: 0.0001, Test Acc: 0.1600\n",
      "Expert Usage: [0.11235595 0.13452305 0.1546895  0.11643726 0.09050896 0.07535744\n",
      " 0.20422535 0.11190248]\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  32\n",
      "probs:  torch.Size([32, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  64\n",
      "probs:  torch.Size([64, 8])\n",
      "x.shape:  8\n",
      "probs:  torch.Size([8, 8])\n",
      "x.shape:  10\n",
      "probs:  torch.Size([10, 8])\n",
      "x.shape:  10\n",
      "probs:  torch.Size([10, 8])\n",
      "复杂输入平均激活专家数: 1.50\n",
      "简单输入平均激活专家数: 4.00\n",
      "x.shape:  100\n",
      "probs:  torch.Size([100, 8])\n",
      "专家容量: 120\n",
      "实际专家负载: [47 51 49 49 54 55 50 40]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABKUAAAGGCAYAAACqvTJ0AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8ekN5oAAAACXBIWXMAAA9hAAAPYQGoP6dpAABo8UlEQVR4nO3deXxU9fX/8fdM9h1IQhIQCHuUTWSJFBUrVKRKAXGBYkG04gIqov0pVtlqDSoq+lVBrYJaEETFaitYRIGqIJsRREFWQSAJCZCVLGTu749kBmMCSSaTubO8no/HPMzcuXPnzMwD+XByzvlYDMMwBAAAAAAAALiR1ewAAAAAAAAA4H9ISgEAAAAAAMDtSEoBAAAAAADA7UhKAQAAAAAAwO1ISgEAAAAAAMDtSEoBAAAAAADA7UhKAQAAAAAAwO1ISgEAAAAAAMDtSEoBAAAAAADA7UhKAWh0N998s5KTk5167owZM2SxWFwbEAAAAADAdCSlAD9msVjqdFuzZo3ZoZri5ptvVmRkpNlhAAAAL+fONVdRUZFmzJjh1LU+/vhjWSwWtWjRQjabrcGxAEBtAs0OAIB53nrrrSr333zzTa1atara8fPPP79Br/Pqq686vbB55JFH9NBDDzXo9QEAAMzkrjWXVJGUmjlzpiTp8ssvr9dzFy1apOTkZB04cECfffaZBg0a1OB4AOBcSEoBfuymm26qcn/Dhg1atWpVteO/VlRUpPDw8Dq/TlBQkFPxSVJgYKACA/lfFQAA8F7OrrncqbCwUP/617+UlpamBQsWaNGiRR6blCosLFRERITZYQBwAdr3AJzT5Zdfrq5du2rLli267LLLFB4erocffliS9K9//UtXX321WrRooZCQELVv315/+9vfVF5eXuUav54pdeDAAVksFs2ZM0evvPKK2rdvr5CQEPXp00ebNm2q8tyaZkpZLBZNmjRJH3zwgbp27aqQkBB16dJFK1eurBb/mjVr1Lt3b4WGhqp9+/Z6+eWXXT6natmyZerVq5fCwsIUFxenm266SYcPH65yTkZGhsaPH6/zzjtPISEhSkpK0rBhw3TgwAHHOZs3b9bgwYMVFxensLAwtW3bVrfccovL4gQAAJ7LZrNp7ty56tKli0JDQ5WQkKDbb79dJ06cqHLeudYLBw4cUHx8vCRp5syZjrbAGTNm1Pr6y5cv16lTp3T99ddr1KhRev/991VcXFztvOLiYs2YMUOdOnVSaGiokpKSdO2112rv3r1V3stzzz2nbt26KTQ0VPHx8brqqqu0efNmR5wWi0ULFy6sdv1fx2tft33//ff64x//qKZNm+qSSy6RJG3btk0333yz2rVrp9DQUCUmJuqWW25RTk5OtesePnxYt956q2Pd2rZtW915550qLS3Vvn37ZLFY9Oyzz1Z73ldffSWLxaK333671s8QQP1RfgCgVjk5ORoyZIhGjRqlm266SQkJCZKkhQsXKjIyUlOmTFFkZKQ+++wzTZs2TXl5eXrqqadqve7ixYuVn5+v22+/XRaLRU8++aSuvfZa7du3r9bqqi+++ELvv/++7rrrLkVFRen555/XyJEjdfDgQcXGxkqSvvnmG1111VVKSkrSzJkzVV5erlmzZjkWa66wcOFCjR8/Xn369FFaWpoyMzP13HPP6csvv9Q333yjJk2aSJJGjhypHTt26O6771ZycrKysrK0atUqHTx40HH/yiuvVHx8vB566CE1adJEBw4c0Pvvv++yWAEAgOe6/fbbHeuKe+65R/v379cLL7ygb775Rl9++aWCgoJqXS/Ex8dr3rx5uvPOOzVixAhde+21kqTu3bvX+vqLFi3Sb3/7WyUmJmrUqFF66KGH9NFHH+n66693nFNeXq5rrrlGq1ev1qhRo3TvvfcqPz9fq1at0nfffaf27dtLkm699VYtXLhQQ4YM0Z///GedPn1a//vf/7Rhwwb17t3bqc/n+uuvV8eOHfX444/LMAxJ0qpVq7Rv3z6NHz9eiYmJ2rFjh1555RXt2LFDGzZscPwS8siRI+rbt69OnjypCRMmKCUlRYcPH9a7776roqIitWvXTv3799eiRYt03333VftcoqKiNGzYMKfiBlALAwAqTZw40fj1/xYGDBhgSDLmz59f7fyioqJqx26//XYjPDzcKC4udhwbN26c0aZNG8f9/fv3G5KM2NhY4/jx447j//rXvwxJxkcffeQ4Nn369GoxSTKCg4ONPXv2OI59++23hiTj//7v/xzHhg4daoSHhxuHDx92HNu9e7cRGBhY7Zo1GTdunBEREXHWx0tLS43mzZsbXbt2NU6dOuU4/u9//9uQZEybNs0wDMM4ceKEIcl46qmnznqt5cuXG5KMTZs21RoXAADwbr9ec/3vf/8zJBmLFi2qct7KlSurHK/LeuHYsWOGJGP69Ol1jiczM9MIDAw0Xn31Vcex3/zmN8awYcOqnPf6668bkoxnnnmm2jVsNpthGIbx2WefGZKMe+6556zn2NeCCxYsqHbOr2O3rwVHjx5d7dya1qJvv/22IclYt26d49jYsWMNq9Va4+dmj+nll182JBk//PCD47HS0lIjLi7OGDduXLXnAXAN2vcA1CokJETjx4+vdjwsLMzxc35+vrKzs3XppZeqqKhIO3furPW6N954o5o2beq4f+mll0qS9u3bV+tzBw0a5PhtnFTxG8Do6GjHc8vLy/Xpp59q+PDhatGiheO8Dh06aMiQIbVevy42b96srKws3XXXXQoNDXUcv/rqq5WSkqL//Oc/kio+p+DgYK1Zs6ZaCb6dvaLq3//+t8rKylwSHwAA8A7Lli1TTEyMfve73yk7O9tx69WrlyIjI/X5559Larz1wpIlS2S1WjVy5EjHsdGjR2vFihVV1i7vvfee4uLidPfdd1e7hr0q6b333pPFYtH06dPPeo4z7rjjjmrHfrkWLS4uVnZ2ti6++GJJ0tatWyVVtBJ+8MEHGjp0aI1VWvaYbrjhBoWGhmrRokWOxz755BNlZ2d71OwvwNeQlAJQq5YtWyo4OLja8R07dmjEiBGKiYlRdHS04uPjHX9p5+bm1nrd1q1bV7lvT1CdLXFzrufan29/blZWlk6dOqUOHTpUO6+mY8746aefJEmdO3eu9lhKSorj8ZCQED3xxBNasWKFEhISdNlll+nJJ59URkaG4/wBAwZo5MiRmjlzpuLi4jRs2DAtWLBAJSUlLokVAAB4rt27dys3N1fNmzdXfHx8lVtBQYGysrIkNd564Z///Kf69u2rnJwc7dmzR3v27FHPnj1VWlqqZcuWOc7bu3evOnfufM5NaPbu3asWLVqoWbNmDYrp19q2bVvt2PHjx3XvvfcqISFBYWFhio+Pd5xnX4seO3ZMeXl56tq16zmv36RJEw0dOlSLFy92HFu0aJFatmypK664woXvBMAvMVMKQK1++Vsou5MnT2rAgAGKjo7WrFmz1L59e4WGhmrr1q168MEHZbPZar1uQEBAjceNyjkBjfVcM0yePFlDhw7VBx98oE8++USPPvqo0tLS9Nlnn6lnz56yWCx69913tWHDBn300Uf65JNPdMstt+jpp5/Whg0bFBkZafZbAAAAjcRms6l58+ZVqnR+yT4PszHWC7t373ZsNNOxY8dqjy9atEgTJkyo93XP5WwVU7/eLOeXalqP3nDDDfrqq6/0l7/8RRdeeKEiIyNls9l01VVX1Wkt+mtjx47VsmXL9NVXX6lbt2768MMPddddd8lqpZYDaCwkpQA4Zc2aNcrJydH777+vyy67zHF8//79JkZ1RvPmzRUaGqo9e/ZUe6ymY85o06aNJGnXrl3VfoO2a9cux+N27du31/3336/7779fu3fv1oUXXqinn35a//znPx3nXHzxxbr44ov197//XYsXL9aYMWO0ZMkS/fnPf3ZJzAAAwPO0b99en376qfr3719j8uXXzrVeqG+L3KJFixQUFKS33nqr2i/9vvjiCz3//PM6ePCgWrdurfbt2+vrr79WWVnZWTelad++vT755BMdP378rNVS9ur4kydPVjlurzKvixMnTmj16tWaOXOmpk2b5ji+e/fuKufFx8crOjpa3333Xa3XvOqqqxQfH69FixYpNTVVRUVF+tOf/lTnmADUHylfAE6xL1p+WZlUWlqql156yayQqggICNCgQYP0wQcf6MiRI47je/bs0YoVK1zyGr1791bz5s01f/78KmXzK1as0A8//KCrr75aklRUVFRtS+X27dsrKirK8bwTJ05Uq/K68MILJYkWPgAAfNwNN9yg8vJy/e1vf6v22OnTpx3Jm7qsF8LDwyVVT/iczaJFi3TppZfqxhtv1HXXXVfl9pe//EWS9Pbbb0uq2E04OztbL7zwQrXr2OMaOXKkDMPQzJkzz3pOdHS04uLitG7duiqP12cdWdNaVJLmzp1b5b7VatXw4cP10UcfafPmzWeNSZICAwM1evRovfPOO1q4cKG6detWp50LATiPSikATvnNb36jpk2baty4cbrnnntksVj01ltveVT73IwZM/Tf//5X/fv315133qny8nK98MIL6tq1q9LT0+t0jbKyMj322GPVjjdr1kx33XWXnnjiCY0fP14DBgzQ6NGjlZmZqeeee07JycmOLYV//PFHDRw4UDfccIMuuOACBQYGavny5crMzNSoUaMkSW+88YZeeukljRgxQu3bt1d+fr5effVVRUdH6/e//73LPhMAAOB5BgwYoNtvv11paWlKT0/XlVdeqaCgIO3evVvLli3Tc889p+uuu65O64WwsDBdcMEFWrp0qTp16qRmzZqpa9euNc5U+vrrr7Vnzx5NmjSpxrhatmypiy66SIsWLdKDDz6osWPH6s0339SUKVO0ceNGXXrppSosLNSnn36qu+66S8OGDdNvf/tb/elPf9Lzzz+v3bt3O1rp/ve//+m3v/2t47X+/Oc/a/bs2frzn/+s3r17a926dfrxxx/r/JlFR0c75nSWlZWpZcuW+u9//1tj1f7jjz+u//73vxowYIAmTJig888/X0ePHtWyZcv0xRdfOAbISxUtfM8//7w+//xzPfHEE3WOB4BzSEoBcEpsbKz+/e9/6/7779cjjzyipk2b6qabbtLAgQM1ePBgs8OTJPXq1UsrVqzQAw88oEcffVStWrXSrFmz9MMPP9Rpd0Cpovrr0UcfrXa8ffv2uuuuu3TzzTcrPDxcs2fP1oMPPqiIiAiNGDFCTzzxhGOB06pVK40ePVqrV6/WW2+9pcDAQKWkpOidd95x7HIzYMAAbdy4UUuWLFFmZqZiYmLUt29fLVq0qMbBngAAwLfMnz9fvXr10ssvv6yHH35YgYGBSk5O1k033aT+/ftLqvt64R//+Ifuvvtu3XfffSotLdX06dNrTErZZ1gNHTr0rHENHTpUM2bM0LZt29S9e3d9/PHHjrbB9957T7GxsbrkkkvUrVs3x3MWLFig7t2767XXXtNf/vIXxcTEqHfv3vrNb37jOGfatGk6duyY3n33Xb3zzjsaMmSIVqxYoebNm9f5M1u8eLHuvvtuvfjiizIMQ1deeaVWrFhRZedlqSK59vXXX+vRRx/VokWLlJeXp5YtW2rIkCGOyjK7Xr16qUuXLvrhhx80ZsyYOscCwDkWw5PKGgDADYYPH64dO3ZUmzkAAAAA9OzZU82aNdPq1avNDgXwecyUAuDTTp06VeX+7t279fHHH+vyyy83JyAAAAB4rM2bNys9PV1jx441OxTAL1ApBcCnJSUl6eabb1a7du30008/ad68eSopKdE333xT47bHAAAA8D/fffedtmzZoqefflrZ2dnat2+fQkNDzQ4L8HnMlALg06666iq9/fbbysjIUEhIiPr166fHH3+chBQAAAAc3n33Xc2aNUudO3fW22+/TUIKcBMqpQAAAAAAAOB2zJQCAAAAAACA25GUAgAAAAAAgNt59Uwpm82mI0eOKCoqShaLxexwAACABzIMQ/n5+WrRooWsVn4f11CsvwAAQG3quv7y6qTUkSNH1KpVK7PDAAAAXuDQoUM677zzzA7D67H+AgAAdVXb+surk1JRUVGSKt5kdHS0ydEAAABPlJeXp1atWjnWDWgY1l8AAKA2dV1/eXVSyl4yHh0dzaIIAACcE61mrsH6CwAA1FVt6y8GKwAAAAAAAMDtSEoBAAAAAADA7UhKAQAAAAAAwO1ISgEAAAAAAMDtSEoBAAAAAADA7UhKAQAAAAAAwO1ISgEAAAAAAMDtSEoBAACY5MUXX1RycrJCQ0OVmpqqjRs3nvXcHTt2aOTIkUpOTpbFYtHcuXNrPO/w4cO66aabFBsbq7CwMHXr1k2bN292PH7zzTfLYrFUuV111VWufmsAAAC1IikFAABggqVLl2rKlCmaPn26tm7dqh49emjw4MHKysqq8fyioiK1a9dOs2fPVmJiYo3nnDhxQv3791dQUJBWrFih77//Xk8//bSaNm1a5byrrrpKR48eddzefvttl78/AACA2gSaHQAAAIA/euaZZ3Tbbbdp/PjxkqT58+frP//5j15//XU99NBD1c7v06eP+vTpI0k1Pi5JTzzxhFq1aqUFCxY4jrVt27baeSEhIWdNbAEAALgLlVJnkVNQosuf+ly/SVttdigAAMDHlJaWasuWLRo0aJDjmNVq1aBBg7R+/Xqnr/vhhx+qd+/euv7669W8eXP17NlTr776arXz1qxZo+bNm6tz58668847lZOTc9ZrlpSUKC8vr8rNlxiGoRkf7tCjH3wnwzDMDgcAAL9CUuosAgOsOpBTpCO5xSo9bTM7HAAA4EOys7NVXl6uhISEKscTEhKUkZHh9HX37dunefPmqWPHjvrkk09055136p577tEbb7zhOOeqq67Sm2++qdWrV+uJJ57Q2rVrNWTIEJWXl9d4zbS0NMXExDhurVq1cjo+T3Q0t1gLvzqgtzb8pJ9PnDI7HAAA/Arte2cRHhzg+PlUabmCA8nfAQAAz2az2dS7d289/vjjkqSePXvqu+++0/z58zVu3DhJ0qhRoxznd+vWTd27d1f79u21Zs0aDRw4sNo1p06dqilTpjju5+Xl+VRialdGfpWfWzULNzEaAAD8C5mWswgKsCoowCJJKio7bXI0AADAl8TFxSkgIECZmZlVjmdmZjZo1lNSUpIuuOCCKsfOP/98HTx48KzPadeuneLi4rRnz54aHw8JCVF0dHSVmy/Z+cukVGb+Oc4EAACuRlLqHMKCKqqlikprLmcHAABwRnBwsHr16qXVq8/MrrTZbFq9erX69evn9HX79++vXbt2VTn2448/qk2bNmd9zs8//6ycnBwlJSU5/brebFfGmRlZv0xQAQCAxkdS6hzCgyu6G0+RlAIAAC42ZcoUvfrqq3rjjTf0ww8/6M4771RhYaFjN76xY8dq6tSpjvNLS0uVnp6u9PR0lZaW6vDhw0pPT69S4XTfffdpw4YNevzxx7Vnzx4tXrxYr7zyiiZOnChJKigo0F/+8hdt2LBBBw4c0OrVqzVs2DB16NBBgwcPdu8H4CGqVEpl+NYQdwAAPB0zpc7BPleKSikAAOBqN954o44dO6Zp06YpIyNDF154oVauXOkYfn7w4EFZrWd+f3jkyBH17NnTcX/OnDmaM2eOBgwYoDVr1kiS+vTpo+XLl2vq1KmaNWuW2rZtq7lz52rMmDGSpICAAG3btk1vvPGGTp48qRYtWujKK6/U3/72N4WEhLjvzXuIsnKb9h4rcNzfd6xQpadtzBIFAMBNSEqdQ1hlUupUGUkpAADgepMmTdKkSZNqfMyeaLJLTk6WYRi1XvOaa67RNddcU+NjYWFh+uSTT+odp6/an12osnJDEcEBslotyi8+rb3HCnR+km/NzQIAwFPxa6BzsM+UOlXKoHMAAABfY2/d65QYpc4JUZKq7sYHAAAaF0mpcwijfQ8AAMBn2WdIpSRGqXNiRVKKYecAALgP7XvnwEwpAAAA32WviuqcEKUAq6XyGMPOAQBwF5JS58DuewAAAL7LXhXVOTH6F0kpKqUAAHAXklLnQPseAACAbyooOa2fT5ySVNG+Z7VUJKWO5BYrr7hM0aFBZoYHAIBfYKbUOYQHsfseAACAL/oxs6IiqnlUiJpGBCsmPEhJMaEVj1EtBQCAW5CUOgd7pRS77wEAAPgWxzypygHnv/yZYecAALgHSalzoH0PAADAN9mTUik1JKWYKwUAgHuQlDoHe/teEe17AAAAPmVn5S57nROjHcdSSEoBAOBWJKXOgd33AAAAfI9hGDVXSiVUJKh2ZuTJMAxTYgMAwJ+QlDqHM+17zJQCAADwFcfyS3SiqExWi9SheaTjePvmEQqwWpRXfFoZecUmRggAgH8gKXUO4Y5B51RKAQAA+Ar7IPPkuAiFVo5rkKSQwAC1i4uocg4AAGg8JKXOwbH7HjOlAAAAfIZj572EqGqPdWKuFAAAbmNqUmrGjBmyWCxVbikpKWaGVEVYELvvAQAA+Bp7FVTnxOpJqZQEklIAALhLoNkBdOnSRZ9++qnjfmCg6SE5MOgcAADA9+zKrNh5L6WGpJQ9UUX7HgAAjc/0DFBgYKASExPNDqNG4cFUSgEAAPiScpuh3ZkFkqTOidHVHk+pPLY3q0Bl5TYFBTDtAgCAxmL637K7d+9WixYt1K5dO40ZM0YHDx40OySHX86UstnYFhgAAMDbHcgpVMlpm0KDrGrdLLza4+c1DVN4cIBKy206kF1oQoQAAPgPU5NSqampWrhwoVauXKl58+Zp//79uvTSS5WfX3O5dElJifLy8qrcGpO9UkqSik9TLQUAAODt7LOiOiVEKcBqqfa41WpRpwRa+AAAcAdTk1JDhgzR9ddfr+7du2vw4MH6+OOPdfLkSb3zzjs1np+WlqaYmBjHrVWrVo0aX2jgmaQULXwAAADeb+c5dt6zS2EHPgAA3ML09r1fatKkiTp16qQ9e/bU+PjUqVOVm5vruB06dKhR47FaLQoNqviIGHYOAADg/XZlVFTa17Tznh3DzgEAcA+PSkoVFBRo7969SkpKqvHxkJAQRUdHV7k1NscOfGUkpQAAALydvfoppYYh53b2pJR9lz4AANA4TE1KPfDAA1q7dq0OHDigr776SiNGjFBAQIBGjx5tZlhVhAWxAx8AAIAvKCo9rZ+OF0k6d6WUPWF16PgpFZScdktsAAD4I1OTUj///LNGjx6tzp0764YbblBsbKw2bNig+Ph4M8Oqwj7svKiUBQkAAIA3251ZIMOQYiOCFR8Vctbzmv3i8R8zaeEDAKCxBJr54kuWLDHz5evEnpRiphQAAIB3s7funatKyi4lMUrH8ku0KyNfF7Vu2tihAQDglzxqppQnCgumfQ8AAMAX7KxHUsq+Ox878AEA0HhIStXCPlOKQecAAADezT64PKUuSSnHDnwMOwcAoLGQlKqFY/c9KqUAAAC82pn2vdp3cLYPO9+VkS/DMBo1LgAA/BVJqVrQvgcAAOD9sgtKlF1QKotF6pQQWev5HRMiZbVIJ4rKdKygxA0RAgDgf0hK1eLMoHN23wMAAPBWP1ZWSbVuFu6ohD+X0KAAJcdGSGKuFAAAjYWkVC2olAIAAPB+jiHnCbXPk7Kzz5UiKQUAQOMgKVWL8KCK36QVMegcAADAa9kTS3UZcm53Ztg5SSkAABoDSalanGnfIykFAADgrXZm1n3IuV0KlVIAADQqklK1CCUpBQAA4NVsNkO7HUmp+lRKVSSwfszMV7mNHfgAAHA1klK1CA+qnClF+x4AAIBXOnSiSEWl5QoOtCo5NrzOz2vdLFyhQVaVnLbpp5zCRowQAAD/RFKqFuy+BwAA4N3sM6E6No9UYEDdl78BVos6JdDCBwBAYyEpVQt23wMAAPBu9oRSfVr37Oy79THsHAAA1yMpVYvw4Ird95gpBQAA4J2c2XnPrjPDzgEAaDQkpWoRTqUUAACAV9uZkSepfjvv2aVUPmdXJkkpAABcjaRULUIrB52fYtA5AACA1ykuK9eBnCJJDauUOpBTSOU8AAAuRlKqFmcGnbMIAQAA8DZ7sgpUbjPUJDxIzaNC6v38+KgQxUYEyzCk3VlUSwEA4EokpWphT0qVltt0utxmcjQAAACoD8eQ84QoWSwWp65hr5Zi2DkAAK5FUqoW9t33JKmIFj4AAACvYp8F5Uzrnh3DzgEAaBwkpWoRHGBVgLXit2q08AEAAHgXe3WTM0PO7VJISgEA0ChIStXCYrEoPIgd+AAAALzRLsfOe5FOX8Oe0KJ9DwAA1yIpVQf2Fr6i0tMmRwIAAHzJiy++qOTkZIWGhio1NVUbN24867k7duzQyJEjlZycLIvForlz59Z43uHDh3XTTTcpNjZWYWFh6tatmzZv3ux43DAMTZs2TUlJSQoLC9OgQYO0e/duV781j3CyqFSZeSWSpE4JzrfvdUqoSGhlF5Qop6DEJbEBAACSUnViT0oVM1MKAAC4yNKlSzVlyhRNnz5dW7duVY8ePTR48GBlZWXVeH5RUZHatWun2bNnKzExscZzTpw4of79+ysoKEgrVqzQ999/r6efflpNmzZ1nPPkk0/q+eef1/z58/X1118rIiJCgwcPVnFxcaO8TzPZK5taNglTVGiQ09cJDw5U62bhkmjhAwDAlUhK1UEY7XsAAMDFnnnmGd12220aP368LrjgAs2fP1/h4eF6/fXXazy/T58+euqppzRq1CiFhITUeM4TTzyhVq1aacGCBerbt6/atm2rK6+8Uu3bt5dUUSU1d+5cPfLIIxo2bJi6d++uN998U0eOHNEHH3zQWG/VNPYEUkOGnNuxAx8AAK4XaHYA3iA8mKQUAABwndLSUm3ZskVTp051HLNarRo0aJDWr1/v9HU//PBDDR48WNdff73Wrl2rli1b6q677tJtt90mSdq/f78yMjI0aNAgx3NiYmKUmpqq9evXa9SoUc6/KQ90Zsh5w5NSKYlRWvV9pssqpb47nKsDOYW6pnsLl1wP8GX7jhVo+TeHddpmmB0K4HPGpLbWeU3DTXt9klJ1EB5c8TGx+x4AAHCF7OxslZeXKyEhocrxhIQE7dy50+nr7tu3T/PmzdOUKVP08MMPa9OmTbrnnnsUHByscePGKSMjw/E6v35d+2O/VlJSopKSM3OU8vLynI7P3fYeK5DkmqSUo1Iq0zVJqUmLt+pATpHaNItQt/NiXHJNwFdN/3CH/rc72+wwAJ806PzmJKU8XRiVUgAAwAvYbDb17t1bjz/+uCSpZ8+e+u677zR//nyNGzfOqWumpaVp5syZrgzTbbLzK5JpCdGhDb6WvQVwd2a+bDZDVqvF6WvlFpXpQE6RJOnbn0+SlALOwTAMfXc4V5I08qLz1CTc+flwAKprHtXwvyMbgqRUHYSz+x4AAHChuLg4BQQEKDMzs8rxzMzMsw4xr4ukpCRdcMEFVY6df/75eu+99yTJce3MzEwlJSVVed0LL7ywxmtOnTpVU6ZMcdzPy8tTq1atnI7RnY5V7pQXF1nzDK76SI6NUHCgVUWl5fr5xCm1jnX+t8q7flFt9aOLKq8AX3WsoEQnispksUh/H9FVoZXzfgH4Bgad14F90DntewAAwBWCg4PVq1cvrV692nHMZrNp9erV6tevn9PX7d+/v3bt2lXl2I8//qg2bdpIktq2bavExMQqr5uXl6evv/76rK8bEhKi6OjoKjdvUHK6XPnFFb9QjIsMbvD1AgOs6hAfKUnamdGwFsZdv3g+g9OBc7PPcUuOjSAhBfggklJ1YG/fO1VGUgoAALjGlClT9Oqrr+qNN97QDz/8oDvvvFOFhYUaP368JGns2LFVBqGXlpYqPT1d6enpKi0t1eHDh5Wenq49e/Y4zrnvvvu0YcMGPf7449qzZ48WL16sV155RRMnTpQkWSwWTZ48WY899pg+/PBDbd++XWPHjlWLFi00fPhwt77/xna8sFSSFGi1KCbMNe0+9ha+hg47/2UialdGvgyD4c3A2dj/vHVOaPhsOACeh/a9OmD3PQAA4Go33nijjh07pmnTpikjI0MXXnihVq5c6RhCfvDgQVmtZ35/eOTIEfXs2dNxf86cOZozZ44GDBigNWvWSJL69Omj5cuXa+rUqZo1a5batm2ruXPnasyYMY7n/b//9/9UWFioCRMm6OTJk7rkkku0cuVKhYaaO1PC1bLzK5JSsZHBslicn//0S64adv7LpFbuqTJl5pUoMca3Pn/AVVy5iyYAz0NSqg7YfQ8AADSGSZMmadKkSTU+Zk802SUnJ9epouaaa67RNddcc9bHLRaLZs2apVmzZtUrVm+TXei6eVJ2nV1QKWUYhmOmVEigVSWnbdqZkUdSCjgL+5+3FJJSgE+ifa8O7DOlimjfAwAA8Ar2nfdiXZiUSkmsmKe1P7tQJaedWxceyS1WfvFpBVotuqxTvKSGtwMCvqrcZjg2A6BSCvBNJKXqwN6+d4rd9wAAALxCTuVMKVcMObdLiA5RTFiQym2G9mQVOHUN+5DzdvER6tYypvIYSSmgJj/lFKrktE2hQVa1iY0wOxwAjYCkVB2EMVMKAADAq9grpVzZvmexWBrcwndmPk70mRlVJKWAGtn/nHVsHqUAq2tmwwHwLCSl6sDevsfuewAAAN6hMSqlpIbvwPfL+Tj2a+05VqDT5TbXBAj4EIacA76PpFQdMOgcAADAu2QXVM6UinBdpZSkBlc3/XJ7+1ZNwxUeHKDS0zYdyCl0WYyAr2DIOeD7SErVAe17AAAA3iW7oLJSKsq1SamGVEqVldu091jFLKrOiVGyWi3qmEALH3A2uxhyDvg8klJ1EE5SCgAAwKucqZRybftep8okUkZesXKLyur13H3HClVWbigyJFDnNQ2TJKUkNKwdEPBVp0rLHRWEJKUA30VSqg7YfQ8AAMB72GyGjlfOlIp3caVUVGiQWjapSCjtrNxJr67s53dKiJTFUjG0mWHnQM12Z+XLMKRmEcGKd+GGBQA8C0mpOnC075WVyzAMk6MBAADAuZw8VaZyW8WarZmLK6WkX7TwZdYvkbTrFzvvVbsWSSmgip2/mL9mT+IC8D0ek5SaPXu2LBaLJk+ebHYo1dh33zMMqeQ0O6MAAAB4spzK1r0m4UEKCnD9ctfZ6qaahjbbr3XweJEKS6jKB+x2sfMe4Bc8Iim1adMmvfzyy+revbvZodTIvvuexA58AAAAnu5YI82TsuvsZHVTTdvbx0aGKK6yNenHelZeAb6MnfcA/2B6UqqgoEBjxozRq6++qqZNm5odTo0CrBYFB1Z8VEVlJKUAAAA8WY59571GmkOTUtl+92NGfp1HO+QXl+nwyVOVz6/6j2xa+IDqakriAvA9pielJk6cqKuvvlqDBg2q9dySkhLl5eVVubkLw84BAAC8g33nvcZKSrWLj1BQgEX5Jacdiaba2KugEqJD1CS8agUXw86BqnIKShx/ju07XgLwTaYmpZYsWaKtW7cqLS2tTuenpaUpJibGcWvVqlUjR3hGeOVcqSLa9wAAADyavVIqNrJx2veCAqxqHx8pqe7VTTtrGHJu52w7IOCr7H8WWjcLV0RIYC1nA/BmpiWlDh06pHvvvVeLFi1SaGhonZ4zdepU5ebmOm6HDh1q5CjPcOzAR1IKAADAozV2pZRU/+qmc83H+eVufuz0DNC6B/gT09LOW7ZsUVZWli666CLHsfLycq1bt04vvPCCSkpKFBAQUOU5ISEhCglpvMXFuYQ52vdISgEAAHiy7EaulJLqX930y+3tf61j8yhZLNLxwlIdKyhR86i6/cIW8FX2dleGnAO+z7Sk1MCBA7V9+/Yqx8aPH6+UlBQ9+OCD1RJSZgsPqvioqJQCAADwbO6olKrPcHLDMM65vX1YcICSYyO0P7tQuzLySUrB71EpBfgP05JSUVFR6tq1a5VjERERio2NrXbcEzgqpdh9DwAAwKPlFNqTUo1ZKVUxG2rvsQKVnrY5dmquSWZeiXJPlSnAalGH5pE1Xy8hypGUurRjfKPEDHgDm82gUgrwI6bvvuct2H0PAADAO2TnV7TvNWalVIuYUEWFBuq0zdC+7IJznrszo2LH6OTYcIUG1dwNwA58QIWfT5xSUWm5ggOtSo6NMDscAI3Mo7YyWLNmjdkhnBWDzgEAADxfUelpR2V7bCMmpSwWizonRGnzTye0KyNfKTXsqmd3Zsj52c+pTzsg4MvsSdwO8ZEKDKCGAvB1/Cmvo3CSUgAAAB7PXiUVGmRVRHDjziita3XTueZJ/fpaP2bmq9zGDnzwX+faqRKA7yEpVUfhwRVFZcyUAgAA8FzZlfOkYiNCZLFYGvW16lrdVJehzW1iIxQaZFXJaZt+yil0XZCAl9mZyZBzwJ+QlKoje/9/ETOlAAAAPFZ2fuWQ86jGa92zsw87P1dS6nS5TXuOVcycOlflR4DVoo7NaeED6lJZCMB3kJSqozODzm0mRwIAAICzySmsHHIe0Xg779l1Tqj4R/Phk6eUV1xW4zkHcgpVetqm8OAAtWoafu7rMewcfq7kdLn2Z1dUCp5rBhsA30FSqo4cSakyKqUAAAA8laNSqhGHnNvFhAcpKSZUkrQ7s+ZE0q6MiiqpjglRslrP3U7IsHP4uz1ZBSq3GYoJC1JCdOP/GQZgPpJSdRQWxKBzAAAAT2evlIqNbPxKKan26qZdlTuJpSTU3opkv9ausyS4AF/3y9a9xp4JB8AzkJSqI/ugc5JSAAAAnutYgfsqpaRfJJLOkpSqy5DzX1/rQE6hTrHmhB9i5z3A/5CUqqMzM6VYIAAAAHiqnMqklLsqpVJqq5TKrPs/suMjQ9QsIliGIe3OoloK/qc+SVwAvoGkVB2x+x4AAIDnyy6oaN+Ld1OlVKeEM5VShmFUeayo9LQOHi+SVLd/ZFssFsfwdOZKwR9RKQX4H5JSdUSlFAAAgOc7UynlnqRUh+aRCrBalHuqTJl5JVUe+zGzQIZR0UpY13hqawcEfFVuUZky8oolnUn2AvB9JKXq6MzueySlAAAAPFFZuU0nisokSXFuat8LCQxQ27gISdLOyqHmdvYh550TI+t8PYadw1/Z//y0bBKmqNAgk6MB4C4kpeooLJjd9wAAADzZicqd96wWqUm4e5JS0tmrmxzzcRKi632ts82oAnyVPRHLPCnAv5CUqiP77nslp20qtxm1nA0AAAB3s++81ywiRAFW920nn3KWOVDOzMexty0dyy/R8cokG+APGHIO+CeSUnVkb9+TaOEDAADwRDmVQ87d1bpnd7bqpl1O/CM7MiRQrZqFVV4vr5azAd/BkHPAP5GUqqOQQKsslb9wYwc+AAAAz5NTWFEpFeemIed2KYkV7Xl7jhXodLlNUkWlU05hqSyW+g9ttrf7Mewc/sIwDP1IpRTgl0hK1ZHFYlFYEDvwAQAAeKrs/IpKqVg3V0qd1zRM4cEBKj1t04GcQklnEkptmoU7ZpPWVQo78MHPHD55SvklpxVotahdXN03BgDg/UhK1UM4w84BAAA8VnZlpVRshHsrpaxWi6Mayt7Ct9Ox8179qz4Ydg5/82PlkPP28ZEKDuSfqIA/4U98Pdh/y8VMKQAAAM9jr5SKi3JvpZRUvbrpzDypuu+89+tr/ZiZLxsb7MAPMOQc8F8kpeohPKhiBz7a9wAAADyPY6aUmyulpOrVTfbt7Z0Z2pwcF6HgAKuKSsv184lTrgsS8FDObAoAwDeQlKqHMNr3AAAAPFZ2QWVSyoRKqc6/qJQqtxmOdiRn/pEdFGBV++YVc3XYgQ/+gJ33AP9FUqoezsyUYvc9AAAAT5NTUDno3IRKKfsOfAePF2lnRp6Ky2wKCbQqOTbCyesx7Bz+oazcpr3HCiRRKQX4I5JS9cDuewAAAJ7JMAxHUiouyv1JqWYRwYqvfN2Pvj0qSeqYEKkAq8Wp6znaATNJSsG37TtWqLJyQ1EhgWrZJMzscAC4GUmpeqB9DwAAwDPlFZ9WablNkhQb4f72PelMddNH3x6RJHVOqP+Qc7vOVErBT9hbVDslRslicS6JC8B7kZSqh3B23wMAAPBI9nlSUSGBCq2sbne3zgkViaTDJyuGkzdkPo79ufuzC1VymrUnfBdDzgH/RlKqHsKD2X0PAAC4zosvvqjk5GSFhoYqNTVVGzduPOu5O3bs0MiRI5WcnCyLxaK5c+dWO2fGjBmyWCxVbikpKVXOufzyy6udc8cdd7j6rbmdY55UpDlVUlL1f1Q35B/ZidGhig4NVLnN0J6sgoaGBngshpwD/o2kVD3QvgcAAFxl6dKlmjJliqZPn66tW7eqR48eGjx4sLKysmo8v6ioSO3atdPs2bOVmJh41ut26dJFR48eddy++OKLaufcdtttVc558sknXfa+zOLYeS/S/fOk7OzDzs/cd/4f2RaLxXE9Wvjgy3baK6USSEoB/oikVD2E2wedl7H7HgAAaJhnnnlGt912m8aPH68LLrhA8+fPV3h4uF5//fUaz+/Tp4+eeuopjRo1SiEhZ0+8BAYGKjEx0XGLi4urdk54eHiVc6KjnZ995ClyKpNSZlZKdUyIlH2uedPwIMfgc2cxVwq+Lr+47Bftrt7//yEA9RdodgDehEopAAD8V3Jysm655RbdfPPNat26dYOuVVpaqi1btmjq1KmOY1arVYMGDdL69esbdO3du3erRYsWCg0NVb9+/ZSWllYt3kWLFumf//ynEhMTNXToUD366KMKDw9v0Oua7Zh95z0TK6VCgwKUHBuhfdmF6uyCoc32pNQ3B09qx5FcV4QIeJQfK3eXTIwOVUx4kMnRADADSal6ICkFAID/mjx5shYuXKhZs2bpt7/9rW699VaNGDHinFVLZ5Odna3y8nIlJCRUOZ6QkKCdO3c6HWNqaqoWLlyozp076+jRo5o5c6YuvfRSfffdd4qKqkhw/PGPf1SbNm3UokULbdu2TQ8++KB27dql999/v8ZrlpSUqKSkxHE/Ly/P6fga05lKKfOSUlJFImlfdqFLqj7s7X8bDxzX1c9Xb8MEfAVDzgH/RVKqHhy775GUAgDA70yePFmTJ0/W1q1btXDhQt19992666679Mc//lG33HKLLrroIrND1JAhQxw/d+/eXampqWrTpo3eeecd3XrrrZKkCRMmOM7p1q2bkpKSNHDgQO3du1ft27evds20tDTNnDmz8YNvIPtMqXgT2/ck6ebfJOtobrFG921YNZ0kdT+viS7rFK+dRz0zEQi4QnCgVWNSG/7nBYB3IilVD2FBlbvvlZGUAgDAX1100UW66KKL9PTTT+ull17Sgw8+qHnz5qlbt2665557NH78+FrbtuLi4hQQEKDMzMwqxzMzM885xLy+mjRpok6dOmnPnj1nPSc1NVWStGfPnhqTUlOnTtWUKVMc9/Py8tSqVSuXxegqZ3bfM7dSKrVdrD6Y2N8l1woOtOrNW/q65FoAAHgiBp3XQzjtewAA+L2ysjK98847+sMf/qD7779fvXv31j/+8Q+NHDlSDz/8sMaMGVPrNYKDg9WrVy+tXr3accxms2n16tXq16+fy2ItKCjQ3r17lZSUdNZz0tPTJems54SEhCg6OrrKzRN5wu57AACgfqiUqocz7XvsvgcAgL/ZunWrFixYoLfffltWq1Vjx47Vs88+q5SUFMc5I0aMUJ8+fep0vSlTpmjcuHHq3bu3+vbtq7lz56qwsFDjx4+XJI0dO1YtW7ZUWlqapIrh6N9//73j58OHDys9PV2RkZHq0KGDJOmBBx7Q0KFD1aZNGx05ckTTp09XQECARo8eLUnau3evFi9erN///veKjY3Vtm3bdN999+myyy5T9+7dXfZZmeFMpZS57XsAAKDuSErVA4POAQDwX3369NHvfvc7zZs3T8OHD1dQUPWdotq2batRo0bV6Xo33nijjh07pmnTpikjI0MXXnihVq5c6Rh+fvDgQVmtZ4rajxw5op49ezruz5kzR3PmzNGAAQO0Zs0aSdLPP/+s0aNHKycnR/Hx8brkkku0YcMGxcfHS6qo0Pr0008dCbBWrVpp5MiReuSRR5z9WDxCcVm58ksqfmlIpRQAAN7DYhiGYXYQzsrLy1NMTIxyc3PdUkq+71iBrnh6raJCArV95uBGfz0AANBwrlov/PTTT2rTpo0LI/NO7l5/1cXhk6fUf/ZnCg6watdjV9U60wsAADSuuq4XmClVD+HBFYVlRWXl8uJcHgAAcEJWVpa+/vrrase//vprbd682YSIYJedXzFPKjYymIQUAABehKRUPdjb98pthsrKSUoBAOBPJk6cqEOHDlU7fvjwYU2cONGEiGCXU3gmKQUAALwHSal6sA86l6RTzJUCAMCvfP/997rooouqHe/Zs6djADnMkZ1fMeSceVIAAHgXklL1EBRgVVBARUl4URk78AEA4E9CQkKUmZlZ7fjRo0cVGMjeMWbKtldKRZCUAgDAm5ialJo3b566d++u6OhoRUdHq1+/flqxYoWZIdUqLIgd+AAA8EdXXnmlpk6dqtzcXMexkydP6uGHH9bvfvc7EyPDmUop2vcAAPAmpv5a77zzztPs2bPVsWNHGYahN954Q8OGDdM333yjLl26mBnaWYUFByiv+DTtewAA+Jk5c+bosssuU5s2bdSzZ09JUnp6uhISEvTWW2+ZHJ1/s8+Uon0PAADvYmpSaujQoVXu//3vf9e8efO0YcMGj01KVezAV0KlFAAAfqZly5batm2bFi1apG+//VZhYWEaP368Ro8eraCgILPD82vZBQw6BwDAG3nMAITy8nItW7ZMhYWF6tevn9nhnNWZ9j1mSgEA4G8iIiI0YcIEs8PAr+QUMOgcAABvZHpSavv27erXr5+Ki4sVGRmp5cuX64ILLqjx3JKSEpWUlDju5+XluStMB/sOfMVlVEoBAOCPvv/+ex08eFClpaVVjv/hD38wKSJQKQUAgHcyPSnVuXNnpaenKzc3V++++67GjRuntWvX1piYSktL08yZM02I8oywYAadAwDgj/bt26cRI0Zo+/btslgsMgxDkmSxVOzMW17O2sAM5TZDxwsrEoTxVEoBAOBVnNp979ChQ/r5558d9zdu3KjJkyfrlVdeqfe1goOD1aFDB/Xq1UtpaWnq0aOHnnvuuRrPte94Y78dOnTImfAbJJykFAAAfunee+9V27ZtlZWVpfDwcO3YsUPr1q1T7969tWbNGrPD81snikplq8gPqmkElVIAAHgTp5JSf/zjH/X5559LkjIyMvS73/1OGzdu1F//+lfNmjWrQQHZbLYqLXq/FBISoujo6Co3d6sYdC523wMAwM+sX79es2bNUlxcnKxWq6xWqy655BKlpaXpnnvuMTs8v2WfJ9U0PEhBAU4tbQEAgEmc+pv7u+++U9++fSVJ77zzjrp27aqvvvpKixYt0sKFC+t8nalTp2rdunU6cOCAtm/frqlTp2rNmjUaM2aMM2G5RWgQlVIAAPij8vJyRUVFSZLi4uJ05MgRSVKbNm20a9cuM0Pza2fmSdG6BwCAt3FqplRZWZlCQir+4v/0008dgz1TUlJ09OjROl8nKytLY8eO1dGjRxUTE6Pu3bvrk08+0e9+9ztnwnILR/teGbvvAQDgT7p27apvv/1Wbdu2VWpqqp588kkFBwfrlVdeUbt27cwOz2/Zk1JxDDkHAMDrOJWU6tKli+bPn6+rr75aq1at0t/+9jdJ0pEjRxQbG1vn67z22mvOvLyp7Ekp2vcAAPAvjzzyiAoLCyVJs2bN0jXXXKNLL71UsbGxWrp0qcnR+a/syvY9KqUAAPA+TiWlnnjiCY0YMUJPPfWUxo0bpx49ekiSPvzwQ0dbn68KIykFAIBfGjx4sOPnDh06aOfOnTp+/LiaNm3q2IEP7pdTWSnFznsAAHgfp5JSl19+ubKzs5WXl6emTZs6jk+YMEHh4eEuC84ThdtnSpWRlAIAwF+UlZUpLCxM6enp6tq1q+N4s2bNTIwK0plB57HsvAcAgNdxatD5qVOnVFJS4khI/fTTT5o7d6527dql5s2buzRAT8PuewAA+J+goCC1bt1a5eX8/e9pHDOloqiUAgDA2ziVlBo2bJjefPNNSdLJkyeVmpqqp59+WsOHD9e8efNcGqCnCbUPOi9l0DkAAP7kr3/9qx5++GEdP37c7FDwC9mFVEoBAOCtnEpKbd26VZdeeqkk6d1331VCQoJ++uknvfnmm3r++eddGqCnsbfvUSkFAIB/eeGFF7Ru3Tq1aNFCnTt31kUXXVTlBnNk51MpBQCAt3JqplRRUZGioqIkSf/973917bXXymq16uKLL9ZPP/3k0gA9TbijUoqkFAAA/mT48OFmh4BfMQxDOYWVSakIklIAAHgbp5JSHTp00AcffKARI0bok08+0X333SdJysrKUnR0tEsD9DSO3fcYdA4AgF+ZPn262SHgVwpLy1VcZpMkxUXRvgcAgLdxqn1v2rRpeuCBB5ScnKy+ffuqX79+kiqqpnr27OnSAD0Ng84BAAA8Q07lkPOwoADHGg0AAHgPp/72vu6663TJJZfo6NGj6tGjh+P4wIEDNWLECJcF54lo3wMAwD9ZrVZZLJazPs7OfO53Zuc9qqQAAPBGTv9KKTExUYmJifr5558lSeedd5769u3rssA81S/b92w2Q1br2RenAADAdyxfvrzK/bKyMn3zzTd64403NHPmTJOi8m/ZBfad95gnBQCAN3IqKWWz2fTYY4/p6aefVkFBgSQpKipK999/v/7617/KanWqK9ArhFXuvidJxafLKRUHAMBPDBs2rNqx6667Tl26dNHSpUt16623mhCVf3NUSkWSlAIAwBs5lVH561//qtdee02zZ89W//79JUlffPGFZsyYoeLiYv397393aZCe5JdJqaJSklIAAPi7iy++WBMmTDA7DL+UU1kpFRdJ+x4AAN7IqYzKG2+8oX/84x/6wx/+4DjWvXt3tWzZUnfddZdPJ6WsVotCg6wqLrMx7BwAAD936tQpPf/882rZsqXZofgle6VULEkpAAC8klNJqePHjyslJaXa8ZSUFB0/frzBQXm68OBAFZeV6lQZSSkAAPxF06ZNqww6NwxD+fn5Cg8P1z//+U8TI/NfZyqlaN8DAMAbOZWU6tGjh1544QU9//zzVY6/8MIL6t69u0sC82T2Fj524AMAwH88++yzVZJSVqtV8fHxSk1NVdOmTU2MzH8dc1RKkZQCAMAbOZWUevLJJ3X11Vfr008/Vb9+/SRJ69ev16FDh/Txxx+7NEBPFB5sT0qdNjkSAADgLjfffLPZIeBXchyDzmnfAwDAGzm1Td6AAQP0448/asSIETp58qROnjypa6+9Vjt27NBbb73l6hg9TlhlUoqZUgAA+I8FCxZo2bJl1Y4vW7ZMb7zxhgkRIZv2PQAAvJpTSSlJatGihf7+97/rvffe03vvvafHHntMJ06c0GuvvebK+DwS7XsAAPiftLQ0xcXFVTvevHlzPf744yZE5N9KT9uUe6pMEkkpAAC8ldNJKX8WTqUUAAB+5+DBg2rbtm21423atNHBgwdNiMi/HS+sqJIKsFrUJCzI5GgAAIAzSEo5ITy4YhQXM6UAAPAfzZs317Zt26od//bbbxUbG2tCRP4tu3KeVLOIYFmtllrOBgAAnoiklBMcM6XKbCZHAgAA3GX06NG655579Pnnn6u8vFzl5eX67LPPdO+992rUqFFmh+d37Emp2AiGnAMA4K3qtfvetddee87HT5482ZBYvMaZ9j0qpQAA8Bd/+9vfdODAAQ0cOFCBgRVLKJvNprFjxzJTygQ5lUPO46OYJwUAgLeqV1IqJiam1sfHjh3boIC8AYPOAQDwP8HBwVq6dKkee+wxpaenKywsTN26dVObNm3MDs0vUSkFAID3q1dSasGCBY0Vh1ext+8VlZGUAgDA33Ts2FEdO3Y0Owy/l1M56Jyd9wAA8F7MlHICu+8BAOB/Ro4cqSeeeKLa8SeffFLXX3+9CRH5t+z8ykopklIAAHgtklJOCGP3PQAA/M66dev0+9//vtrxIUOGaN26dSZE5N+yHZVStO8BAOCtSEo5ITyI3fcAAPA3BQUFCg6ungAJCgpSXl6eCRH5N3ulFO17AAB4L5JSTmD3PQAA/E+3bt20dOnSaseXLFmiCy64wISI/FtOIUkpAAC8Xb0GnaOCY9A5M6UAAPAbjz76qK699lrt3btXV1xxhSRp9erVWrx4sd59912To/MvNpuhnIKK9r1Y2vcAAPBaJKWcEBbEoHMAAPzN0KFD9cEHH+jxxx/Xu+++q7CwMPXo0UOfffaZmjVrZnZ4fiWvuEynbYYkklIAAHgz2vecEO4YdE5SCgAAf3L11Vfryy+/VGFhofbt26cbbrhBDzzwgHr06OHU9V588UUlJycrNDRUqamp2rhx41nP3bFjh0aOHKnk5GRZLBbNnTu32jkzZsyQxWKpcktJSalyTnFxsSZOnKjY2FhFRkZq5MiRyszMdCp+s2QXVLTuRYUGKiQwwORoAACAs0hKOeFM+x4zpQAA8Dfr1q3TuHHj1KJFCz399NO64oortGHDhnpfZ+nSpZoyZYqmT5+urVu3qkePHho8eLCysrJqPL+oqEjt2rXT7NmzlZiYeNbrdunSRUePHnXcvvjiiyqP33ffffroo4+0bNkyrV27VkeOHNG1115b7/jNlF3ZuhfPPCkAALwa7XtOcAw6L6NSCgAAf5CRkaGFCxfqtddeU15enm644QaVlJTogw8+cHrI+TPPPKPbbrtN48ePlyTNnz9f//nPf/T666/roYceqnZ+nz591KdPH0mq8XG7wMDAsyatcnNz9dprr2nx4sWOuVgLFizQ+eefrw0bNujiiy926r24m71SitY9AAC8G5VSTrAnpcrKDZWV20yOBgAANKahQ4eqc+fO2rZtm+bOnasjR47o//7v/xp0zdLSUm3ZskWDBg1yHLNarRo0aJDWr1/foGvv3r1bLVq0ULt27TRmzBgdPHjQ8diWLVtUVlZW5XVTUlLUunXrs75uSUmJ8vLyqtzMZh9yzs57AAB4N5JSTrC370lUSwEA4OtWrFihW2+9VTNnztTVV1+tgICGzzDKzs5WeXm5EhISqhxPSEhQRkaG09dNTU3VwoULtXLlSs2bN0/79+/XpZdeqvz8fEkVFV/BwcFq0qRJnV83LS1NMTExjlurVq2cjs9V7JVSzSKolAIAwJuRlHJCcIBVVkvFz+zABwCAb/viiy+Un5+vXr16KTU1VS+88IKys7PNDqtGQ4YM0fXXX6/u3btr8ODB+vjjj3Xy5Em98847Tl9z6tSpys3NddwOHTrkwoidk02lFAAAPoGklBMsFgs78AEA4Ccuvvhivfrqqzp69Khuv/12LVmyRC1atJDNZtOqVascVUj1ERcXp4CAgGq73mVmZp5ziHl9NWnSRJ06ddKePXskSYmJiSotLdXJkyfr/LohISGKjo6ucjNbTmWlVFwUSSkAALwZSSknsQMfAAD+JSIiQrfccou++OILbd++Xffff79mz56t5s2b6w9/+EO9rhUcHKxevXpp9erVjmM2m02rV69Wv379XBZzQUGB9u7dq6SkJElSr169FBQUVOV1d+3apYMHD7r0dRubvX0vjvY9AAC8GkkpJzl24KNSCgAAv9O5c2c9+eST+vnnn/X22287dY0pU6bo1Vdf1RtvvKEffvhBd955pwoLCx278Y0dO1ZTp051nF9aWqr09HSlp6ertLRUhw8fVnp6uqMKSpIeeOABrV27VgcOHNBXX32lESNGKCAgQKNHj5YkxcTE6NZbb9WUKVP0+eefa8uWLRo/frz69evnNTvvSVJOYWX7HpVSAAB4tUCzA/BWYUH2SimSUgAA+KuAgAANHz5cw4cPr/dzb7zxRh07dkzTpk1TRkaGLrzwQq1cudIx/PzgwYOyWs/8/vDIkSPq2bOn4/6cOXM0Z84cDRgwQGvWrJEk/fzzzxo9erRycnIUHx+vSy65RBs2bFB8fLzjec8++6ysVqtGjhypkpISDR48WC+99JJzH4BJsvMrKqViqZQCAMCrWQzDMMx68bS0NL3//vvauXOnwsLC9Jvf/EZPPPGEOnfuXKfn5+XlKSYmRrm5uW6fb3DtS19q68GTevlPvTS4i+tmPwAAANcyc73gi8z+PE+Vluv8aSslSdtmXKno0CC3xwAAAM6trusFU9v31q5dq4kTJ2rDhg1atWqVysrKdOWVV6qwsNDMsOrEPuic9j0AAAD3sc+TCg6wKiqEon8AALyZqX+Tr1y5ssr9hQsXqnnz5tqyZYsuu+wyk6Kqm1Da9wAAANzOMU8qMlgWi8XkaAAAQEN41K+XcnNzJUnNmjWr8fGSkhKVlJQ47ufl5bklrpqEs/seAACA2znmSUUy5BwAAG/nMbvv2Ww2TZ48Wf3791fXrl1rPCctLU0xMTGOW6tWrdwc5RnsvgcAAOB+OYUVSam4SIacAwDg7TwmKTVx4kR99913WrJkyVnPmTp1qnJzcx23Q4cOuTHCqsLslVJlJKUAAADcJbugon2PSikAALyfR7TvTZo0Sf/+97+1bt06nXfeeWc9LyQkRCEhnrEAoVIKAADA/eyDzuNISgEA4PVMTUoZhqG7775by5cv15o1a9S2bVszw6kXdt8DAABwP3ulFO17AAB4P1OTUhMnTtTixYv1r3/9S1FRUcrIyJAkxcTEKCwszMzQauXYfY/2PQAAALfJoVIKAACfYepMqXnz5ik3N1eXX365kpKSHLelS5eaGVadnGnfY/c9AAAAd7G378VSKQUAgNczvX3PW9mTUkW07wEAALhNjqN9j0opAAC8ncfsvudtwoJISgEAALjT6XKbjhfZd9+jUgoAAG9HUspJDDoHAABwrxNFZTIMyWKRmoWTlAIAwNuRlHJSmH2mFIPOAQAA3MI+T6ppeLACA1jGAgDg7fjb3EnMlAIAAHCvM/OkqJICAMAXkJRykn2mFLvvAQAAuIdj570IhpwDAOALSEo5yVEpVVbu1bsIAgAAeAt7UiouiqQUAAC+gKSUk+wzpQxDKjltMzkaAAAA35dd2b4XG0H7HgAAvoCklJPsu+9JzJUCAABwh5zKSql4KqUAAPAJJKWcFGC1KDiw4uNjBz4AAIDGd2amFJVSAAD4ApJSDWCfK8WwcwAAgMaXU2jffY9KKQAAfAFJqQaw78BH+x4AAEDjy86vrJSKpFIKAABfQFKqAezDzklKAQAANC7DMJRNpRQAAD6FpFQDnGnfIykFAADQmPJLTqu0csdjklIAAPgGklINEB5UsQMflVIAAACNK6egokoqIjjAUa0OAAC8G0mpBjjTvsegcwAAgMbk2HmPKikAAHwGSakGsLfvFZdRKQUAANCYcgoYcg4AgK8hKdUA7L4HAADgHscKGHIOAICvISnVAOy+BwAA4B72Sqk4KqUAAPAZJKUawLH7Hu17AAAAjSqHSikAAHwOSakGCAu2777HoHMAAIDG5Bh0HkGlFAAAvoKkVAOE074HAADgFo5KqSgqpQAA8BUkpRrA0b5HUgoAAKBRnamUIikFAICvICnVAPbd95gpBQAA0LjsSan4KNr3AADwFSSlGoDd9wAAABpfyely5RVXzPCkUgoAAN9BUqoBaN8DAABofMcLK+ZJBVotigkLMjkaAADgKiSlGiAsiN33AAAAGlt2fkVSqllEsKxWi8nRAAAAVyEp1QBUSgEAADS+7MKKeVJxkbTuAQDgS0hKNYA9KVXEoHMAAIBGk51fufNeJEPOAQDwJSSlGiCMSikAAIBGl1M5UyqeSikAAHwKSakGCAuqSEqVnLap3GaYHA0AAIBvolIKAADfRFKqAcKDAx0/n6KFDwAAoFHYK6WYKQUAgG8hKdUAoUFWWSo3gGEHPgAAgMaRXWCvlCIpBQCALyEp1QAWi8XRwsdcKQAAgMaRXWCvlKJ9DwAAX0JSqoEcO/CRlAIAAGgU9kop2vcAAPAtJKUaKIykFAAAcNKLL76o5ORkhYaGKjU1VRs3bjzruTt27NDIkSOVnJwsi8WiuXPnnvPas2fPlsVi0eTJk6scv/zyy2WxWKrc7rjjDhe8m8Zhsxk6zkwpAAB8EkmpBgoPqhh2XsygcwAAUA9Lly7VlClTNH36dG3dulU9evTQ4MGDlZWVVeP5RUVFateunWbPnq3ExMRzXnvTpk16+eWX1b179xofv+2223T06FHH7cknn2zw+2ksJ0+VOXY5bhZB+x4AAL6EpFQDhVIpBQAAnPDMM8/otttu0/jx43XBBRdo/vz5Cg8P1+uvv17j+X369NFTTz2lUaNGKSTk7BVDBQUFGjNmjF599VU1bdq0xnPCw8OVmJjouEVHR7vkPTWGnMrWvZiwIAUHsnQFAMCX8Dd7A4UH2ZNS7L4HAADqprS0VFu2bNGgQYMcx6xWqwYNGqT169c36NoTJ07U1VdfXeXav7Zo0SLFxcWpa9eumjp1qoqKihr0mo3pmGPnPaqkAADwNYFmB+Dt7IPO2X0PAADUVXZ2tsrLy5WQkFDleEJCgnbu3On0dZcsWaKtW7dq06ZNZz3nj3/8o9q0aaMWLVpo27ZtevDBB7Vr1y69//77NZ5fUlKikpISx/28vDyn43NGTgHzpAAA8FWmJqXWrVunp556Slu2bNHRo0e1fPlyDR8+3MyQ6o1B5wAAwBMcOnRI9957r1atWqXQ0NCznjdhwgTHz926dVNSUpIGDhyovXv3qn379tXOT0tL08yZMxsl5ro4s/MelVIAAPgaU9v3CgsL1aNHD7344otmhtEgjkopBp0DAIA6iouLU0BAgDIzM6scz8zMrHWI+dls2bJFWVlZuuiiixQYGKjAwECtXbtWzz//vAIDA1VeXvNaJTU1VZK0Z8+eGh+fOnWqcnNzHbdDhw45FZ+zqJQCAMB3mVopNWTIEA0ZMsTMEBosPLjiI2SmFAAAqKvg4GD16tVLq1evdlSJ22w2rV69WpMmTXLqmgMHDtT27durHBs/frxSUlL04IMPKiAgoMbnpaenS5KSkpJqfDwkJOScg9Ubm71SKjaCpBQAAL6GmVINFBpknyllMzkSAADgTaZMmaJx48apd+/e6tu3r+bOnavCwkKNHz9ekjR27Fi1bNlSaWlpkiqGo3///feOnw8fPqz09HRFRkaqQ4cOioqKUteuXau8RkREhGJjYx3H9+7dq8WLF+v3v/+9YmNjtW3bNt1333267LLL1L17dze++7rLtldKRdG+BwCAr/GqpJTZgzZrcqZ9j0opAABQdzfeeKOOHTumadOmKSMjQxdeeKFWrlzpGH5+8OBBWa1nJi0cOXJEPXv2dNyfM2eO5syZowEDBmjNmjV1es3g4GB9+umnjgRYq1atNHLkSD3yyCMufW+uRKUUAAC+y6uSUmYP2qxJOIPOAQCAkyZNmnTWdr1fJ5qSk5NlGEa9rv/ra7Rq1Upr166t1zXMllPIoHMAAHyVqYPO68vsQZs1Yfc9AACAxpOdz6BzAAB8lVdVSpk9aLMmkSEVH+GJwlKTIwEAAPAtRaWnHTscx1IpBQCAzzE1KVVQUFBl++H9+/crPT1dzZo1U+vWrU2MrO66toyRJG07nKvisnLH4HMAAAA0jL1KKiTQ6vhFIAAA8B2mtu9t3rxZPXv2dAztnDJlinr27Klp06aZGVa9tIuLUHxUiEpP25R+6KTZ4QAAAPiMbMc8qRBZLBaTowEAAK5m6q+cLr/88noP7PQ0FotFF7eL1UffHtH6vTm6uF2s2SEBAAD4hJwC+zwpWvcAAPBFXjXo3FP1q0xEbdiXY3IkAAAAviO7oKJSKpYh5wAA+CSSUi5wcbtmkqRvDp1UcRm78AEAALhCToG9fY9KKQAAfBFJKRdoGxeh5pVzpbYePGF2OAAAAD4hu7J9j0opAAB8E0kpF7BYLOrX3t7Cd9zkaAAAAHxDdsGZQecAAMD3kJRykYuZKwUAAOBS2bTvAQDg00hKuYg9KZV+kLlSAAAArnBm9z0qpQAA8EUkpVwkOTZcidGhKi23aetPzJUCAABoqDO771EpBQCALyIp5SIWi8WxCx8tfAAAAA1zutymE0VlkqiUAgDAV5GUciF7C996klIAAAANcrywonXPapGahlMpBQCALyIp5UL2HfjSD53UqVLmSgEAADgru3KeVLOIYAVYLSZHAwAAGgNJKRdq3SxcSTGhKis3tPUgc6UAAACc5ZgnFUHrHgAAvoqklAtVzJWqbOHbSwsfAACAs3IKK5JScVG07gEA4KtISrlYv8qkFMPOAQAAnJedX9G+R6UUAAC+i6SUi9krpb79+aSKSk+bHA0AAIB3yrZXSrHzHgAAPouklIu1ahamFpVzpbb8xFwpAAAAZzgqpSJp3wMAwFeRlHIxi8Wii9vTwgcAANAQ9plS8VRKAQDgs0hKNYKLHXOljpscCQAAgHdy7L5HpRQAAD6LpFQjsA87//bQSRWWMFcKAACgvnIKKtr3mCkFAIDvIinVCFo1C1fLJmE6bWOuFAAAQH0ZhuFISlEpBQCA7yIp1UjOtPAxVwoAAKA+8opPq7TcJolKKQAAfBlJqUZycbtmkqT1JKUAAADqxT5PKjIkUKFBASZHAwAAGgtJqUZir5Ta9nMuc6UAAADq4cw8KVr3AADwZSSlGkmrZuE6r2mYym2GNjNXCgAAoM7O7LxH6x4AAL6MpFQjsldLrd9LCx8AAEBd5VQmpaiUAgDAt5GUakT9GHYOAABQb8ccO+9RKQUAgC8jKdWIUiuHnW8/nKsC5koBAADUiaNSKoJKKQAAfBlJqUZ0XtNwtWpWMVdq04HjZocDAADgFewzpeKiqJQCAMCXkZRqZLTwAQAA1I99973YCJJSAAD4MpJSjexiR1KKSikAAIC6yGbQOQAAfoGkVCOzJ6W+O5yr/OIyk6MBAADwfDkMOgcAwC+QlGpkLZqEqU1suMpthjYfOGF2OAAAAB6tuKxc+ZUbxMSTlAIAwKeRlHKDi9syVwoAAKAucgorqqSCAiyKDgs0ORoAANCYSEq5wcXtm0mS1pOUAgAAOKecynlSsREhslgsJkcDAAAaE0kpN/jlXKk85koBAACclX3IeSxDzgEA8HkkpdwgKSZMybHhshnSO5sOyTAMs0MCAADwSNmVQ87jmCcFAIDPIynlJld2SZQkPfafHzRy3lfa8tNxkyMCAADwPFRKAQDgP0hKucmU33XS5EEdFRYUoK0HT2rkvPW6460t2p9daHZoAAAAHiOnslKKnfcAAPB9JKXcJDQoQJMHddLav1yuUX1ayWqRVu7I0O+eWasZH+7Q8cqdZgAAAPwZlVIAAPgPklJu1jw6VLNHdteKey/TbzvH67TN0MKvDmjAk59r3pq9Ki4rNztEAAAA0+QwUwoAAL/hEUmpF198UcnJyQoNDVVqaqo2btxodkiNrnNilBaM76tFf07VBUnRyi85rSdW7tQVc9ZoycaDVE4BAOAH6rMG2rFjh0aOHKnk5GRZLBbNnTv3nNeePXu2LBaLJk+eXOV4cXGxJk6cqNjYWEVGRmrkyJHKzMx0wbtxjTOVUiSlAADwdaYnpZYuXaopU6Zo+vTp2rp1q3r06KHBgwcrKyvL7NDcon+HOP377kv09PU9lBQTqiO5xXro/e3q9dgqDXvxSz2z6kdtPXhC5TZ27AMAwJfUdw1UVFSkdu3aafbs2UpMTDzntTdt2qSXX35Z3bt3r/bYfffdp48++kjLli3T2rVrdeTIEV177bUueU+ucGb3Pdr3AADwdRbDMEzNdqSmpqpPnz564YUXJEk2m02tWrXS3XffrYceeuicz83Ly1NMTIxyc3MVHR3tjnAbVXFZuRZ+dUAffHNYOzPyqzzWJDxIl3aM14BO8bqsU5yaR4VWe75hGCosLVfuqTLlFpUp91SZCkpOKzo0UPFRIYqPClFkSKAsFou73hIAAKbz1PVCQ9ZAycnJmjx5crUqKEkqKCjQRRddpJdeekmPPfaYLrzwQkdVVW5uruLj47V48WJdd911kqSdO3fq/PPP1/r163XxxRfXGndjfp7lNkMd//qxbIb09cMDlRBdfb0DAAA8X13XC4FujKma0tJSbdmyRVOnTnUcs1qtGjRokNavX29iZOYIDQrQHQPa644B7ZWZV6y1Px7T2l3H9L/dx3SyqEwffXtEH317RJLUpUW04qNCqiSgck+V6XQtFVWhQdaKBFVkiCNRFRcZoiZhQbIZks0wVG4zVG4YstkMnbZV/LfcMFRukwKsUlhQgEIrb/afw4KtjmOhgQEKDKie+KopFWZOfoykHAB4oubRIYoODTI7DLdozDXQxIkTdfXVV2vQoEF67LHHqjy2ZcsWlZWVadCgQY5jKSkpat26dZ2TUo3pZFGp7EuZZhFUSgEA4OtMTUplZ2ervLxcCQkJVY4nJCRo586d1c4vKSlRSUmJ435eXl6jx2iWhOhQ3dC7lW7o3Uqny2369ueTWrPrmNb+eEzbfs7VjiNnf+/BAVZFhwWpSXiQIkIClXeqTMfyS1RQclrFZTYdOn5Kh46fcuO7AQCgds+NulDDLmxpdhhuUd81UF0tWbJEW7du1aZNm2p8PCMjQ8HBwWrSpEm1183IyKjxOe5cf9lb95qEBykowPQpEwAAoJGZmpSqr7S0NM2cOdPsMNwuMMCqXm2aqVebZrr/ys7KLijRV3tzVFxWrpiwIDUJC1JMeFDlz8EKDbLW2KJXVHpa2fmlOlZQomP5JWf+m1+ivFNlslotCrCo8r8WBVgtVX+2WGQzDBWXletUWXnlf20qrvzZfvxUqU2/7gqtqX6rrp2jTNMC/Ed9GspdWWlpbiM77EhCNMyhQ4d07733atWqVQoNdV3bmzvXX0WlpxUfFaJYqqQAAPALpial4uLiFBAQUG3Hl8zMzBoHeE6dOlVTpkxx3M/Ly1OrVq0aPU5PExcZoj/0aFHv54UHB6p1bKBax4Y3QlQAAKCu6rsGqostW7YoKytLF110keNYeXm51q1bpxdeeEElJSVKTExUaWmpTp48WaVa6lyv6871V8/WTbXpr4Pq/IsrAADg3Uz9lWRwcLB69eql1atXO47ZbDatXr1a/fr1q3Z+SEiIoqOjq9wAAAC8TX3XQHUxcOBAbd++Xenp6Y5b7969NWbMGKWnpysgIEC9evVSUFBQldfdtWuXDh48eNbXNWP9xaYsAAD4B9Pb96ZMmaJx48apd+/e6tu3r+bOnavCwkKNHz/e7NAAAAAaTW1roLFjx6ply5ZKS0uTVDEc/fvvv3f8fPjwYaWnpysyMlIdOnRQVFSUunbtWuU1IiIiFBsb6zgeExOjW2+9VVOmTFGzZs0UHR2tu+++W/369TN9yDkAAPA/pielbrzxRh07dkzTpk1TRkaGLrzwQq1cubLa4E8AAABfUtsa6ODBg7JazxS1HzlyRD179nTcnzNnjubMmaMBAwZozZo1dX7dZ599VlarVSNHjlRJSYkGDx6sl156yWXvCwAAoK4shhc37efl5SkmJka5ubm08gEAgBqxXnAtPk8AAFCbuq4X2OYGAAAAAAAAbkdSCgAAAAAAAG5HUgoAAAAAAABuR1IKAAAAAAAAbkdSCgAAAAAAAG5HUgoAAAAAAABuR1IKAAAAAAAAbhdodgANYRiGJCkvL8/kSAAAgKeyrxPs6wY0DOsvAABQm7quv7w6KZWfny9JatWqlcmRAAAAT5efn6+YmBizw/B6rL8AAEBd1bb+shhe/GtDm82mI0eOKCoqShaLxeXXz8vLU6tWrXTo0CFFR0e7/PqoHd+B+fgOPAPfg/n4Dszn7HdgGIby8/PVokULWa1MLmgo1l++j+/AfHwH5uM78Ax8D+Zr7PWXV1dKWa1WnXfeeY3+OtHR0fwBMBnfgfn4DjwD34P5+A7M58x3QIWU67D+8h98B+bjOzAf34Fn4HswX2Otv/h1IQAAAAAAANyOpBQAAAAAAADcjqTUOYSEhGj69OkKCQkxOxS/xXdgPr4Dz8D3YD6+A/PxHfgHvmfz8R2Yj+/AfHwHnoHvwXyN/R149aBzAAAAAAAAeCcqpQAAAAAAAOB2JKUAAAAAAADgdiSlAAAAAAAA4HYkpc7ixRdfVHJyskJDQ5WamqqNGzeaHZJPW7dunYYOHaoWLVrIYrHogw8+qPK4YRiaNm2akpKSFBYWpkGDBmn37t3mBOuj0tLS1KdPH0VFRal58+YaPny4du3aVeWc4uJiTZw4UbGxsYqMjNTIkSOVmZlpUsS+Z968eerevbuio6MVHR2tfv36acWKFY7H+fzdb/bs2bJYLJo8ebLjGN9D45oxY4YsFkuVW0pKiuNxPn/fxvrLvVh/mY/1l/lYf3ke1l/uZ+b6i6RUDZYuXaopU6Zo+vTp2rp1q3r06KHBgwcrKyvL7NB8VmFhoXr06KEXX3yxxseffPJJPf/885o/f76+/vprRUREaPDgwSouLnZzpL5r7dq1mjhxojZs2KBVq1aprKxMV155pQoLCx3n3Hffffroo4+0bNkyrV27VkeOHNG1115rYtS+5bzzztPs2bO1ZcsWbd68WVdccYWGDRumHTt2SOLzd7dNmzbp5ZdfVvfu3asc53tofF26dNHRo0cdty+++MLxGJ+/72L95X6sv8zH+st8rL88C+sv85i2/jJQTd++fY2JEyc67peXlxstWrQw0tLSTIzKf0gyli9f7rhvs9mMxMRE46mnnnIcO3nypBESEmK8/fbbJkToH7KysgxJxtq1aw3DqPjMg4KCjGXLljnO+eGHHwxJxvr1680K0+c1bdrU+Mc//sHn72b5+flGx44djVWrVhkDBgww7r33XsMw+HPgDtOnTzd69OhR42N8/r6N9Ze5WH95BtZfnoH1lzlYf5nHzPUXlVK/Ulpaqi1btmjQoEGOY1arVYMGDdL69etNjMx/7d+/XxkZGVW+k5iYGKWmpvKdNKLc3FxJUrNmzSRJW7ZsUVlZWZXvISUlRa1bt+Z7aATl5eVasmSJCgsL1a9fPz5/N5s4caKuvvrqKp+3xJ8Dd9m9e7datGihdu3aacyYMTp48KAkPn9fxvrL87D+MgfrL3Ox/jIX6y9zmbX+CmzwFXxMdna2ysvLlZCQUOV4QkKCdu7caVJU/i0jI0OSavxO7I/BtWw2myZPnqz+/fura9eukiq+h+DgYDVp0qTKuXwPrrV9+3b169dPxcXFioyM1PLly3XBBRcoPT2dz99NlixZoq1bt2rTpk3VHuPPQeNLTU3VwoUL1blzZx09elQzZ87UpZdequ+++47P34ex/vI8rL/cj/WXeVh/mY/1l7nMXH+RlAJQzcSJE/Xdd99V6SOGe3Tu3Fnp6enKzc3Vu+++q3Hjxmnt2rVmh+U3Dh06pHvvvVerVq1SaGio2eH4pSFDhjh+7t69u1JTU9WmTRu98847CgsLMzEyAGhcrL/Mw/rLXKy/zGfm+ov2vV+Ji4tTQEBAtUnymZmZSkxMNCkq/2b/3PlO3GPSpEn697//rc8//1znnXee43hiYqJKS0t18uTJKufzPbhWcHCwOnTooF69eiktLU09evTQc889x+fvJlu2bFFWVpYuuugiBQYGKjAwUGvXrtXzzz+vwMBAJSQk8D24WZMmTdSpUyft2bOHPwc+jPWX52H95V6sv8zF+stcrL88jzvXXySlfiU4OFi9evXS6tWrHcdsNptWr16tfv36mRiZ/2rbtq0SExOrfCd5eXn6+uuv+U5cyDAMTZo0ScuXL9dnn32mtm3bVnm8V69eCgoKqvI97Nq1SwcPHuR7aEQ2m00lJSV8/m4ycOBAbd++Xenp6Y5b7969NWbMGMfPfA/uVVBQoL179yopKYk/Bz6M9ZfnYf3lHqy/PBPrL/di/eV53Lr+avCodB+0ZMkSIyQkxFi4cKHx/fffGxMmTDCaNGliZGRkmB2az8rPzze++eYb45tvvjEkGc8884zxzTffGD/99JNhGIYxe/Zso0mTJsa//vUvY9u2bcawYcOMtm3bGqdOnTI5ct9x5513GjExMcaaNWuMo0ePOm5FRUWOc+644w6jdevWxmeffWZs3rzZ6Nevn9GvXz8To/YtDz30kLF27Vpj//79xrZt24yHHnrIsFgsxn//+1/DMPj8zfLL3V8Mg++hsd1///3GmjVrjP379xtffvmlMWjQICMuLs7IysoyDIPP35ex/nI/1l/mY/1lPtZfnon1l3uZuf4iKXUW//d//2e0bt3aCA4ONvr27Wts2LDB7JB82ueff25IqnYbN26cYRgV2xI/+uijRkJCghESEmIMHDjQ2LVrl7lB+5iaPn9JxoIFCxznnDp1yrjrrruMpk2bGuHh4caIESOMo0ePmhe0j7nllluMNm3aGMHBwUZ8fLwxcOBAx4LIMPj8zfLrRRHfQ+O68cYbjaSkJCM4ONho2bKlceONNxp79uxxPM7n79tYf7kX6y/zsf4yH+svz8T6y73MXH9ZDMMwGl5vBQAAAAAAANQdM6UAAAAAAADgdiSlAAAAAAAA4HYkpQAAAAAAAOB2JKUAAAAAAADgdiSlAAAAAAAA4HYkpQAAAAAAAOB2JKUAAAAAAADgdiSlAAAAAAAA4HYkpQBAksVi0QcffGB2GAAAAH6D9RcAklIATHfzzTfLYrFUu1111VVmhwYAAOCTWH8B8ASBZgcAAJJ01VVXacGCBVWOhYSEmBQNAACA72P9BcBsVEoB8AghISFKTEyscmvatKmkitLuefPmaciQIQoLC1O7du307rvvVnn+9u3bdcUVVygsLEyxsbGaMGGCCgoKqpzz+uuvq0uXLgoJCVFSUpImTZpU5fHs7GyNGDFC4eHh6tixoz788MPGfdMAAAAmYv0FwGwkpQB4hUcffVQjR47Ut99+qzFjxmjUqFH64YcfJEmFhYUaPHiwmjZtqk2bNmnZsmX69NNPqyx65s2bp4kTJ2rChAnavn27PvzwQ3Xo0KHKa8ycOVM33HCDtm3bpt///vcaM2aMjh8/7tb3CQAA4ClYfwFodAYAmGzcuHFGQECAERERUeX297//3TAMw5Bk3HHHHVWek5qaatx5552GYRjGK6+8YjRt2tQoKChwPP6f//zHsFqtRkZGhmEYhtGiRQvjr3/961ljkGQ88sgjjvsFBQWGJGPFihUue58AAACegvUXAE/ATCkAHuG3v/2t5s2bV+VYs2bNHD/369evymP9+vVTenq6JOmHH35Qjx49FBER4Xi8f//+stls2rVrlywWi44cOaKBAweeM4bu3bs7fo6IiFB0dLSysrKcfUsAAAAejfUXALORlALgESIiIqqVc7tKWFhYnc4LCgqqct9ischmszVGSAAAAKZj/QXAbMyUAuAVNmzYUO3++eefL0k6//zz9e2336qwsNDx+Jdffimr1arOnTsrKipKycnJWr16tVtjBgAA8GasvwA0NiqlAHiEkpISZWRkVDkWGBiouLg4SdKyZcvUu3dvXXLJJVq0aJE2btyo1157TZI0ZswYTZ8+XePGjdOMGTN07Ngx3X333frTn/6khIQESdKMGTN0xx13qHnz5hoyZIjy8/P15Zdf6u6773bvGwUAAPAQrL8AmI2kFACPsHLlSiUlJVU51rlzZ+3cuVNSxc4sS5Ys0V133aWkpCS9/fbbuuCCCyRJ4eHh+uSTT3TvvfeqT58+Cg8P18iRI/XMM884rjVu3DgVFxfr2Wef1QMPPKC4uDhdd9117nuDAAAAHob1FwCzWQzDMMwOAgDOxWKxaPny5Ro+fLjZoQAAAPgF1l8A3IGZUgAAAAAAAHA7klIAAAAAAABwO9r3AAAAAAAA4HZUSgEAAAAAAMDtSEoBAAAAAADA7UhKAQAAAAAAwO1ISgEAAAAAAMDtSEoBAAAAAADA7UhKAQAAAAAAwO1ISgEAAAAAAMDtSEoBAAAAAADA7UhKAQAAAAAAwO3+P/GGIanEwxUfAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 使用提供的DynamicMoE和DynamicMoELoss类\n",
    "\n",
    "# 创建一个小型测试数据集\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=512,\n",
    "    n_informative=400,\n",
    "    n_classes=10,\n",
    "    random_state=42\n",
    ")\n",
    "X = torch.FloatTensor(X)\n",
    "y = torch.LongTensor(y)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 创建数据加载器\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# 初始化模型\n",
    "model = DynamicMoE(\n",
    "    input_dim=512,\n",
    "    num_experts=8,\n",
    "    expert_hidden_dim=1024,\n",
    "    output_dim=10,\n",
    "    router_threshold=0.7,\n",
    "    min_experts=1,\n",
    "    max_experts=4\n",
    ")\n",
    "\n",
    "# 初始化损失函数和优化器\n",
    "criterion = DynamicMoELoss(aux_weight=0.01)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "train_losses = []\n",
    "test_accuracies = []\n",
    "expert_usage_history = []\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets, model)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "    \n",
    "    # 评估模型\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    test_accuracy = correct / total\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    # 记录专家使用情况\n",
    "    expert_usage = model.get_expert_usage().cpu().numpy()\n",
    "    expert_usage_history.append(expert_usage)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {epoch_loss:.4f}, Test Acc: {test_accuracy:.4f}')\n",
    "        print(f'Expert Usage: {expert_usage}')\n",
    "\n",
    "# 绘制训练损失和测试准确率曲线\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(test_accuracies)\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_metrics.png')\n",
    "\n",
    "# 绘制专家使用情况随时间的变化\n",
    "plt.figure(figsize=(10, 6))\n",
    "expert_usage_history = np.array(expert_usage_history)\n",
    "for i in range(expert_usage_history.shape[1]):\n",
    "    plt.plot(expert_usage_history[:, i], label=f'Expert {i}')\n",
    "plt.title('Expert Usage Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Usage Ratio')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig('expert_usage.png')\n",
    "\n",
    "# 测试不同复杂度输入的专家选择数量\n",
    "complex_input = torch.randn(10, 512) * 5  # 高方差 = 更复杂\n",
    "simple_input = torch.randn(10, 512) * 0.5  # 低方差 = 更简单\n",
    "\n",
    "with torch.no_grad():\n",
    "    complex_gates, _ = model.dynamic_router(complex_input.to(device))\n",
    "    simple_gates, _ = model.dynamic_router(simple_input.to(device))\n",
    "    \n",
    "complex_expert_count = torch.sum(complex_gates > 1e-6, dim=1).cpu().numpy()\n",
    "simple_expert_count = torch.sum(simple_gates > 1e-6, dim=1).cpu().numpy()\n",
    "\n",
    "print(f\"复杂输入平均激活专家数: {np.mean(complex_expert_count):.2f}\")\n",
    "print(f\"简单输入平均激活专家数: {np.mean(simple_expert_count):.2f}\")\n",
    "\n",
    "# 测试专家容量控制\n",
    "batch_size = 100\n",
    "large_batch = torch.randn(batch_size, 512).to(device)\n",
    "with torch.no_grad():\n",
    "    gates, expert_load = model.dynamic_router(large_batch)\n",
    "    expert_capacity = int(batch_size * model.capacity_factor)\n",
    "    \n",
    "print(f\"专家容量: {expert_capacity}\")\n",
    "print(f\"实际专家负载: {expert_load.cpu().numpy()}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Total Loss: 3.2442\n",
      "Epoch 2, Total Loss: 3.0339\n",
      "Epoch 3, Total Loss: 3.8106\n",
      "Epoch 4, Total Loss: 3.5408\n",
      "Epoch 5, Total Loss: 3.2263\n",
      "Epoch 6, Total Loss: 3.1929\n",
      "Epoch 7, Total Loss: 2.7240\n",
      "Epoch 8, Total Loss: 3.2649\n",
      "Epoch 9, Total Loss: 2.6452\n",
      "Epoch 10, Total Loss: 2.5835\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Optional, List, Dict, Any\n",
    "\n",
    "class LoRALayer:\n",
    "    \"\"\"LoRA层的基类，为线性层添加低秩适应功能\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        r: int, \n",
    "        lora_alpha: int, \n",
    "        lora_dropout: float,\n",
    "        merge_weights: bool\n",
    "    ):\n",
    "        self.r = r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.lora_dropout = nn.Dropout(p=lora_dropout)\n",
    "        self.merged = False\n",
    "        self.merge_weights = merge_weights\n",
    "\n",
    "class LoRALinear(nn.Linear, LoRALayer):\n",
    "    \"\"\"带LoRA的线性层\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_features: int, \n",
    "        out_features: int,\n",
    "        r: int = 0, \n",
    "        lora_alpha: int = 1, \n",
    "        lora_dropout: float = 0.,\n",
    "        **kwargs\n",
    "    ):\n",
    "        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n",
    "        LoRALayer.__init__(self, r, lora_alpha, lora_dropout, False)\n",
    "        \n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Parameter(self.weight.new_zeros((r, in_features)))\n",
    "            self.lora_B = nn.Parameter(self.weight.new_zeros((out_features, r)))\n",
    "            self.scaling = self.lora_alpha / self.r\n",
    "            self.weight.requires_grad = False\n",
    "            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if self.r > 0 and not self.merged:\n",
    "            result = F.linear(x, self.weight, bias=self.bias)\n",
    "            if self.training and self.lora_dropout.p > 0:\n",
    "                result += self.lora_dropout(x) @ (self.lora_B @ self.lora_A) * self.scaling\n",
    "            else:\n",
    "                result += (x @ (self.lora_B @ self.lora_A)) * self.scaling\n",
    "            return result\n",
    "        else:\n",
    "            return F.linear(x, self.weight, bias=self.bias)\n",
    "\n",
    "class Expert(nn.Module):\n",
    "    \"\"\"MMoE中的单个专家\"\"\"\n",
    "    def __init__(self, in_features: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class DynamicGatingNetwork(nn.Module):\n",
    "    \"\"\"动态门控网络，根据输入调整专家权重\"\"\"\n",
    "    def __init__(self, in_features: int, num_experts: int, num_tasks: int, hidden_size: int = 64):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.num_tasks = num_tasks\n",
    "        self.shared_layer = nn.Linear(in_features, hidden_size)\n",
    "        self.gates = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, num_experts)\n",
    "            for _ in range(num_tasks)\n",
    "        ])\n",
    "        self.adaptive_layer = nn.Linear(in_features, num_tasks * num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_repr = F.relu(self.shared_layer(x))\n",
    "        base_gates = [gate(shared_repr) for gate in self.gates]\n",
    "        adaptive_weights = self.adaptive_layer(x).view(-1, self.num_tasks, self.num_experts)\n",
    "        final_gates = []\n",
    "        for i in range(self.num_tasks):\n",
    "            task_gate = base_gates[i] + adaptive_weights[:, i, :]\n",
    "            final_gates.append(F.softmax(task_gate, dim=-1))\n",
    "        return final_gates\n",
    "\n",
    "class DynamicMMoE(nn.Module):\n",
    "    \"\"\"动态多门混合专家模型\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_features: int, \n",
    "        num_experts: int = 4, \n",
    "        num_tasks: int = 3,\n",
    "        expert_hidden_size: Optional[int] = None,  # 改为可选参数\n",
    "        gate_hidden_size: int = 64\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.num_tasks = num_tasks\n",
    "        \n",
    "        # 关键修改：默认专家输出维度与输入维度一致\n",
    "        if expert_hidden_size is None:\n",
    "            expert_hidden_size = in_features  # 与输入维度对齐\n",
    "        \n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(in_features, expert_hidden_size)\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "        self.gating = DynamicGatingNetwork(\n",
    "            in_features, \n",
    "            num_experts, \n",
    "            num_tasks,\n",
    "            gate_hidden_size\n",
    "        )\n",
    "        \n",
    "        self.loss_weight = 0.01\n",
    "\n",
    "    def forward(self, x):\n",
    "        expert_outputs = [expert(x) for expert in self.experts]\n",
    "        expert_outputs = torch.stack(expert_outputs, dim=1)\n",
    "        \n",
    "        gates = self.gating(x)\n",
    "        task_outputs = []\n",
    "        for i in range(self.num_tasks):\n",
    "            expanded_gates = gates[i].unsqueeze(2).expand_as(expert_outputs)\n",
    "            weighted_output = expert_outputs * expanded_gates\n",
    "            task_output = torch.sum(weighted_output, dim=1)\n",
    "            task_outputs.append(task_output)\n",
    "        \n",
    "        gates_tensor = torch.stack(gates, dim=1)\n",
    "        importance = gates_tensor.mean(0).mean(0)\n",
    "        load = torch.stack([(gates_tensor.argmax(2) == i).float().mean() for i in range(self.num_experts)])\n",
    "        balance_loss = (importance * load).sum() * self.loss_weight\n",
    "        \n",
    "        return task_outputs, balance_loss\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    \"\"\"结合LoRA和动态MMoE的基模型\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int = 768,\n",
    "        hidden_dim: int = 2048,\n",
    "        num_tasks: int = 3,\n",
    "        num_layers: int = 4,\n",
    "        num_experts: int = 4,\n",
    "        lora_r: int = 8,\n",
    "        lora_alpha: int = 16\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.shared_layers = nn.ModuleList([\n",
    "            LoRALinear(\n",
    "                in_features=hidden_dim,\n",
    "                out_features=hidden_dim,\n",
    "                r=lora_r,\n",
    "                lora_alpha=lora_alpha\n",
    "            )\n",
    "            for _ in range(num_layers // 2)\n",
    "        ])\n",
    "        \n",
    "        # 关键修改：MMoE的输入维度是hidden_dim，专家输出维度也会自动设为hidden_dim\n",
    "        self.mmoe_layer = DynamicMMoE(\n",
    "            in_features=hidden_dim,  # 输入维度为hidden_dim\n",
    "            num_experts=num_experts,\n",
    "            num_tasks=num_tasks,\n",
    "            # 不指定expert_hidden_size，自动与in_features（hidden_dim）对齐\n",
    "        )\n",
    "        \n",
    "        # 任务输出层输入维度为hidden_dim（与专家输出一致）\n",
    "        self.task_output_layers = nn.ModuleList([\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "            for _ in range(num_tasks)\n",
    "        ])\n",
    "        \n",
    "        self.act = nn.GELU()\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act(self.norm(self.input_layer(x)))\n",
    "        \n",
    "        for layer in self.shared_layers:\n",
    "            x = self.act(self.norm(layer(x)))\n",
    "        \n",
    "        task_reprs, balance_loss = self.mmoe_layer(x)\n",
    "        \n",
    "        task_outputs = [self.task_output_layers[i](task_reprs[i]) for i in range(len(task_reprs))]\n",
    "        \n",
    "        return task_outputs, balance_loss\n",
    "\n",
    "def train_example():\n",
    "    model = BaseModel(\n",
    "        input_dim=768,\n",
    "        hidden_dim=2048,\n",
    "        num_tasks=3,\n",
    "        num_layers=4,\n",
    "        num_experts=4,\n",
    "        lora_r=8,\n",
    "        lora_alpha=16\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        [p for n, p in model.named_parameters() if p.requires_grad],\n",
    "        lr=1e-4\n",
    "    )\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        inputs = torch.randn(32, 768)\n",
    "        labels = [torch.randn(32, 1) for _ in range(3)]\n",
    "        \n",
    "        task_outputs, balance_loss = model(inputs)\n",
    "        task_losses = [F.mse_loss(task_outputs[i], labels[i]) for i in range(3)]\n",
    "        total_loss = sum(task_losses) + balance_loss\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Total Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0+cu111\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nAutoModel requires the PyTorch library but it was not found in your environment. Check out the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 278\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Total Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 278\u001b[0m     \u001b[43mtrain_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 233\u001b[0m, in \u001b[0;36mtrain_example\u001b[0;34m()\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_example\u001b[39m():\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;66;03m# 初始化模型（使用BERT-base作为基模型）\u001b[39;00m\n\u001b[0;32m--> 233\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceMultiTaskModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_tasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 假设有3个任务\u001b[39;49;00m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_experts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlora_r\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlora_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlora_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_lora_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkey\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 只对注意力机制添加LoRA\u001b[39;49;00m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# 冻结基模型的大部分参数，只训练LoRA和MMoE相关参数\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters():\n",
      "Cell \u001b[0;32mIn[22], line 164\u001b[0m, in \u001b[0;36mHuggingFaceMultiTaskModel.__init__\u001b[0;34m(self, model_name, num_tasks, num_experts, lora_r, lora_alpha, lora_dropout, add_lora_to)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# 加载Hugging Face预训练模型\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[1;32m    165\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhidden_size\n",
      "File \u001b[0;32m~/anaconda3/envs/nuplan/lib/python3.9/site-packages/transformers/utils/import_utils.py:1994\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_dummy\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmro\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1993\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1994\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nuplan/lib/python3.9/site-packages/transformers/utils/import_utils.py:1980\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1977\u001b[0m         failed\u001b[38;5;241m.\u001b[39mappend(msg\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[1;32m   1979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nAutoModel requires the PyTorch library but it was not found in your environment. Check out the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from typing import Optional, List, Dict, Any\n",
    "from transformers import AutoModel, AutoConfig\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)  # 应该输出版本号，如 2.0.1\n",
    "\n",
    "# LoRA相关模块\n",
    "class LoRALayer:\n",
    "    \"\"\"LoRA层的基类，为线性层添加低秩适应功能\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        r: int, \n",
    "        lora_alpha: int, \n",
    "        lora_dropout: float,\n",
    "        merge_weights: bool\n",
    "    ):\n",
    "        self.r = r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.lora_dropout = nn.Dropout(p=lora_dropout)\n",
    "        self.merged = False\n",
    "        self.merge_weights = merge_weights\n",
    "\n",
    "class LoRALinear(nn.Linear, LoRALayer):\n",
    "    \"\"\"带LoRA的线性层\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_features: int, \n",
    "        out_features: int,\n",
    "        r: int = 0, \n",
    "        lora_alpha: int = 1, \n",
    "        lora_dropout: float = 0.,\n",
    "        **kwargs\n",
    "    ):\n",
    "        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n",
    "        LoRALayer.__init__(self, r, lora_alpha, lora_dropout, False)\n",
    "        \n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Parameter(self.weight.new_zeros((r, in_features)))\n",
    "            self.lora_B = nn.Parameter(self.weight.new_zeros((out_features, r)))\n",
    "            self.scaling = self.lora_alpha / self.r\n",
    "            self.weight.requires_grad = False\n",
    "            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.lora_B)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if self.r > 0 and not self.merged:\n",
    "            result = F.linear(x, self.weight, bias=self.bias)\n",
    "            if self.training and self.lora_dropout.p > 0:\n",
    "                result += self.lora_dropout(x) @ (self.lora_B @ self.lora_A) * self.scaling\n",
    "            else:\n",
    "                result += (x @ (self.lora_B @ self.lora_A)) * self.scaling\n",
    "            return result\n",
    "        else:\n",
    "            return F.linear(x, self.weight, bias=self.bias)\n",
    "\n",
    "# 动态MMoE相关模块\n",
    "class Expert(nn.Module):\n",
    "    \"\"\"MMoE中的单个专家\"\"\"\n",
    "    def __init__(self, in_features: int, hidden_size: int):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class DynamicGatingNetwork(nn.Module):\n",
    "    \"\"\"动态门控网络，根据输入调整专家权重\"\"\"\n",
    "    def __init__(self, in_features: int, num_experts: int, num_tasks: int, hidden_size: int = 64):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.num_tasks = num_tasks\n",
    "        self.shared_layer = nn.Linear(in_features, hidden_size)\n",
    "        self.gates = nn.ModuleList([\n",
    "            nn.Linear(hidden_size, num_experts)\n",
    "            for _ in range(num_tasks)\n",
    "        ])\n",
    "        self.adaptive_layer = nn.Linear(in_features, num_tasks * num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        shared_repr = F.relu(self.shared_layer(x))\n",
    "        base_gates = [gate(shared_repr) for gate in self.gates]\n",
    "        adaptive_weights = self.adaptive_layer(x).view(-1, self.num_tasks, self.num_experts)\n",
    "        final_gates = []\n",
    "        for i in range(self.num_tasks):\n",
    "            task_gate = base_gates[i] + adaptive_weights[:, i, :]\n",
    "            final_gates.append(F.softmax(task_gate, dim=-1))\n",
    "        return final_gates\n",
    "\n",
    "class DynamicMMoE(nn.Module):\n",
    "    \"\"\"动态多门混合专家模型\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_features: int, \n",
    "        num_experts: int = 4, \n",
    "        num_tasks: int = 3,\n",
    "        expert_hidden_size: Optional[int] = None,\n",
    "        gate_hidden_size: int = 64\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.num_tasks = num_tasks\n",
    "        \n",
    "        if expert_hidden_size is None:\n",
    "            expert_hidden_size = in_features\n",
    "        \n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(in_features, expert_hidden_size)\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "        self.gating = DynamicGatingNetwork(\n",
    "            in_features, \n",
    "            num_experts, \n",
    "            num_tasks,\n",
    "            gate_hidden_size\n",
    "        )\n",
    "        \n",
    "        self.loss_weight = 0.01\n",
    "\n",
    "    def forward(self, x):\n",
    "        expert_outputs = [expert(x) for expert in self.experts]\n",
    "        expert_outputs = torch.stack(expert_outputs, dim=1)\n",
    "        \n",
    "        gates = self.gating(x)\n",
    "        task_outputs = []\n",
    "        for i in range(self.num_tasks):\n",
    "            expanded_gates = gates[i].unsqueeze(2).expand_as(expert_outputs)\n",
    "            weighted_output = expert_outputs * expanded_gates\n",
    "            task_output = torch.sum(weighted_output, dim=1)\n",
    "            task_outputs.append(task_output)\n",
    "        \n",
    "        gates_tensor = torch.stack(gates, dim=1)\n",
    "        importance = gates_tensor.mean(0).mean(0)\n",
    "        load = torch.stack([(gates_tensor.argmax(2) == i).float().mean() for i in range(self.num_experts)])\n",
    "        balance_loss = (importance * load).sum() * self.loss_weight\n",
    "        \n",
    "        return task_outputs, balance_loss\n",
    "\n",
    "# 整合Hugging Face基模型的多任务模型\n",
    "class HuggingFaceMultiTaskModel(nn.Module):\n",
    "    \"\"\"结合Hugging Face基模型、LoRA和动态MMoE的多任务模型\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"bert-base-uncased\",\n",
    "        num_tasks: int = 3,\n",
    "        num_experts: int = 4,\n",
    "        lora_r: int = 8,\n",
    "        lora_alpha: int = 16,\n",
    "        lora_dropout: float = 0.1,\n",
    "        add_lora_to: List[str] = [\"key\", \"query\", \"value\", \"intermediate\", \"output\"],\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 加载Hugging Face预训练模型\n",
    "        self.base_model = AutoModel.from_pretrained(model_name)\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_tasks = num_tasks\n",
    "        \n",
    "        # 为基模型的特定层添加LoRA\n",
    "        self._add_lora_to_model(lora_r, lora_alpha, lora_dropout, add_lora_to)\n",
    "        \n",
    "        # 动态MMoE层\n",
    "        self.mmoe_layer = DynamicMMoE(\n",
    "            in_features=self.hidden_size,\n",
    "            num_experts=num_experts,\n",
    "            num_tasks=num_tasks\n",
    "        )\n",
    "        \n",
    "        # 任务特定输出层\n",
    "        self.task_output_layers = nn.ModuleList([\n",
    "            nn.Linear(self.hidden_size, 1)  # 假设每个任务都是回归问题\n",
    "            for _ in range(num_tasks)\n",
    "        ])\n",
    "        \n",
    "        # 池化层（从基模型获取句子表示）\n",
    "        self.pooler = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.act = nn.Tanh()\n",
    "\n",
    "    def _add_lora_to_model(self, r, lora_alpha, lora_dropout, add_lora_to):\n",
    "        \"\"\"为基模型的指定层添加LoRA\"\"\"\n",
    "        for name, module in self.base_model.named_modules():\n",
    "            if any(layer_type in name for layer_type in add_lora_to) and isinstance(module, nn.Linear):\n",
    "                # 创建LoRA线性层替换原始线性层\n",
    "                lora_module = LoRALinear(\n",
    "                    in_features=module.in_features,\n",
    "                    out_features=module.out_features,\n",
    "                    r=r,\n",
    "                    lora_alpha=lora_alpha,\n",
    "                    lora_dropout=lora_dropout\n",
    "                )\n",
    "                # 复制原始权重\n",
    "                lora_module.weight.data = module.weight.data\n",
    "                if module.bias is not None:\n",
    "                    lora_module.bias.data = module.bias.data\n",
    "                \n",
    "                # 替换模块\n",
    "                parent_name, child_name = name.rsplit(\".\", 1)\n",
    "                parent_module = self.base_model.get_submodule(parent_name)\n",
    "                setattr(parent_module, child_name, lora_module)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        # 通过基模型获取表示\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        # 获取[CLS]标记的表示作为句子表示\n",
    "        pooled_output = self.act(self.pooler(outputs.last_hidden_state[:, 0]))\n",
    "        \n",
    "        # 通过动态MMoE层\n",
    "        task_reprs, balance_loss = self.mmoe_layer(pooled_output)\n",
    "        \n",
    "        # 每个任务的输出\n",
    "        task_outputs = [self.task_output_layers[i](task_reprs[i]) for i in range(self.num_tasks)]\n",
    "        \n",
    "        return task_outputs, balance_loss\n",
    "\n",
    "# 多任务训练示例\n",
    "def train_example():\n",
    "    # 初始化模型（使用BERT-base作为基模型）\n",
    "    model = HuggingFaceMultiTaskModel(\n",
    "        model_name=\"bert-base-uncased\",\n",
    "        num_tasks=3,  # 假设有3个任务\n",
    "        num_experts=4,\n",
    "        lora_r=8,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        add_lora_to=[\"key\", \"query\", \"value\"]  # 只对注意力机制添加LoRA\n",
    "    )\n",
    "    \n",
    "    # 冻结基模型的大部分参数，只训练LoRA和MMoE相关参数\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"lora\" not in name and \"mmoe\" not in name and \"task_output\" not in name:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # 优化器 - 只训练LoRA参数、MMoE相关参数和任务输出层\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [p for n, p in model.named_parameters() if p.requires_grad],\n",
    "        lr=1e-4\n",
    "    )\n",
    "    \n",
    "    # 模拟训练\n",
    "    for epoch in range(10):\n",
    "        # 生成随机输入和多任务标签\n",
    "        input_ids = torch.randint(0, 30000, (32, 128))  # 批次大小32，序列长度128\n",
    "        attention_mask = torch.ones(32, 128)\n",
    "        labels = [torch.randn(32, 1) for _ in range(3)]  # 3个任务的标签\n",
    "        \n",
    "        # 前向传播\n",
    "        task_outputs, balance_loss = model(input_ids, attention_mask)\n",
    "        \n",
    "        # 计算每个任务的损失\n",
    "        task_losses = [F.mse_loss(task_outputs[i], labels[i]) for i in range(3)]\n",
    "        \n",
    "        # 总损失 = 任务损失之和 + MMoE负载均衡损失\n",
    "        total_loss = sum(task_losses) + balance_loss\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Total Loss: {total_loss.item():.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Disabling PyTorch because PyTorch >= 2.1 is required but found 1.9.0+cu111\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nAutoModel requires the PyTorch library but it was not found in your environment. Check out the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 332\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTask Losses: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mv\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mk,\u001b[38;5;250m \u001b[39mv\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mtask_losses\u001b[38;5;241m.\u001b[39mitems()])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 332\u001b[0m     \u001b[43mtrain_multi_task_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 250\u001b[0m, in \u001b[0;36mtrain_multi_task_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain_multi_task_model\u001b[39m():\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# 初始化模型\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceDynamicMMoEModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_experts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentiment\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclassification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_classes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 情感分析\u001b[39;49;00m\n\u001b[1;32m    255\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mner\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken_classification\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_classes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 命名实体识别\u001b[39;49;00m\n\u001b[1;32m    256\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mregression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mregression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_classes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 文本评分\u001b[39;49;00m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_lora\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 使用LoRA进行参数高效微调\u001b[39;49;00m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlora_r\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlora_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\n\u001b[1;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# 优化器 - 只训练LoRA参数和MMoE相关参数\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(\n\u001b[1;32m    265\u001b[0m         [p \u001b[38;5;28;01mfor\u001b[39;00m n, p \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_parameters() \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad],\n\u001b[1;32m    266\u001b[0m         lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m\n\u001b[1;32m    267\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[2], line 132\u001b[0m, in \u001b[0;36mHuggingFaceDynamicMMoEModel.__init__\u001b[0;34m(self, model_name, num_experts, tasks, expert_hidden_dim, gate_hidden_dim, dropout, load_weights, use_lora, lora_r, lora_alpha)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# 加载预训练模型\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[1;32m    133\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mhidden_size\n",
      "File \u001b[0;32m~/anaconda3/envs/nuplan/lib/python3.9/site-packages/transformers/utils/import_utils.py:1994\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1992\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_dummy\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmro\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1993\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1994\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/nuplan/lib/python3.9/site-packages/transformers/utils/import_utils.py:1980\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1977\u001b[0m         failed\u001b[38;5;241m.\u001b[39mappend(msg\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[1;32m   1979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1980\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nAutoModel requires the PyTorch library but it was not found in your environment. Check out the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import List, Dict, Optional, Union\n",
    "from transformers import AutoModel, AutoConfig, PreTrainedModel, PreTrainedTokenizerFast\n",
    "\n",
    "# 动态门控网络\n",
    "class DynamicGatingNetwork(nn.Module):\n",
    "    \"\"\"根据输入动态调整专家权重的门控网络\"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_dim: int, \n",
    "        num_experts: int, \n",
    "        num_tasks: int, \n",
    "        hidden_dim: int = 64\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.num_tasks = num_tasks\n",
    "        \n",
    "        # 共享底层\n",
    "        self.shared_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # 每个任务的门控网络\n",
    "        self.gates = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, num_experts)\n",
    "            )\n",
    "            for _ in range(num_tasks)\n",
    "        ])\n",
    "        \n",
    "        # 输入依赖的门控调整\n",
    "        self.adaptive_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_tasks * num_experts)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> List[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        计算每个任务的专家权重\n",
    "        \n",
    "        Args:\n",
    "            inputs: 输入特征 [batch_size, input_dim]\n",
    "            \n",
    "        Returns:\n",
    "            gates: 每个任务的专家权重列表，每个元素形状为 [batch_size, num_experts]\n",
    "        \"\"\"\n",
    "        # 计算共享表示\n",
    "        shared_repr = F.relu(self.shared_layer(inputs))\n",
    "        \n",
    "        # 基础门控权重\n",
    "        base_gates = [gate(shared_repr) for gate in self.gates]\n",
    "        \n",
    "        # 输入依赖的门控调整\n",
    "        adaptive_weights = self.adaptive_layer(inputs).view(-1, self.num_tasks, self.num_experts)\n",
    "        \n",
    "        # 组合基础门控和自适应调整\n",
    "        final_gates = []\n",
    "        for i in range(self.num_tasks):\n",
    "            task_gate = base_gates[i] + adaptive_weights[:, i, :]\n",
    "            final_gates.append(F.softmax(task_gate, dim=-1))\n",
    "            \n",
    "        return final_gates\n",
    "\n",
    "# 专家网络\n",
    "class Expert(nn.Module):\n",
    "    \"\"\"单个专家网络\"\"\"\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layers(inputs)\n",
    "\n",
    "# 任务特定输出层\n",
    "class TaskOutputLayer(nn.Module):\n",
    "    \"\"\"任务特定的输出层\"\"\"\n",
    "    def __init__(self, input_dim: int, task_type: str, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.task_type = task_type\n",
    "        \n",
    "        if task_type == \"classification\":\n",
    "            self.output_layer = nn.Linear(input_dim, num_classes)\n",
    "        elif task_type == \"regression\":\n",
    "            self.output_layer = nn.Linear(input_dim, 1)\n",
    "        elif task_type == \"token_classification\":\n",
    "            self.output_layer = nn.Linear(input_dim, num_classes)\n",
    "        else:\n",
    "            raise ValueError(f\"不支持的任务类型: {task_type}\")\n",
    "            \n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        if self.task_type == \"token_classification\":\n",
    "            # 对于token级任务，保持序列维度\n",
    "            return self.output_layer(inputs)\n",
    "        else:\n",
    "            # 对于句子级任务，取[CLS]标记的表示\n",
    "            return self.output_layer(inputs[:, 0])\n",
    "\n",
    "# 基于Hugging Face的动态MMoE多任务模型\n",
    "class HuggingFaceDynamicMMoEModel(nn.Module):\n",
    "    \"\"\"结合Hugging Face预训练模型和动态MMoE的多任务学习模型\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"bert-base-uncased\",\n",
    "        num_experts: int = 4,\n",
    "        tasks: List[Dict[str, Union[str, int]]] = [\n",
    "            {\"name\": \"classification\", \"type\": \"classification\", \"num_classes\": 2},\n",
    "            {\"name\": \"ner\", \"type\": \"token_classification\", \"num_classes\": 5},\n",
    "            {\"name\": \"regression\", \"type\": \"regression\", \"num_classes\": 1}\n",
    "        ],\n",
    "        expert_hidden_dim: int = 768,\n",
    "        gate_hidden_dim: int = 64,\n",
    "        dropout: float = 0.1,\n",
    "        load_weights: bool = True,\n",
    "        use_lora: bool = False,\n",
    "        lora_r: int = 8,\n",
    "        lora_alpha: int = 16\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 加载预训练模型\n",
    "        self.base_model = AutoModel.from_pretrained(model_name)\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        self.hidden_dim = config.hidden_size\n",
    "        self.tasks = tasks\n",
    "        self.num_tasks = len(tasks)\n",
    "        \n",
    "        # 动态MMoE组件\n",
    "        self.experts = nn.ModuleList([\n",
    "            Expert(self.hidden_dim, expert_hidden_dim, dropout)\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "        self.gating_network = DynamicGatingNetwork(\n",
    "            input_dim=self.hidden_dim,\n",
    "            num_experts=num_experts,\n",
    "            num_tasks=self.num_tasks,\n",
    "            hidden_dim=gate_hidden_dim\n",
    "        )\n",
    "        \n",
    "        # 任务特定输出层\n",
    "        self.task_output_layers = nn.ModuleList([\n",
    "            TaskOutputLayer(\n",
    "                input_dim=expert_hidden_dim,\n",
    "                task_type=task[\"type\"],\n",
    "                num_classes=task[\"num_classes\"]\n",
    "            )\n",
    "            for task in tasks\n",
    "        ])\n",
    "        \n",
    "        # 负载均衡损失权重\n",
    "        self.loss_weight = 0.01\n",
    "        \n",
    "        # LoRA支持（简化版）\n",
    "        if use_lora:\n",
    "            self._apply_lora(lora_r, lora_alpha)\n",
    "            \n",
    "    def _apply_lora(self, r: int, alpha: int):\n",
    "        \"\"\"为模型应用LoRA（低秩适应）\"\"\"\n",
    "        # 注意：这是一个简化版的LoRA实现，实际应用中可能需要更复杂的处理\n",
    "        from peft import get_peft_model, LoraConfig, TaskType\n",
    "        \n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.FEATURE_EXTRACTION,\n",
    "            r=r,\n",
    "            lora_alpha=alpha,\n",
    "            lora_dropout=0.1,\n",
    "            target_modules=[\"query\", \"key\", \"value\"]  # 只对注意力机制应用LoRA\n",
    "        )\n",
    "        \n",
    "        self.base_model = get_peft_model(self.base_model, lora_config)\n",
    "            \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.Tensor,\n",
    "        attention_mask: torch.Tensor = None,\n",
    "        token_type_ids: torch.Tensor = None,\n",
    "        task_indices: Optional[List[int]] = None\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        \n",
    "        Args:\n",
    "            input_ids: 输入token IDs [batch_size, seq_len]\n",
    "            attention_mask: 注意力掩码 [batch_size, seq_len]\n",
    "            token_type_ids: token类型ID [batch_size, seq_len]\n",
    "            task_indices: 任务索引列表，如果提供，则只计算指定任务的输出\n",
    "            \n",
    "        Returns:\n",
    "            outputs: 包含每个任务输出的字典\n",
    "        \"\"\"\n",
    "        # 通过预训练模型获取表示\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        \n",
    "        # 获取[CLS]标记的表示作为句子表示（用于门控网络）\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # 获取所有专家的输出\n",
    "        expert_outputs = [expert(outputs.last_hidden_state) for expert in self.experts]\n",
    "        expert_outputs = torch.stack(expert_outputs, dim=1)  # [batch_size, num_experts, seq_len, hidden_dim]\n",
    "        \n",
    "        # 获取门控权重\n",
    "        gates = self.gating_network(pooled_output)  # 每个任务一个门控权重\n",
    "        \n",
    "        # 为每个任务计算输出\n",
    "        task_outputs = {}\n",
    "        for i, task in enumerate(self.tasks):\n",
    "            if task_indices is not None and i not in task_indices:\n",
    "                continue\n",
    "                \n",
    "            # 扩展门控权重以匹配专家输出维度\n",
    "            expanded_gates = gates[i].unsqueeze(2).unsqueeze(2).expand_as(expert_outputs)\n",
    "            \n",
    "            # 加权组合专家输出\n",
    "            weighted_output = expert_outputs * expanded_gates\n",
    "            task_repr = torch.sum(weighted_output, dim=1)  # [batch_size, seq_len, hidden_dim]\n",
    "            \n",
    "            # 通过任务特定输出层\n",
    "            task_output = self.task_output_layers[i](task_repr)\n",
    "            task_outputs[task[\"name\"]] = task_output\n",
    "        \n",
    "        # 计算负载均衡损失\n",
    "        gates_tensor = torch.stack(gates, dim=1)  # [batch_size, num_tasks, num_experts]\n",
    "        importance = gates_tensor.mean(0).mean(0)  # 专家重要性\n",
    "        load = torch.stack([(gates_tensor.argmax(2) == i).float().mean() for i in range(len(self.experts))])\n",
    "        balance_loss = (importance * load).sum() * self.loss_weight\n",
    "        \n",
    "        return {\n",
    "            \"task_outputs\": task_outputs,\n",
    "            \"balance_loss\": balance_loss\n",
    "        }\n",
    "\n",
    "# 训练示例\n",
    "def train_multi_task_model():\n",
    "    # 初始化模型\n",
    "    model = HuggingFaceDynamicMMoEModel(\n",
    "        model_name=\"bert-base-uncased\",\n",
    "        num_experts=4,\n",
    "        tasks=[\n",
    "            {\"name\": \"sentiment\", \"type\": \"classification\", \"num_classes\": 3},  # 情感分析\n",
    "            {\"name\": \"ner\", \"type\": \"token_classification\", \"num_classes\": 5},  # 命名实体识别\n",
    "            {\"name\": \"regression\", \"type\": \"regression\", \"num_classes\": 1}  # 文本评分\n",
    "        ],\n",
    "        use_lora=True,  # 使用LoRA进行参数高效微调\n",
    "        lora_r=8,\n",
    "        lora_alpha=16\n",
    "    )\n",
    "    \n",
    "    # 优化器 - 只训练LoRA参数和MMoE相关参数\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [p for n, p in model.named_parameters() if p.requires_grad],\n",
    "        lr=1e-4\n",
    "    )\n",
    "    \n",
    "    # 假设我们有一个多任务数据集\n",
    "    # 在实际应用中，应该使用真实数据集\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "    \n",
    "    # 模拟训练\n",
    "    model.train()\n",
    "    for epoch in range(10):\n",
    "        # 准备一批混合任务的数据\n",
    "        # 注意：在实际应用中，需要适当组织多任务数据\n",
    "        batch_size = 16\n",
    "        \n",
    "        # 示例输入\n",
    "        texts = [\n",
    "            \"This movie is really amazing! I love it.\",\n",
    "            \"Apple is planning to release a new iPhone next month.\",\n",
    "            \"The stock market dropped significantly today due to economic concerns.\"\n",
    "        ] * batch_size\n",
    "        \n",
    "        # 编码输入\n",
    "        inputs = tokenizer(\n",
    "            texts, \n",
    "            return_tensors=\"torch\", \n",
    "            padding=True, \n",
    "            truncation=True, \n",
    "            max_length=128\n",
    "        )\n",
    "        \n",
    "        # 模拟任务标签\n",
    "        sentiment_labels = torch.randint(0, 3, (batch_size,))  # 情感标签\n",
    "        ner_labels = torch.randint(0, 5, (batch_size, 128))  # NER标签\n",
    "        regression_labels = torch.randn(batch_size, 1)  # 回归标签\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # 计算每个任务的损失\n",
    "        task_losses = {\n",
    "            \"sentiment\": F.cross_entropy(\n",
    "                outputs[\"task_outputs\"][\"sentiment\"], \n",
    "                sentiment_labels\n",
    "            ),\n",
    "            \"ner\": F.cross_entropy(\n",
    "                outputs[\"task_outputs\"][\"ner\"].view(-1, 5), \n",
    "                ner_labels.view(-1)\n",
    "            ),\n",
    "            \"regression\": F.mse_loss(\n",
    "                outputs[\"task_outputs\"][\"regression\"], \n",
    "                regression_labels\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # 总损失 = 任务损失之和 + MMoE负载均衡损失\n",
    "        total_loss = sum(task_losses.values()) + outputs[\"balance_loss\"]\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Total Loss: {total_loss.item():.4f}\")\n",
    "        print(f\"Task Losses: {', '.join([f'{k}: {v.item():.4f}' for k, v in task_losses.items()])}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_multi_task_model()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "第21章-多智能体强化学习进阶.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nuplan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
